<!DOCTYPE html>
<html><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Complete Snowflake &amp; Data Engineering Interview Guide - 74 Questions</title>

</head><body><style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f9f9f9;
        }
        
        
        
        header {
            border-bottom: 3px solid #007acc;
            padding-bottom: 20px;
            margin-bottom: 40px;
        }
        
        h1 {
            color: #007acc;
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .subtitle {
            color: #666;
            font-size: 1.1em;
        }

        /* Table of Contents Styling */
        .toc-container {
            background-color: #f0f7ff;
            border: 2px solid #007acc;
            border-radius: 8px;
            padding: 30px;
            margin-bottom: 50px;
        }

        .toc-container h2 {
            color: #007acc;
            font-size: 1.8em;
            margin-bottom: 25px;
            border-bottom: 2px solid #007acc;
            padding-bottom: 10px;
        }

        .toc-sections {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin-top: 20px;
        }

        .toc-section {
            background-color: white;
            border-left: 4px solid #0066cc;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .toc-section h3 {
            color: #0066cc;
            font-size: 1.2em;
            margin-bottom: 15px;
            padding-bottom: 8px;
            border-bottom: 1px solid #e0e0e0;
        }

        .toc-section ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .toc-section li {
            margin-bottom: 10px;
        }

        .toc-section a {
            color: #0066cc;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .toc-section a:hover {
            color: #007acc;
            text-decoration: underline;
        }

        .toc-question-number {
            color: #e74c3c;
            font-weight: bold;
            margin-right: 5px;
        }

        .back-to-toc {
            display: inline-block;
            margin-top: 20px;
            padding: 8px 15px;
            background-color: #007acc;
            color: white;
            border-radius: 4px;
            text-decoration: none;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }

        .back-to-toc:hover {
            background-color: #0066cc;
        }
        
        .question {
            margin-bottom: 40px;
            border-left: 4px solid #007acc;
            padding-left: 20px;
            scroll-margin-top: 100px;
        }
        
        .question-number {
            font-size: 1.1em;
            color: #e74c3c;
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .question-title {
            font-size: 1.4em;
            color: #2980b9;
            margin: 10px 0;
            font-weight: 600;
        }
        
        .answer {
            margin-top: 15px;
            color: #333;
            line-height: 1.8;
        }
        
        .answer p {
            margin-bottom: 12px;
        }
        
        pre {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.4;
        }
        
        code {
            background-color: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        strong {
            color: #0066cc;
            font-weight: 600;
        }
        
        ul, ol {
            margin: 15px 0 15px 30px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .highlight {
            background-color: #e7f3ff;
            padding: 15px;
            border-left: 3px solid #0066cc;
            margin: 15px 0;
            border-radius: 3px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: #f9f9f9;
        }
        
        table th {
            background-color: #007acc;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        
        table td {
            padding: 12px;
            border-bottom: 1px solid #ddd;
        }
        
        table tr:hover {
            background-color: #f0f0f0;
        }
        
        @media print {
            body {
                background-color: white;
            }
            
            h1 {
                font-size: 1.8em;
                page-break-after: avoid;
            }
            .question {
                page-break-inside: avoid;
            }
            pre {
                font-size: 8pt;
                padding: 10px;
            }
            code {
                font-size: 8pt;
            }
        }
    
html { scroll-behavior: smooth; }
.question-title { color: #8e44ad !important; font-weight: 700; }

/* ---------------- Responsive Enhancements ---------------- */


pre {
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-x: auto;
}

code {
  word-break: break-word;
}



@media (min-width: 768px) {
  
}

@media (min-width: 1024px) {
  
}

@media (max-width: 767px) {
  body {
    font-size: 15px;
  }

  h1 {
    font-size: 1.6em;
  }

  .question-title {
    font-size: 1.15em;
  }

  .question {
    padding-left: 15px;
  }

  .back-to-toc {
    font-size: 0.8em;
    padding: 6px 10px;
  }
}

/* -------- TOC Layout Control -------- */
.toc-container ul {
  columns: 1;
}

@media (min-width: 1024px) {
  .toc-container ul {
    columns: 2;
  }
}
.container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
</style><div class="container"><h1>Snowflake</h1><div class="question" id="q2">
<div class="question-title">Snowflake Architecture</div>
<div class="answer">
<p>Snowflake's architecture is a unique multi-cluster shared data architecture. It separates compute and storage, allowing them to scale independently, and includes a cloud services layer for management.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Database Storage:</strong> Data is reorganized into a columnar, compressed format, encrypted, and stored in micro-partitions. This storage layer is elastic and can scale dynamically.</li>
<li><strong>Query Processing (Compute Layer):</strong> Consists of virtual warehouses - independent compute clusters that execute queries without sharing compute resources.</li>
<li><strong>Cloud Services Layer:</strong> The brain of Snowflake, coordinating all activities including authentication, metadata management, query optimization, infrastructure management, and transaction management.</li>
</ul>
<p><strong>Analogy:</strong> Imagine a library where storage is the books on shelves, compute is the reading rooms, and cloud services is the librarian system managing everything.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q3">
<div class="question-title">Time-Travel with scenarios like offset, query_id and timestamp</div>
<div class="answer">
<p>Snowflake's Time Travel feature allows you to query historical data that has been changed or deleted within a specified retention period (default 1 day, up to 90 days for Enterprise Edition).</p>
<p><strong>Using AT (OFFSET):</strong> Query data from a specific point in time relative to current time (negative offset for past).</p>
<pre>-- Current data
SELECT * FROM my_table;

-- Data from 5 minutes (300 seconds) ago
SELECT * FROM my_table AT (OFFSET =&gt; -300);</pre>
<p><strong>Using AT (TIMESTAMP):</strong> Query data as it existed at an exact past timestamp.</p>
<pre>SELECT * FROM my_table AT (TIMESTAMP =&gt; '2025-06-25 10:30:00'::TIMESTAMP_LTZ);</pre>
<p><strong>Using BEFORE (STATEMENT):</strong> Query data immediately before a specific DML statement executed.</p>
<pre>CREATE TABLE my_table_recovery AS
SELECT * FROM my_table BEFORE (STATEMENT =&gt; '123abc456def');</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q4">
<div class="question-title">Optimize the query performance and how you do it</div>
<div class="answer">
<p><strong>Key Optimization Strategies:</strong></p>
<ol>
<li><strong>Choose the Right Virtual Warehouse Size:</strong> Larger warehouses provide more compute power and memory. Use larger warehouses for complex queries on large datasets.</li>
<li><strong>Cluster Your Tables:</strong> Define clustering keys on columns frequently used in WHERE clauses, JOIN conditions, or GROUP BY clauses.</li>
<li><strong>Use Materialized Views:</strong> Pre-compute and store results of complex queries, automatically updating when base tables change.</li>
<li><strong>Query Pruning:</strong> Ensure your WHERE clauses are effective. Filter on clustered columns or columns with high cardinality.</li>
<li><strong>Effective Caching:</strong> Leverage Snowflake's Result Cache and Warehouse Cache by running the same queries multiple times.</li>
<li><strong>Avoid Anti-Patterns:</strong> Don't use SELECT *, avoid correlated subqueries, eliminate unnecessary ORDER BY or DISTINCT.</li>
<li><strong>Monitor Query Profile:</strong> Use Snowflake's Query Profile to identify bottlenecks and optimize accordingly.</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q5">
<div class="question-title">What is partition and what is micro-partitions</div>
<div class="answer">
<p><strong>Partition (General Database Concept):</strong> A strategy to divide a large table into smaller, manageable pieces based on a specified column (e.g., date, region). Usually managed explicitly by the database administrator.</p>
<p><strong>Micro-partitions (Snowflake Specific):</strong> Snowflake automatically organizes all data into immutable, compressed, columnar units typically ranging from 50 MB to 500 MB. This is automatic and transparent.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>Automatic and Transparent - handled by Snowflake</li>
<li>Rich metadata stored about each micro-partition (value ranges, distinct values, null counts)</li>
<li>Query Pruning - metadata allows Snowflake to skip irrelevant micro-partitions</li>
<li>Clustering - you can define clustering keys to optimize physical co-location of data</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q7">
<div class="question-title">How you ingested JSON data into Snowflake, what steps you perform</div>
<div class="answer">
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>Prepare JSON Data:</strong> Ensure well-formed JSON files (individual objects or newline-delimited)</li>
<li><strong>Stage the Files:</strong> Use Internal Stage (PUT command) or External Stage (S3, Azure, GCP)</li>
<li><strong>Create File Format:</strong> Define how to interpret JSON files
                        <pre>CREATE FILE FORMAT json_file_format
    TYPE = 'JSON'
    STRIP_OUTER_ARRAY = TRUE
    NULL_IF = ('', 'NULL');</pre>
</li>
<li><strong>Create Target Table:</strong> Either single VARIANT column or structured columns
                        <pre>CREATE TABLE raw_json_data (
    id INT,
    json_payload VARIANT,
    load_timestamp TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);</pre>
</li>
<li><strong>Copy Data:</strong> Use COPY INTO to load data from stage
                        <pre>COPY INTO raw_json_data
FROM @my_external_json_stage/
FILE_FORMAT = json_file_format
ON_ERROR = 'CONTINUE';</pre>
</li>
<li><strong>Query &amp; Transform:</strong> Use VARIANT functions to extract and transform data</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q9">
<div class="question-title">Internal and External stages in Snowflake - Uses for each</div>
<div class="answer">
<p><strong>Internal Stages (Snowflake-managed Storage):</strong></p>
<ul>
<li>User stage: @~/</li>
<li>Table stage: @%table_name</li>
<li>Named stage: CREATE STAGE my_internal_stage</li>
<li>Uses: Quick data loading, secure data transfer, temporary files</li>
</ul>
<p><strong>External Stages (User-managed Cloud Storage):</strong></p>
<ul>
<li>Points to AWS S3, Azure Blob, Google Cloud Storage</li>
<li>Uses: Large-scale automated loads, data lake integration, Snowpipe, continuous loading</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Internal Stage</th>
<th>External Stage</th>
</tr>
<tr>
<td>Storage</td>
<td>Snowflake managed</td>
<td>Your cloud storage</td>
</tr>
<tr>
<td>File Access</td>
<td>PUT / GET commands</td>
<td>COPY INTO</td>
</tr>
<tr>
<td>Cost</td>
<td>Included in Snowflake</td>
<td>Cloud provider costs</td>
</tr>
<tr>
<td>Ideal Use</td>
<td>Ad-hoc, smaller loads</td>
<td>Large-scale automated</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q10">
<div class="question-title">How you schedule a data pipeline in Snowflake</div>
<div class="answer">
<p><strong>Snowflake Tasks (Native):</strong></p>
<pre>-- Create warehouse for tasks
CREATE WAREHOUSE ETL_WH WAREHOUSE_SIZE = 'XSMALL';

-- 1. Root Task (scheduled)
CREATE TASK load_raw_data
  WAREHOUSE = ETL_WH
  SCHEDULE = 'USING CRON 0 9 * * * Asia/Kolkata'
  AS
  COPY INTO RAW_TABLE FROM @my_external_stage/raw_files/;

-- 2. Child Task (depends on load_raw_data)
CREATE TASK transform_data
  WAREHOUSE = ETL_WH
  AFTER load_raw_data
  AS
  INSERT INTO STAGING_TABLE SELECT ... FROM RAW_TABLE;

-- Enable tasks
ALTER TASK load_raw_data RESUME;
ALTER TASK transform_data RESUME;</pre>
<p><strong>External Orchestration Tools:</strong> Apache Airflow, AWS Step Functions, Azure Data Factory, Control-M, Autosys - for complex cross-platform pipelines.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q29">
<div class="question-title">What is Zero Copy clone - Uses</div>
<div class="answer">
<p>Zero-Copy Cloning creates an instant copy of a database, schema, or table without physically duplicating data. Snowflake creates metadata pointers to the same micro-partitions. Changes use copy-on-write.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li>Instantaneous - created almost instantly</li>
<li>No storage cost at creation - pay only for incremental changes</li>
<li>No data movement</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>CREATE DATABASE prod_db_clone CLONE prod_db;
CREATE SCHEMA temp_schema CLONE prod_schema;
CREATE TABLE test_table CLONE prod_table;</pre>
<p><strong>Uses:</strong></p>
<ul>
<li><strong>Development/Testing:</strong> Clone production DB for developers to test without affecting prod</li>
<li><strong>Disaster Recovery:</strong> Create point-in-time recovery environment quickly</li>
<li><strong>Analytical Workloads:</strong> Isolate complex queries from main system</li>
<li><strong>Version Control:</strong> Snapshot data at milestones for rollback capability</li>
<li><strong>Historical Analysis:</strong> Create monthly snapshots with minimal cost</li>
<li><strong>What-If Scenarios:</strong> Run simulations without impacting production</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q30">
<div class="question-title">Convert timestamp from one timezone to another</div>
<div class="answer">
<p>Snowflake has three TIMESTAMP types: TIMESTAMP_LTZ (Local Time Zone), TIMESTAMP_NTZ (No Time Zone), TIMESTAMP_TZ (Time Zone).</p>
<p><strong>Using CONVERT_TIMEZONE (Recommended):</strong></p>
<pre>SELECT CONVERT_TIMEZONE('UTC', 'America/New_York', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 06:00:00.000'

SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'Asia/Kolkata', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 22:30:00.000'

-- With a column
SELECT event_id, event_timestamp,
       CONVERT_TIMEZONE('UTC', 'America/New_York', event_timestamp) AS event_timestamp_est
FROM events;</pre>
<p><strong>Using AT TIME ZONE:</strong></p>
<pre>SELECT '2025-07-04 10:00:00'::TIMESTAMP_NTZ AT TIME ZONE 'America/New_York';

-- Set session timezone first
ALTER SESSION SET TIMEZONE = 'UTC';
SELECT event_timestamp AT TIME ZONE 'Asia/Kolkata'
FROM events;</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q31">
<div class="question-title">Clustering Keys in Snowflake</div>
<div class="answer">
<p>Clustering Keys optimize the physical organization of data within micro-partitions to improve query performance.</p>
<p><strong>How They Work:</strong> Snowflake's Automatic Clustering service re-clusters data if clustering becomes stale, physically reorganizing micro-partitions to group similar values together.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Improved query performance for filtered/joined queries</li>
<li>Reduced data scanning → fewer credits consumed</li>
<li>Automatic and transparent</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>-- During creation
CREATE TABLE sales (
    event_date DATE,
    category VARCHAR,
    value DECIMAL
) CLUSTER BY (event_date, category);

-- After creation
ALTER TABLE sales CLUSTER BY (event_date, category);

-- Remove clustering
ALTER TABLE sales DROP CLUSTERING KEY;</pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Very large tables (hundreds of GB to TB)</li>
<li>Frequent filtering/joining on specific columns</li>
<li>High cardinality columns with skewed data</li>
<li>Data ingestion patterns cause poor clustering over time</li>
</ul>
<p><strong>Monitor with:</strong> SYSTEM$CLUSTERING_INFORMATION() function</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q32">
<div class="question-title">How to monitor Snowflake performance</div>
<div class="answer">
<p><strong>Snowflake Web Interface:</strong></p>
<ul>
<li>Query History - view all queries, duration, credits, status</li>
<li>Query Profile - visualize execution plan, identify bottlenecks</li>
<li>Warehouses - monitor status, usage, credit consumption</li>
</ul>
<p><strong>Account Usage Views (SNOWFLAKE.ACCOUNT_USAGE):</strong></p>
<ul>
<li>QUERY_HISTORY - comprehensive query data</li>
<li>WAREHOUSE_METERING_HISTORY - daily credit consumption per warehouse</li>
<li>AUTOMATIC_CLUSTERING_HISTORY - credits consumed by clustering</li>
<li>PIPE_USAGE_HISTORY - credits consumed by Snowpipe</li>
<li>COPY_HISTORY - COPY INTO command status</li>
<li>TASK_HISTORY - task execution history</li>
</ul>
<p><strong>External Tools:</strong></p>
<ul>
<li>Cloud monitoring (CloudWatch, Azure Monitor)</li>
<li>Third-party APM tools (Datadog, Splunk)</li>
<li>BI tools for custom dashboards</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q40">
<div class="question-title">Data Masking and Masking Policy</div>
<div class="answer">
<p><strong>Data Masking:</strong> Hiding sensitive data with fictional data while maintaining usefulness for development/testing/analytics.</p>
<p><strong>Snowflake Masking Policy:</strong> Dynamic masking at query time based on user role. Underlying data is never modified.</p>
<p><strong>Example:</strong></p>
<pre>-- Create masking policy
CREATE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR -&gt;
    CASE
        WHEN CURRENT_ROLE() IN ('ANALYST_ROLE') THEN '****'
        WHEN CURRENT_ROLE() = 'DATA_STEWARD_ROLE' THEN val
        ELSE 'No access'
    END;

-- Apply to column
ALTER TABLE users ALTER COLUMN email SET MASKING POLICY email_mask;</pre>
<p><strong>Benefits:</strong> Dynamic, centralized, granular control, secure, no data duplication</p>
<p><strong>Common Techniques:</strong> Substitution, shuffling, redaction, tokenization, hashing, encryption</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q42">
<div class="question-title">Snowflake storage integration with AWS S3</div>
<div class="answer">
<p><strong>Purpose:</strong> Secure, credential-less access to S3 from Snowflake</p>
<p><strong>Step-by-Step Setup:</strong></p>
<ol>
<li><strong>AWS IAM Policy:</strong> Grant Snowflake permissions (s3:GetObject, s3:PutObject, s3:ListBucket)</li>
<li><strong>AWS IAM Role:</strong> Create role, attach policy</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="4">
<li><strong>Get Snowflake's External ID &amp; IAM User:</strong>
<pre>DESCRIBE INTEGRATION s3_integration;</pre>
</li>
<li><strong>Update AWS IAM Role Trust Relationship:</strong> Add Snowflake's IAM user ARN and external ID</li>
<li><strong>Create External Stage:</strong></li>
</ol>
<pre>CREATE STAGE my_s3_stage
  STORAGE_INTEGRATION = s3_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<p><strong>Benefits:</strong> Enhanced security, centralized control, no exposed AWS keys</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q2">
<div class="question-title">Snowflake Architecture</div>
<div class="answer">
<p>Snowflake's architecture is a unique multi-cluster shared data architecture. It separates compute and storage, allowing them to scale independently, and includes a cloud services layer for management.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Database Storage:</strong> Data is reorganized into a columnar, compressed format, encrypted, and stored in micro-partitions. This storage layer is elastic and can scale dynamically.</li>
<li><strong>Query Processing (Compute Layer):</strong> Consists of virtual warehouses - independent compute clusters that execute queries without sharing compute resources.</li>
<li><strong>Cloud Services Layer:</strong> The brain of Snowflake, coordinating all activities including authentication, metadata management, query optimization, infrastructure management, and transaction management.</li>
</ul>
<p><strong>Analogy:</strong> Imagine a library where storage is the books on shelves, compute is the reading rooms, and cloud services is the librarian system managing everything.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q3">
<div class="question-title">Time-Travel with scenarios like offset, query_id and timestamp</div>
<div class="answer">
<p>Snowflake's Time Travel feature allows you to query historical data that has been changed or deleted within a specified retention period (default 1 day, up to 90 days for Enterprise Edition).</p>
<p><strong>Using AT (OFFSET):</strong> Query data from a specific point in time relative to current time (negative offset for past).</p>
<pre>-- Current data
SELECT * FROM my_table;

-- Data from 5 minutes (300 seconds) ago
SELECT * FROM my_table AT (OFFSET =&gt; -300);</pre>
<p><strong>Using AT (TIMESTAMP):</strong> Query data as it existed at an exact past timestamp.</p>
<pre>SELECT * FROM my_table AT (TIMESTAMP =&gt; '2025-06-25 10:30:00'::TIMESTAMP_LTZ);</pre>
<p><strong>Using BEFORE (STATEMENT):</strong> Query data immediately before a specific DML statement executed.</p>
<pre>CREATE TABLE my_table_recovery AS
SELECT * FROM my_table BEFORE (STATEMENT =&gt; '123abc456def');</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q4">
<div class="question-title">Optimize the query performance and how you do it</div>
<div class="answer">
<p><strong>Key Optimization Strategies:</strong></p>
<ol>
<li><strong>Choose the Right Virtual Warehouse Size:</strong> Larger warehouses provide more compute power and memory. Use larger warehouses for complex queries on large datasets.</li>
<li><strong>Cluster Your Tables:</strong> Define clustering keys on columns frequently used in WHERE clauses, JOIN conditions, or GROUP BY clauses.</li>
<li><strong>Use Materialized Views:</strong> Pre-compute and store results of complex queries, automatically updating when base tables change.</li>
<li><strong>Query Pruning:</strong> Ensure your WHERE clauses are effective. Filter on clustered columns or columns with high cardinality.</li>
<li><strong>Effective Caching:</strong> Leverage Snowflake's Result Cache and Warehouse Cache by running the same queries multiple times.</li>
<li><strong>Avoid Anti-Patterns:</strong> Don't use SELECT *, avoid correlated subqueries, eliminate unnecessary ORDER BY or DISTINCT.</li>
<li><strong>Monitor Query Profile:</strong> Use Snowflake's Query Profile to identify bottlenecks and optimize accordingly.</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q5">
<div class="question-title">What is partition and what is micro-partitions</div>
<div class="answer">
<p><strong>Partition (General Database Concept):</strong> A strategy to divide a large table into smaller, manageable pieces based on a specified column (e.g., date, region). Usually managed explicitly by the database administrator.</p>
<p><strong>Micro-partitions (Snowflake Specific):</strong> Snowflake automatically organizes all data into immutable, compressed, columnar units typically ranging from 50 MB to 500 MB. This is automatic and transparent.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>Automatic and Transparent - handled by Snowflake</li>
<li>Rich metadata stored about each micro-partition (value ranges, distinct values, null counts)</li>
<li>Query Pruning - metadata allows Snowflake to skip irrelevant micro-partitions</li>
<li>Clustering - you can define clustering keys to optimize physical co-location of data</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q7">
<div class="question-title">How you ingested JSON data into Snowflake, what steps you perform</div>
<div class="answer">
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>Prepare JSON Data:</strong> Ensure well-formed JSON files (individual objects or newline-delimited)</li>
<li><strong>Stage the Files:</strong> Use Internal Stage (PUT command) or External Stage (S3, Azure, GCP)</li>
<li><strong>Create File Format:</strong> Define how to interpret JSON files
                        <pre>CREATE FILE FORMAT json_file_format
    TYPE = 'JSON'
    STRIP_OUTER_ARRAY = TRUE
    NULL_IF = ('', 'NULL');</pre>
</li>
<li><strong>Create Target Table:</strong> Either single VARIANT column or structured columns
                        <pre>CREATE TABLE raw_json_data (
    id INT,
    json_payload VARIANT,
    load_timestamp TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);</pre>
</li>
<li><strong>Copy Data:</strong> Use COPY INTO to load data from stage
                        <pre>COPY INTO raw_json_data
FROM @my_external_json_stage/
FILE_FORMAT = json_file_format
ON_ERROR = 'CONTINUE';</pre>
</li>
<li><strong>Query &amp; Transform:</strong> Use VARIANT functions to extract and transform data</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q9">
<div class="question-title">Internal and External stages in Snowflake - Uses for each</div>
<div class="answer">
<p><strong>Internal Stages (Snowflake-managed Storage):</strong></p>
<ul>
<li>User stage: @~/</li>
<li>Table stage: @%table_name</li>
<li>Named stage: CREATE STAGE my_internal_stage</li>
<li>Uses: Quick data loading, secure data transfer, temporary files</li>
</ul>
<p><strong>External Stages (User-managed Cloud Storage):</strong></p>
<ul>
<li>Points to AWS S3, Azure Blob, Google Cloud Storage</li>
<li>Uses: Large-scale automated loads, data lake integration, Snowpipe, continuous loading</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Internal Stage</th>
<th>External Stage</th>
</tr>
<tr>
<td>Storage</td>
<td>Snowflake managed</td>
<td>Your cloud storage</td>
</tr>
<tr>
<td>File Access</td>
<td>PUT / GET commands</td>
<td>COPY INTO</td>
</tr>
<tr>
<td>Cost</td>
<td>Included in Snowflake</td>
<td>Cloud provider costs</td>
</tr>
<tr>
<td>Ideal Use</td>
<td>Ad-hoc, smaller loads</td>
<td>Large-scale automated</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q10">
<div class="question-title">How you schedule a data pipeline in Snowflake</div>
<div class="answer">
<p><strong>Snowflake Tasks (Native):</strong></p>
<pre>-- Create warehouse for tasks
CREATE WAREHOUSE ETL_WH WAREHOUSE_SIZE = 'XSMALL';

-- 1. Root Task (scheduled)
CREATE TASK load_raw_data
  WAREHOUSE = ETL_WH
  SCHEDULE = 'USING CRON 0 9 * * * Asia/Kolkata'
  AS
  COPY INTO RAW_TABLE FROM @my_external_stage/raw_files/;

-- 2. Child Task (depends on load_raw_data)
CREATE TASK transform_data
  WAREHOUSE = ETL_WH
  AFTER load_raw_data
  AS
  INSERT INTO STAGING_TABLE SELECT ... FROM RAW_TABLE;

-- Enable tasks
ALTER TASK load_raw_data RESUME;
ALTER TASK transform_data RESUME;</pre>
<p><strong>External Orchestration Tools:</strong> Apache Airflow, AWS Step Functions, Azure Data Factory, Control-M, Autosys - for complex cross-platform pipelines.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q29">
<div class="question-title">What is Zero Copy clone - Uses</div>
<div class="answer">
<p>Zero-Copy Cloning creates an instant copy of a database, schema, or table without physically duplicating data. Snowflake creates metadata pointers to the same micro-partitions. Changes use copy-on-write.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li>Instantaneous - created almost instantly</li>
<li>No storage cost at creation - pay only for incremental changes</li>
<li>No data movement</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>CREATE DATABASE prod_db_clone CLONE prod_db;
CREATE SCHEMA temp_schema CLONE prod_schema;
CREATE TABLE test_table CLONE prod_table;</pre>
<p><strong>Uses:</strong></p>
<ul>
<li><strong>Development/Testing:</strong> Clone production DB for developers to test without affecting prod</li>
<li><strong>Disaster Recovery:</strong> Create point-in-time recovery environment quickly</li>
<li><strong>Analytical Workloads:</strong> Isolate complex queries from main system</li>
<li><strong>Version Control:</strong> Snapshot data at milestones for rollback capability</li>
<li><strong>Historical Analysis:</strong> Create monthly snapshots with minimal cost</li>
<li><strong>What-If Scenarios:</strong> Run simulations without impacting production</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q30">
<div class="question-title">Convert timestamp from one timezone to another</div>
<div class="answer">
<p>Snowflake has three TIMESTAMP types: TIMESTAMP_LTZ (Local Time Zone), TIMESTAMP_NTZ (No Time Zone), TIMESTAMP_TZ (Time Zone).</p>
<p><strong>Using CONVERT_TIMEZONE (Recommended):</strong></p>
<pre>SELECT CONVERT_TIMEZONE('UTC', 'America/New_York', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 06:00:00.000'

SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'Asia/Kolkata', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 22:30:00.000'

-- With a column
SELECT event_id, event_timestamp,
       CONVERT_TIMEZONE('UTC', 'America/New_York', event_timestamp) AS event_timestamp_est
FROM events;</pre>
<p><strong>Using AT TIME ZONE:</strong></p>
<pre>SELECT '2025-07-04 10:00:00'::TIMESTAMP_NTZ AT TIME ZONE 'America/New_York';

-- Set session timezone first
ALTER SESSION SET TIMEZONE = 'UTC';
SELECT event_timestamp AT TIME ZONE 'Asia/Kolkata'
FROM events;</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q31">
<div class="question-title">Clustering Keys in Snowflake</div>
<div class="answer">
<p>Clustering Keys optimize the physical organization of data within micro-partitions to improve query performance.</p>
<p><strong>How They Work:</strong> Snowflake's Automatic Clustering service re-clusters data if clustering becomes stale, physically reorganizing micro-partitions to group similar values together.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Improved query performance for filtered/joined queries</li>
<li>Reduced data scanning → fewer credits consumed</li>
<li>Automatic and transparent</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>-- During creation
CREATE TABLE sales (
    event_date DATE,
    category VARCHAR,
    value DECIMAL
) CLUSTER BY (event_date, category);

-- After creation
ALTER TABLE sales CLUSTER BY (event_date, category);

-- Remove clustering
ALTER TABLE sales DROP CLUSTERING KEY;</pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Very large tables (hundreds of GB to TB)</li>
<li>Frequent filtering/joining on specific columns</li>
<li>High cardinality columns with skewed data</li>
<li>Data ingestion patterns cause poor clustering over time</li>
</ul>
<p><strong>Monitor with:</strong> SYSTEM$CLUSTERING_INFORMATION() function</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q32">
<div class="question-title">How to monitor Snowflake performance</div>
<div class="answer">
<p><strong>Snowflake Web Interface:</strong></p>
<ul>
<li>Query History - view all queries, duration, credits, status</li>
<li>Query Profile - visualize execution plan, identify bottlenecks</li>
<li>Warehouses - monitor status, usage, credit consumption</li>
</ul>
<p><strong>Account Usage Views (SNOWFLAKE.ACCOUNT_USAGE):</strong></p>
<ul>
<li>QUERY_HISTORY - comprehensive query data</li>
<li>WAREHOUSE_METERING_HISTORY - daily credit consumption per warehouse</li>
<li>AUTOMATIC_CLUSTERING_HISTORY - credits consumed by clustering</li>
<li>PIPE_USAGE_HISTORY - credits consumed by Snowpipe</li>
<li>COPY_HISTORY - COPY INTO command status</li>
<li>TASK_HISTORY - task execution history</li>
</ul>
<p><strong>External Tools:</strong></p>
<ul>
<li>Cloud monitoring (CloudWatch, Azure Monitor)</li>
<li>Third-party APM tools (Datadog, Splunk)</li>
<li>BI tools for custom dashboards</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q40">
<div class="question-title">Data Masking and Masking Policy</div>
<div class="answer">
<p><strong>Data Masking:</strong> Hiding sensitive data with fictional data while maintaining usefulness for development/testing/analytics.</p>
<p><strong>Snowflake Masking Policy:</strong> Dynamic masking at query time based on user role. Underlying data is never modified.</p>
<p><strong>Example:</strong></p>
<pre>-- Create masking policy
CREATE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR -&gt;
    CASE
        WHEN CURRENT_ROLE() IN ('ANALYST_ROLE') THEN '****'
        WHEN CURRENT_ROLE() = 'DATA_STEWARD_ROLE' THEN val
        ELSE 'No access'
    END;

-- Apply to column
ALTER TABLE users ALTER COLUMN email SET MASKING POLICY email_mask;</pre>
<p><strong>Benefits:</strong> Dynamic, centralized, granular control, secure, no data duplication</p>
<p><strong>Common Techniques:</strong> Substitution, shuffling, redaction, tokenization, hashing, encryption</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q42">
<div class="question-title">Snowflake storage integration with AWS S3</div>
<div class="answer">
<p><strong>Purpose:</strong> Secure, credential-less access to S3 from Snowflake</p>
<p><strong>Step-by-Step Setup:</strong></p>
<ol>
<li><strong>AWS IAM Policy:</strong> Grant Snowflake permissions (s3:GetObject, s3:PutObject, s3:ListBucket)</li>
<li><strong>AWS IAM Role:</strong> Create role, attach policy</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="4">
<li><strong>Get Snowflake's External ID &amp; IAM User:</strong>
<pre>DESCRIBE INTEGRATION s3_integration;</pre>
</li>
<li><strong>Update AWS IAM Role Trust Relationship:</strong> Add Snowflake's IAM user ARN and external ID</li>
<li><strong>Create External Stage:</strong></li>
</ol>
<pre>CREATE STAGE my_s3_stage
  STORAGE_INTEGRATION = s3_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<p><strong>Benefits:</strong> Enhanced security, centralized control, no exposed AWS keys</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div></div></body></html>