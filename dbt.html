<!DOCTYPE html>
<html><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Complete Snowflake &amp; Data Engineering Interview Guide - 74 Questions</title>

</head><body><style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f9f9f9;
        }
        
        
        
        header {
            border-bottom: 3px solid #007acc;
            padding-bottom: 20px;
            margin-bottom: 40px;
        }
        
        h1 {
            color: #007acc;
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .subtitle {
            color: #666;
            font-size: 1.1em;
        }

        /* Table of Contents Styling */
        .toc-container {
            background-color: #f0f7ff;
            border: 2px solid #007acc;
            border-radius: 8px;
            padding: 30px;
            margin-bottom: 50px;
        }

        .toc-container h2 {
            color: #007acc;
            font-size: 1.8em;
            margin-bottom: 25px;
            border-bottom: 2px solid #007acc;
            padding-bottom: 10px;
        }

        .toc-sections {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin-top: 20px;
        }

        .toc-section {
            background-color: white;
            border-left: 4px solid #0066cc;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .toc-section h3 {
            color: #0066cc;
            font-size: 1.2em;
            margin-bottom: 15px;
            padding-bottom: 8px;
            border-bottom: 1px solid #e0e0e0;
        }

        .toc-section ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .toc-section li {
            margin-bottom: 10px;
        }

        .toc-section a {
            color: #0066cc;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
        }

        .toc-section a:hover {
            color: #007acc;
            text-decoration: underline;
        }

        .toc-question-number {
            color: #e74c3c;
            font-weight: bold;
            margin-right: 5px;
        }

        .back-to-toc {
            display: inline-block;
            margin-top: 20px;
            padding: 8px 15px;
            background-color: #007acc;
            color: white;
            border-radius: 4px;
            text-decoration: none;
            font-size: 0.9em;
            transition: background-color 0.3s ease;
        }

        .back-to-toc:hover {
            background-color: #0066cc;
        }
        
        .question {
            margin-bottom: 40px;
            border-left: 4px solid #007acc;
            padding-left: 20px;
            scroll-margin-top: 100px;
        }
        
        .question-number {
            font-size: 1.1em;
            color: #e74c3c;
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .question-title {
            font-size: 1.4em;
            color: #2980b9;
            margin: 10px 0;
            font-weight: 600;
        }
        
        .answer {
            margin-top: 15px;
            color: #333;
            line-height: 1.8;
        }
        
        .answer p {
            margin-bottom: 12px;
        }
        
        pre {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.4;
        }
        
        code {
            background-color: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        strong {
            color: #0066cc;
            font-weight: 600;
        }
        
        ul, ol {
            margin: 15px 0 15px 30px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .highlight {
            background-color: #e7f3ff;
            padding: 15px;
            border-left: 3px solid #0066cc;
            margin: 15px 0;
            border-radius: 3px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: #f9f9f9;
        }
        
        table th {
            background-color: #007acc;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        
        table td {
            padding: 12px;
            border-bottom: 1px solid #ddd;
        }
        
        table tr:hover {
            background-color: #f0f0f0;
        }
        
        @media print {
            body {
                background-color: white;
            }
            
            h1 {
                font-size: 1.8em;
                page-break-after: avoid;
            }
            .question {
                page-break-inside: avoid;
            }
            pre {
                font-size: 8pt;
                padding: 10px;
            }
            code {
                font-size: 8pt;
            }
        }
    
html { scroll-behavior: smooth; }
.question-title { color: #8e44ad !important; font-weight: 700; }

/* ---------------- Responsive Enhancements ---------------- */


pre {
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-x: auto;
}

code {
  word-break: break-word;
}



@media (min-width: 768px) {
  
}

@media (min-width: 1024px) {
  
}

@media (max-width: 767px) {
  body {
    font-size: 15px;
  }

  h1 {
    font-size: 1.6em;
  }

  .question-title {
    font-size: 1.15em;
  }

  .question {
    padding-left: 15px;
  }

  .back-to-toc {
    font-size: 0.8em;
    padding: 6px 10px;
  }
}

/* -------- TOC Layout Control -------- */
.toc-container ul {
  columns: 1;
}

@media (min-width: 1024px) {
  .toc-container ul {
    columns: 2;
  }
}
.container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
</style><div class="container"><h1>Dbt</h1><div class="question" id="q13">
<div class="question-title">What is Git</div>
<div class="answer">
<p>Git is a free, open-source distributed version control system (DVCS) designed to handle projects of any size. It tracks changes in source code and other files during software development, enabling collaborative work and complete change history.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Distributed:</strong> Every developer has a complete copy of the entire repository with full history. No single point of failure</li>
<li><strong>Version Control:</strong> Tracks every change with author, timestamp, and message. Complete history allows reverting to any previous state</li>
<li><strong>Branches:</strong> Parallel lines of development (main, develop, feature branches) allowing teams to work independently</li>
<li><strong>Commits:</strong> Snapshots of the entire repository at specific points with unique SHA-1 hash identifiers</li>
<li><strong>Repository:</strong> Complete collection of files, branches, and entire change history (can be local or on remote servers like GitHub)</li>
<li><strong>Staging Area (Index):</strong> Intermediate area where you select which changes to include in the next commit</li>
</ul>
<p><strong>Typical Workflow:</strong></p>
<pre>git clone https://github.com/user/repo.git  # Clone remote repo
git branch feature/new-analysis              # Create feature branch
git checkout feature/new-analysis             # Switch to feature branch
git add analysis_script.sql                   # Stage changes
git commit -m "Add new sales analysis"        # Commit with message
git push origin feature/new-analysis          # Push to remote
# Create pull request on GitHub for review
git checkout main                             # Switch back to main
git pull origin main                          # Get latest main
git merge feature/new-analysis                # Merge feature branch</pre>
<p><strong>For Data Teams:</strong> Git is essential for version controlling DBT projects, SQL scripts, Python ETL code, and documentation. Enables code review, audit trails, and rollback capabilities</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q15">
<div class="question-title">Benefits of Snowflake</div>
<div class="answer">
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Separation of Compute and Storage:</strong> Scale independently - add storage without compute cost. Run small XS warehouse for lightweight queries, large 3X warehouse for complex jobs. Only pay for compute when warehouse is running</li>
<li><strong>Elasticity and Scalability:</strong> Instantly resize warehouses (XS to 3X in seconds) or auto-scale with multi-cluster warehouses for concurrency</li>
<li><strong>Near-Zero Management:</strong> Fully managed - Snowflake handles patching, updates, hardware provisioning, index maintenance. No DBAs needed for infrastructure</li>
<li><strong>Support for Semi-Structured Data:</strong> Native VARIANT type handles JSON/Avro/Parquet natively. Query nested data with dot notation without flattening</li>
<li><strong>Concurrency:</strong> Multiple independent virtual warehouses access same data simultaneously without locking or contention. BI team on WH1, ETL team on WH2, no impact</li>
<li><strong>Data Sharing:</strong> Secure, instantaneous live data sharing with other Snowflake accounts (same or different regions). Share without copying data</li>
<li><strong>Time Travel &amp; Fail-safe:</strong> Query data as it existed 1-90 days ago. Accidentally dropped table? UNDROP within retention period. 7-day fail-safe for disaster recovery</li>
<li><strong>Performance Optimization:</strong> Automatic micro-partitioning with intelligent pruning, multi-layer caching (result + warehouse cache), optional clustering keys</li>
<li><strong>Security &amp; Compliance:</strong> AES-256 encryption at rest/transit, multi-factor MFA, RBAC + object-level privileges, row/column masking, SOC2/PCI/HIPAA/GDPR certified</li>
<li><strong>Ecosystem Integration:</strong> Native connectors for Tableau, Power BI, Looker. DBT integration, Spark through connectors, Python/Pandas via snowpark_python</li>
</ul>
<p><strong>Cost Efficiency Example:</strong> Traditional data warehouse requires expensive hardware, undergoes periods of low utilization (pay for unused capacity). Snowflake: suspend warehouse when not used, pay only for storage. Scale up for batch jobs, down immediately after</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q17">
<div class="question-title">Python questions - List and Tuple</div>
<div class="answer">
<p><strong>List []: Mutable (changeable)</strong> - ordered collection that can be modified after creation</p>
<pre>my_list = [10, "apple", 3.14, True, [1, 2, 3]]  # Can contain mixed types, even nested lists
print(my_list[1])              # Output: apple (zero-indexed)
my_list[0] = 20                # Modify element
my_list.append("banana")       # Add element
my_list.extend([4, 5, 6])      # Add multiple elements
my_list.insert(1, "orange")    # Insert at specific position
my_list.remove("apple")        # Remove by value
popped = my_list.pop()         # Remove and return last element
my_list.sort()                 # Sort in-place
my_list.clear()                # Remove all elements

# Common operations
for item in my_list:
    print(item)  # Iterate

sliced = my_list[1:4]  # Slicing creates new list
reversed_list = my_list[::-1]  # Reverse</pre>
<p><strong>Tuple (): Immutable (unchangeable)</strong> - ordered collection that cannot be modified after creation</p>
<pre>my_tuple = (10, "apple", 3.14, True, (1, 2, 3))  # Can contain mixed types
print(my_tuple[1])           # Output: apple
# my_tuple[0] = 20           # TypeError! Tuples are immutable

# Single element tuple REQUIRES trailing comma
single_tuple = (5,)          # Correct
wrong_single = (5)           # This is just an int, not a tuple!

# Tuple packing and unpacking
coordinates = 10, 20, 30     # Automatic packing into tuple
x, y, z = coordinates        # Unpacking tuple to variables

# Tuples can be used as dictionary keys (lists cannot)
my_dict = {(0, 0): "origin", (1, 1): "diagonal"}

# Operations (read-only)
print(my_tuple.count(10))     # Count occurrences
print(my_tuple.index("apple"))  # Find index
print(len(my_tuple))          # Length</pre>
<table>
<tr>
<th>Feature</th>
<th>List</th>
<th>Tuple</th>
</tr>
<tr>
<td>Mutability</td>
<td>Mutable (changeable)</td>
<td>Immutable (fixed)</td>
</tr>
<tr>
<td>Performance</td>
<td>Slower (tracking changes)</td>
<td>Faster (fixed size)</td>
</tr>
<tr>
<td>Memory</td>
<td>More memory overhead</td>
<td>Less memory usage</td>
</tr>
<tr>
<td>As Dict Key</td>
<td>No - not hashable</td>
<td>Yes - if elements are hashable</td>
</tr>
<tr>
<td>Best Use Case</td>
<td>Dynamic data, frequent changes</td>
<td>Fixed data, constant values</td>
</tr>
<tr>
<td>Return from Function</td>
<td>N/A</td>
<td>Yes - functions return tuples</td>
</tr>
</table>
<p><strong>In Data Engineering:</strong> Tuples used for immutable data records, fixed field orders. Lists used for accumulating/processing data. DBT source definitions, data lineage metadata often use tuples</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q24">
<div class="question-title">What is DBT and uses - Execution plan</div>
<div class="answer">
<p>DBT (Data Build Tool) is an open-source framework that enables data analysts to transform data in their warehouse using SQL and software engineering best practices. It focuses on the T in ELT.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>SQL-centric transformations</li>
<li>Modularity and reusability</li>
<li>Version control with Git</li>
<li>Built-in data quality tests</li>
<li>Auto-generated documentation</li>
<li>Dependency graph (DAG)</li>
<li>Jinja templating for dynamic SQL</li>
</ul>
<p><strong>DBT Execution Plan (dbt run):</strong></p>
<ol>
<li><strong>Parsing:</strong> Read .sql files, parse Jinja templates, resolve ref() functions</li>
<li><strong>DAG Building:</strong> Create dependency graph of all models</li>
<li><strong>Dependency Resolution:</strong> Determine execution order</li>
<li><strong>Materialization:</strong> Decide how to build each model (view, table, incremental, ephemeral)</li>
<li><strong>SQL Generation &amp; Execution:</strong> Generate SQL, execute in warehouse in correct order</li>
<li><strong>Post-Run Actions:</strong> Run tests, generate documentation</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q26">
<div class="question-title">Explain end-to-end data pipeline - What are the logics you followed</div>
<div class="answer">
<p><strong>Typical Stages:</strong></p>
<ol>
<li><strong>Data Sources:</strong> Operational databases, SaaS apps, APIs, logs (identify what data, how to access, frequency)</li>
<li><strong>Ingestion/Extraction:</strong> Batch or streaming (full load vs incremental CDC)</li>
<li><strong>Landing/Staging:</strong> Raw data storage with schema-on-read approach</li>
<li><strong>Transformation:</strong> Clean, enrich, conform, aggregate (using DBT, SQL, or external engines)</li>
<li><strong>Serving/Presentation:</strong> Optimized views/tables for consumption</li>
<li><strong>Consumption:</strong> BI tools, dashboards, data science, applications</li>
</ol>
<p><strong>Cross-cutting Concerns:</strong></p>
<ul>
<li><strong>Orchestration:</strong> Schedule and manage pipeline stages (Airflow, Tasks, Step Functions)</li>
<li><strong>Monitoring &amp; Alerting:</strong> Track pipeline health, errors, data quality</li>
<li><strong>Data Lineage:</strong> Understand data flow, debug issues</li>
<li><strong>Cost Management:</strong> Monitor and optimize cloud resource usage</li>
<li><strong>Version Control:</strong> Git for all code (SQL, Python, DBT)</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q38">
<div class="question-title">DBT Project Architecture</div>
<div class="answer">
<p><strong>Typical DBT Project Structure:</strong></p>
<pre>my_dbt_project/
├── dbt_project.yml         # Core configuration
├── models/                 # Data transformation models
│   ├── staging/           # Raw, light transformations
│   ├── intermediate/      # Complex joins, business logic
│   └── marts/             # Final, user-facing models
├── analysis/              # Ad-hoc SQL queries
├── macros/                # Reusable SQL snippets
├── seeds/                 # Static CSV lookup tables
├── snapshots/             # SCD Type 2 configurations
├── tests/                 # Custom data quality tests
├── logs/                  # DBT execution logs
└── target/                # Compiled SQL, manifests</pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>dbt_project.yml:</strong> Project configuration, default materializations</li>
<li><strong>Models:</strong> SQL SELECT statements - heart of DBT project</li>
<li><strong>Tests:</strong> Data quality validations (not_null, unique, accepted_values, custom)</li>
<li><strong>Macros:</strong> Reusable Jinja templates for dynamic SQL</li>
<li><strong>Seeds:</strong> Static CSV files loaded as tables</li>
<li><strong>Snapshots:</strong> Capture historical changes (SCD Type 2)</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q43">
<div class="question-title">What is DBT and what problem does it solve in modern data stack</div>
<div class="answer">
<p>DBT (data build tool) is a command-line framework for transforming data in your warehouse with modular SQL and software engineering best practices.</p>
<p><strong>Problems it Solves:</strong></p>
<ul>
<li><strong>Code Proliferation:</strong> Centralizes transformation logic instead of scattered ETL scripts/BI tools</li>
<li><strong>Lack of Best Practices:</strong> Brings version control, modularity, testing, documentation to data work</li>
<li><strong>Dependency Management:</strong> Automatically determines correct model execution order</li>
<li><strong>Testing &amp; Quality:</strong> Provides framework for data quality tests and auto-documentation</li>
<li><strong>Transparency:</strong> Makes entire transformation process transparent and auditable</li>
</ul>
<p><strong>Core Philosophy:</strong> Leverage the power of the data warehouse itself (pushing computations) rather than using separate processing engines</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q44">
<div class="question-title">Core components of a DBT project: Models, Tests, Seeds</div>
<div class="answer">
<p><strong>Models:</strong> SQL SELECT statements organized into transformation logic</p>
<ul>
<li>Each .sql file = one model</li>
<li>Output: view or table (depends on materialization)</li>
<li>Example: Model calculating monthly sales from raw transactions</li>
</ul>
<p><strong>Tests:</strong> Data quality checks ensuring data integrity</p>
<ul>
<li><strong>Singular Tests:</strong> Custom SQL returning failing rows if condition not met</li>
<li><strong>Generic Tests:</strong> Pre-defined tests applied via YAML (not_null, unique, accepted_values)</li>
<li>Purpose: Prevent downstream errors, ensure data reliability</li>
</ul>
<p><strong>Seeds:</strong> Static CSV files loaded as reference/lookup tables</p>
<ul>
<li>Small, infrequently-changing data (country codes, mapping tables)</li>
<li>Loaded via dbt seed command</li>
</ul>
<p><strong>Macros (Bonus):</strong> Reusable Jinja+SQL code snippets (like functions)</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q45">
<div class="question-title">Materializations in DBT - Four default types</div>
<div class="answer">
<p><strong>View (Default):</strong></p>
<ul>
<li>Mechanism: Creates SQL view</li>
<li>Pros: Always latest data, fast compile</li>
<li>Cons: Slow queries (logic runs every time)</li>
<li>Best for: Simple transformations, rarely-queried models</li>
</ul>
<p><strong>Table:</strong></p>
<ul>
<li>Mechanism: Creates persistent table with CREATE TABLE AS</li>
<li>Pros: Fast queries (pre-calculated)</li>
<li>Cons: Slow to build, consumes storage</li>
<li>Best for: Complex/frequently-queried models, large datasets</li>
</ul>
<p><strong>Incremental:</strong></p>
<ul>
<li>Mechanism: INSERT/MERGE only new/changed records</li>
<li>Pros: Very fast after initial build</li>
<li>Cons: Complex design, prone to errors</li>
<li>Best for: Large fact tables with incremental changes</li>
</ul>
<p><strong>Ephemeral:</strong></p>
<ul>
<li>Mechanism: No physical object, compiles to CTE</li>
<li>Pros: Excellent modularity</li>
<li>Cons: Not queryable directly</li>
<li>Best for: Simple intermediate cleaning steps</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q46">
<div class="question-title">Managing dependencies and referencing models in DBT</div>
<div class="answer">
<p><strong>The ref() Function:</strong> Reference models without hardcoding schema names</p>
<pre>-- Instead of FROM SCHEMA.TABLE
SELECT * FROM {{ ref('stg_orders') }}
WHERE is_valid = TRUE</pre>
<p><strong>How it Works:</strong></p>
<ul>
<li>{{ ref('stg_orders') }} replaced with correct schema/table name during compilation</li>
<li>DBT automatically knows stg_orders must be built before this model</li>
<li>Creates dependency edge in DAG</li>
</ul>
<p><strong>Automatic DAG Generation:</strong></p>
<ul>
<li>DBT scans all models and their ref() calls</li>
<li>Builds DAG showing ALL dependencies</li>
<li>dbt run executes models in correct order, ensuring source tables exist before use</li>
</ul>
<p><strong>Benefits:</strong> No manual ordering, automatic parallelization, clear dependencies</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q47">
<div class="question-title">What is Jinja in DBT - Simple example</div>
<div class="answer">
<p>Jinja is a templating language that adds programming logic to SQL. Processed before SQL reaches the warehouse.</p>
<p><strong>Capabilities:</strong> if/else statements, variables, loops, macro reuse, dynamic SQL</p>
<p><strong>Simple Example:</strong></p>
<pre>{% set limit_date = '2023-01-01' %}

SELECT * FROM {{ ref('raw_data') }}
WHERE created_at &gt;= '{{ limit_date }}'

{% if target.name == 'prod' %}
  AND is_active = TRUE
{% endif %}</pre>
<p><strong>Compiled SQL (Development):</strong></p>
<pre>SELECT * FROM my_dev_schema.raw_data
WHERE created_at &gt;= '2023-01-01'</pre>
<p><strong>Compiled SQL (Production):</strong></p>
<pre>SELECT * FROM my_prod_schema.raw_data
WHERE created_at &gt;= '2023-01-01'
  AND is_active = TRUE</pre>
<p><strong>Benefits:</strong> DRY (Don't Repeat Yourself), environment-specific logic, dynamic SQL generation</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q48">
<div class="question-title">Data Governance best practices</div>
<div class="answer">
<p><strong>Key Best Practices:</strong></p>
<ol>
<li><strong>Define Clear Ownership:</strong> Assign data owners and stewards for each domain/dataset</li>
<li><strong>Establish Data Dictionary:</strong> Document all data elements, definitions, relationships</li>
<li><strong>Implement Metadata Management:</strong> Track technical metadata (schema, lineage) and business metadata</li>
<li><strong>Quality Standards:</strong> Implement data quality rules, monitoring, and remediation processes</li>
<li><strong>Access Control:</strong> RBAC, row-level security, column-level masking</li>
<li><strong>Data Lineage:</strong> Understand data flow from source to consumption</li>
<li><strong>Compliance &amp; Auditing:</strong> Track data usage, audit logs, regulatory compliance (GDPR, HIPAA)</li>
<li><strong>Version Control:</strong> Version all code (SQL, DBT, Python) in Git</li>
<li><strong>Documentation:</strong> Auto-generate and maintain docs (DBT, Collibra, Ataccama)</li>
<li><strong>Collaboration:</strong> Foster data culture, communication between teams</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q49">
<div class="question-title">Data quality testing strategies</div>
<div class="answer">
<p><strong>Common Testing Approaches:</strong></p>
<ol>
<li><strong>Schema Tests:</strong> Validate data types, constraints
                        <ul>
<li><strong>not_null:</strong> Column should not have NULL values</li>
<li><strong>unique:</strong> Column values must be distinct</li>
<li><strong>relationships:</strong> Foreign key relationships</li>
<li><strong>accepted_values:</strong> Column values from allowed set</li>
</ul>
</li>
<li><strong>Statistical Tests:</strong> Validate data distributions, outliers
                        <ul>
<li>Range checks</li>
<li>Distribution analysis</li>
<li>Outlier detection</li>
</ul>
</li>
<li><strong>Business Logic Tests:</strong> Validate business rules
                        <ul>
<li>Cross-table consistency</li>
<li>Aggregation accuracy</li>
<li>Recency checks</li>
</ul>
</li>
<li><strong>Freshness Tests:</strong> Ensure data is current
                        <ul>
<li>Last update timestamp</li>
<li>Row count expectations</li>
</ul>
</li>
</ol>
<p><strong>Tools:</strong> DBT tests, Great Expectations, Ataccama, custom SQL checks</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q50">
<div class="question-title">Building a modern data stack - Technology selection</div>
<div class="answer">
<p><strong>Typical Modern Data Stack Layers:</strong></p>
<ul>
<li><strong>Source Systems:</strong> Operational databases, SaaS applications, APIs</li>
<li><strong>Ingestion:</strong> Fivetran, Stitch, Airbyte, AWS DMS, Qlik Replicate</li>
<li><strong>Cloud Data Warehouse:</strong> Snowflake, BigQuery, Redshift</li>
<li><strong>Transformation:</strong> DBT, Spark, Python</li>
<li><strong>Data Governance:</strong> Collibra, Ataccama, DataHub</li>
<li><strong>Analytics/BI:</strong> Tableau, Power BI, Looker</li>
<li><strong>Orchestration:</strong> Airflow, Prefect, Dagster</li>
<li><strong>Reverse ETL:</strong> Hightouch, Census (sync data back to source systems)</li>
</ul>
<p><strong>Selection Criteria:</strong></p>
<ul>
<li>Scalability and performance</li>
<li>Cost efficiency</li>
<li>Ease of use / learning curve</li>
<li>Integration with existing tools</li>
<li>Community and support</li>
<li>Security and compliance requirements</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q51">
<div class="question-title">Future trends in data engineering</div>
<div class="answer">
<p><strong>Emerging Trends:</strong></p>
<ul>
<li><strong>DataOps:</strong> Applying DevOps principles to data pipelines (CI/CD, monitoring, automation)</li>
<li><strong>Real-time Analytics:</strong> Move beyond batch to streaming/real-time data pipelines</li>
<li><strong>AI/ML Integration:</strong> ML models integrated into data pipelines, automated feature engineering</li>
<li><strong>Data Mesh:</strong> Decentralized data ownership model, domain-oriented data architecture</li>
<li><strong>Lakehouse Architecture:</strong> Combining data lake flexibility with warehouse performance (Delta Lake, Iceberg, Hudi)</li>
<li><strong>Cloud-Native:</strong> Serverless data platforms, compute separation from storage</li>
<li><strong>Data Quality as First-Class:</strong> Increased focus on data quality, governance, and observability</li>
<li><strong>Self-Service Analytics:</strong> Tools enabling business users to perform analysis without technical expertise</li>
<li><strong>Reverse ETL:</strong> Syncing aggregated/enriched data back to operational systems</li>
<li><strong>Privacy-Preserving Technologies:</strong> Differential privacy, federated learning for sensitive data</li>
</ul>
<p><strong>Skills for Future:</strong> Cloud platforms (AWS/Azure/GCP), DBT, SQL, Python, Spark, containers, Kubernetes, ML basics</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q58">
<div class="question-title">dbt Materializations — Syntax &amp; When to Use</div>
<div class="answer">
<p><strong>Materialization defines how dbt builds a model in the warehouse.</strong></p>
<p><strong>1. view — Syntax</strong></p>
<pre>{{ config(materialized='view') }}

select *
from {{ ref('stg_orders') }}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Creates a Snowflake VIEW (no data stored)</li>
<li>Always reflects latest source data</li>
</ul>
<p><strong>When to use</strong></p>
<ul>
<li>Staging models, light transformations, fast iteration</li>
</ul>
<p><strong>2. table — Syntax</strong></p>
<pre>{{ config(materialized='table') }}

select *
from {{ ref('int_sales') }}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Creates a physical table rebuilt completely on every run</li>
</ul>
<p><strong>3. incremental — Syntax (MOST IMPORTANT)</strong></p>
<pre>{{ config(
        materialized='incremental',
        unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}

{% if is_incremental() %}
    where updated_at &gt; (select max(updated_at) from {{ this }})
{% endif %}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Loads only new or changed data — saves cost and time</li>
</ul>
<p><strong>4. ephemeral — Syntax</strong></p>
<pre>{{ config(materialized='ephemeral') }}

select
    order_id,
    sum(amount) as total_amount
from {{ ref('stg_payments') }}
group by order_id</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>No table/view — SQL is inlined into downstream models</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q59">
<div class="question-title">dbt Snapshots — Syntax &amp; Purpose</div>
<div class="answer">
<p><strong>Snapshots are used for SCD Type 2.</strong></p>
<p><strong>Timestamp strategy</strong></p>
<pre>{% snapshot dim_customer_snapshot %}
{{
    config(
        target_schema='snapshots',
        unique_key='customer_id',
        strategy='timestamp',
        updated_at='updated_at',
        invalidate_hard_deletes=true
    )
}}

select *
from {{ ref('stg_customers') }}

{% endsnapshot %}</pre>
<p><strong>What dbt adds</strong></p>
<ul>
<li><code>dbt_valid_from</code>, <code>dbt_valid_to</code> and history tracking</li>
</ul>
<p><strong>Check strategy (track specific cols)</strong></p>
<pre>{% snapshot dim_product_snapshot %}
{{
    config(
        target_schema='snapshots',
        unique_key='product_id',
        strategy='check',
        check_cols=['price', 'status']
    )
}}

select *
from {{ ref('stg_products') }}

{% endsnapshot %}</pre>
<p><strong>Use when</strong></p>
<ul>
<li>Timestamp: source has reliable <code>updated_at</code></li>
<li>Check: no reliable timestamp — track changes on specific columns</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q60">
<div class="question-title">dbt YAML Files — Syntax, Indentation &amp; Purpose</div>
<div class="answer">
<p><strong>YAML defines metadata, not SQL logic.</strong></p>
<p><strong>schema.yml (example)</strong></p>
<pre>version: 2
                                <pre>version: 2

models:
    - name: stg_orders
        description: "Cleaned orders data"
        columns:
            - name: order_id
                description: "Primary key"
                tests:
                    - not_null
                    - unique</pre>

                                <p><strong>Indentation rules</strong></p>
                                <p>YAML breaks if indentation is wrong — use two spaces only.</p>

                                <p><strong>Source YAML (example)</strong></p>
                                <pre>version: 2

sources:
    - name: sales
        database: raw
        schema: public
        tables:
            - name: orders
                description: "Raw orders table"</pre>

                                <p><strong>Snapshot &amp; Seeds YAML</strong></p>
                                <pre>version: 2

snapshots:
    - name: dim_customer_snapshot
        description: "Customer SCD Type 2 history"</pre>

                                <pre>version: 2

seeds:
    - name: country_codes
        description: "ISO country codes"
        columns:
            - name: country_code
                tests:
                    - not_null</pre>
                        <a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</pre></div>
</div><div class="question" id="q62">
<div class="question-title">Materialization Configuration via dbt_project.yml</div>
<div class="answer">
<p><strong>Example</strong></p>
<pre>models:
    my_project:
        staging:
            +materialized: view
        marts:
            +materialized: incremental</pre>
<p><strong>Why this matters</strong></p>
<ul>
<li>Enforces standards and avoids per-model config</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q63">
<div class="question-title">Quick Interview Cheat Table &amp; Power Lines</div>
<div class="answer">
<p><strong>Cheat table</strong></p>
<table>
<tr><th>Feature</th><th>Used For</th></tr>
<tr><td>view</td><td>Staging</td></tr>
<tr><td>table</td><td>Small dimensions</td></tr>
<tr><td>incremental</td><td>Large facts</td></tr>
<tr><td>ephemeral</td><td>Reusable logic</td></tr>
<tr><td>snapshot</td><td>SCD Type 2</td></tr>
<tr><td>schema.yml</td><td>Tests &amp; docs</td></tr>
<tr><td>sources.yml</td><td>Source metadata</td></tr>
<tr><td>seeds</td><td>Static reference data</td></tr>
</table>
<p><strong>Interview Power Lines</strong></p>
<ul>
<li>“dbt handles transformations, Airflow handles orchestration.”</li>
<li>“Incremental models are critical for cost control in Snowflake.”</li>
<li>“Snapshots are the cleanest way to implement SCD Type 2.”</li>
<li>“YAML defines what should be true, SQL defines how data is built.”</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q13">
<div class="question-title">What is Git</div>
<div class="answer">
<p>Git is a free, open-source distributed version control system (DVCS) designed to handle projects of any size. It tracks changes in source code and other files during software development, enabling collaborative work and complete change history.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Distributed:</strong> Every developer has a complete copy of the entire repository with full history. No single point of failure</li>
<li><strong>Version Control:</strong> Tracks every change with author, timestamp, and message. Complete history allows reverting to any previous state</li>
<li><strong>Branches:</strong> Parallel lines of development (main, develop, feature branches) allowing teams to work independently</li>
<li><strong>Commits:</strong> Snapshots of the entire repository at specific points with unique SHA-1 hash identifiers</li>
<li><strong>Repository:</strong> Complete collection of files, branches, and entire change history (can be local or on remote servers like GitHub)</li>
<li><strong>Staging Area (Index):</strong> Intermediate area where you select which changes to include in the next commit</li>
</ul>
<p><strong>Typical Workflow:</strong></p>
<pre>git clone https://github.com/user/repo.git  # Clone remote repo
git branch feature/new-analysis              # Create feature branch
git checkout feature/new-analysis             # Switch to feature branch
git add analysis_script.sql                   # Stage changes
git commit -m "Add new sales analysis"        # Commit with message
git push origin feature/new-analysis          # Push to remote
# Create pull request on GitHub for review
git checkout main                             # Switch back to main
git pull origin main                          # Get latest main
git merge feature/new-analysis                # Merge feature branch</pre>
<p><strong>For Data Teams:</strong> Git is essential for version controlling DBT projects, SQL scripts, Python ETL code, and documentation. Enables code review, audit trails, and rollback capabilities</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q15">
<div class="question-title">Benefits of Snowflake</div>
<div class="answer">
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Separation of Compute and Storage:</strong> Scale independently - add storage without compute cost. Run small XS warehouse for lightweight queries, large 3X warehouse for complex jobs. Only pay for compute when warehouse is running</li>
<li><strong>Elasticity and Scalability:</strong> Instantly resize warehouses (XS to 3X in seconds) or auto-scale with multi-cluster warehouses for concurrency</li>
<li><strong>Near-Zero Management:</strong> Fully managed - Snowflake handles patching, updates, hardware provisioning, index maintenance. No DBAs needed for infrastructure</li>
<li><strong>Support for Semi-Structured Data:</strong> Native VARIANT type handles JSON/Avro/Parquet natively. Query nested data with dot notation without flattening</li>
<li><strong>Concurrency:</strong> Multiple independent virtual warehouses access same data simultaneously without locking or contention. BI team on WH1, ETL team on WH2, no impact</li>
<li><strong>Data Sharing:</strong> Secure, instantaneous live data sharing with other Snowflake accounts (same or different regions). Share without copying data</li>
<li><strong>Time Travel &amp; Fail-safe:</strong> Query data as it existed 1-90 days ago. Accidentally dropped table? UNDROP within retention period. 7-day fail-safe for disaster recovery</li>
<li><strong>Performance Optimization:</strong> Automatic micro-partitioning with intelligent pruning, multi-layer caching (result + warehouse cache), optional clustering keys</li>
<li><strong>Security &amp; Compliance:</strong> AES-256 encryption at rest/transit, multi-factor MFA, RBAC + object-level privileges, row/column masking, SOC2/PCI/HIPAA/GDPR certified</li>
<li><strong>Ecosystem Integration:</strong> Native connectors for Tableau, Power BI, Looker. DBT integration, Spark through connectors, Python/Pandas via snowpark_python</li>
</ul>
<p><strong>Cost Efficiency Example:</strong> Traditional data warehouse requires expensive hardware, undergoes periods of low utilization (pay for unused capacity). Snowflake: suspend warehouse when not used, pay only for storage. Scale up for batch jobs, down immediately after</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q17">
<div class="question-title">Python questions - List and Tuple</div>
<div class="answer">
<p><strong>List []: Mutable (changeable)</strong> - ordered collection that can be modified after creation</p>
<pre>my_list = [10, "apple", 3.14, True, [1, 2, 3]]  # Can contain mixed types, even nested lists
print(my_list[1])              # Output: apple (zero-indexed)
my_list[0] = 20                # Modify element
my_list.append("banana")       # Add element
my_list.extend([4, 5, 6])      # Add multiple elements
my_list.insert(1, "orange")    # Insert at specific position
my_list.remove("apple")        # Remove by value
popped = my_list.pop()         # Remove and return last element
my_list.sort()                 # Sort in-place
my_list.clear()                # Remove all elements

# Common operations
for item in my_list:
    print(item)  # Iterate

sliced = my_list[1:4]  # Slicing creates new list
reversed_list = my_list[::-1]  # Reverse</pre>
<p><strong>Tuple (): Immutable (unchangeable)</strong> - ordered collection that cannot be modified after creation</p>
<pre>my_tuple = (10, "apple", 3.14, True, (1, 2, 3))  # Can contain mixed types
print(my_tuple[1])           # Output: apple
# my_tuple[0] = 20           # TypeError! Tuples are immutable

# Single element tuple REQUIRES trailing comma
single_tuple = (5,)          # Correct
wrong_single = (5)           # This is just an int, not a tuple!

# Tuple packing and unpacking
coordinates = 10, 20, 30     # Automatic packing into tuple
x, y, z = coordinates        # Unpacking tuple to variables

# Tuples can be used as dictionary keys (lists cannot)
my_dict = {(0, 0): "origin", (1, 1): "diagonal"}

# Operations (read-only)
print(my_tuple.count(10))     # Count occurrences
print(my_tuple.index("apple"))  # Find index
print(len(my_tuple))          # Length</pre>
<table>
<tr>
<th>Feature</th>
<th>List</th>
<th>Tuple</th>
</tr>
<tr>
<td>Mutability</td>
<td>Mutable (changeable)</td>
<td>Immutable (fixed)</td>
</tr>
<tr>
<td>Performance</td>
<td>Slower (tracking changes)</td>
<td>Faster (fixed size)</td>
</tr>
<tr>
<td>Memory</td>
<td>More memory overhead</td>
<td>Less memory usage</td>
</tr>
<tr>
<td>As Dict Key</td>
<td>No - not hashable</td>
<td>Yes - if elements are hashable</td>
</tr>
<tr>
<td>Best Use Case</td>
<td>Dynamic data, frequent changes</td>
<td>Fixed data, constant values</td>
</tr>
<tr>
<td>Return from Function</td>
<td>N/A</td>
<td>Yes - functions return tuples</td>
</tr>
</table>
<p><strong>In Data Engineering:</strong> Tuples used for immutable data records, fixed field orders. Lists used for accumulating/processing data. DBT source definitions, data lineage metadata often use tuples</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q24">
<div class="question-title">What is DBT and uses - Execution plan</div>
<div class="answer">
<p>DBT (Data Build Tool) is an open-source framework that enables data analysts to transform data in their warehouse using SQL and software engineering best practices. It focuses on the T in ELT.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>SQL-centric transformations</li>
<li>Modularity and reusability</li>
<li>Version control with Git</li>
<li>Built-in data quality tests</li>
<li>Auto-generated documentation</li>
<li>Dependency graph (DAG)</li>
<li>Jinja templating for dynamic SQL</li>
</ul>
<p><strong>DBT Execution Plan (dbt run):</strong></p>
<ol>
<li><strong>Parsing:</strong> Read .sql files, parse Jinja templates, resolve ref() functions</li>
<li><strong>DAG Building:</strong> Create dependency graph of all models</li>
<li><strong>Dependency Resolution:</strong> Determine execution order</li>
<li><strong>Materialization:</strong> Decide how to build each model (view, table, incremental, ephemeral)</li>
<li><strong>SQL Generation &amp; Execution:</strong> Generate SQL, execute in warehouse in correct order</li>
<li><strong>Post-Run Actions:</strong> Run tests, generate documentation</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q26">
<div class="question-title">Explain end-to-end data pipeline - What are the logics you followed</div>
<div class="answer">
<p><strong>Typical Stages:</strong></p>
<ol>
<li><strong>Data Sources:</strong> Operational databases, SaaS apps, APIs, logs (identify what data, how to access, frequency)</li>
<li><strong>Ingestion/Extraction:</strong> Batch or streaming (full load vs incremental CDC)</li>
<li><strong>Landing/Staging:</strong> Raw data storage with schema-on-read approach</li>
<li><strong>Transformation:</strong> Clean, enrich, conform, aggregate (using DBT, SQL, or external engines)</li>
<li><strong>Serving/Presentation:</strong> Optimized views/tables for consumption</li>
<li><strong>Consumption:</strong> BI tools, dashboards, data science, applications</li>
</ol>
<p><strong>Cross-cutting Concerns:</strong></p>
<ul>
<li><strong>Orchestration:</strong> Schedule and manage pipeline stages (Airflow, Tasks, Step Functions)</li>
<li><strong>Monitoring &amp; Alerting:</strong> Track pipeline health, errors, data quality</li>
<li><strong>Data Lineage:</strong> Understand data flow, debug issues</li>
<li><strong>Cost Management:</strong> Monitor and optimize cloud resource usage</li>
<li><strong>Version Control:</strong> Git for all code (SQL, Python, DBT)</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q38">
<div class="question-title">DBT Project Architecture</div>
<div class="answer">
<p><strong>Typical DBT Project Structure:</strong></p>
<pre>my_dbt_project/
├── dbt_project.yml         # Core configuration
├── models/                 # Data transformation models
│   ├── staging/           # Raw, light transformations
│   ├── intermediate/      # Complex joins, business logic
│   └── marts/             # Final, user-facing models
├── analysis/              # Ad-hoc SQL queries
├── macros/                # Reusable SQL snippets
├── seeds/                 # Static CSV lookup tables
├── snapshots/             # SCD Type 2 configurations
├── tests/                 # Custom data quality tests
├── logs/                  # DBT execution logs
└── target/                # Compiled SQL, manifests</pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>dbt_project.yml:</strong> Project configuration, default materializations</li>
<li><strong>Models:</strong> SQL SELECT statements - heart of DBT project</li>
<li><strong>Tests:</strong> Data quality validations (not_null, unique, accepted_values, custom)</li>
<li><strong>Macros:</strong> Reusable Jinja templates for dynamic SQL</li>
<li><strong>Seeds:</strong> Static CSV files loaded as tables</li>
<li><strong>Snapshots:</strong> Capture historical changes (SCD Type 2)</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q43">
<div class="question-title">What is DBT and what problem does it solve in modern data stack</div>
<div class="answer">
<p>DBT (data build tool) is a command-line framework for transforming data in your warehouse with modular SQL and software engineering best practices.</p>
<p><strong>Problems it Solves:</strong></p>
<ul>
<li><strong>Code Proliferation:</strong> Centralizes transformation logic instead of scattered ETL scripts/BI tools</li>
<li><strong>Lack of Best Practices:</strong> Brings version control, modularity, testing, documentation to data work</li>
<li><strong>Dependency Management:</strong> Automatically determines correct model execution order</li>
<li><strong>Testing &amp; Quality:</strong> Provides framework for data quality tests and auto-documentation</li>
<li><strong>Transparency:</strong> Makes entire transformation process transparent and auditable</li>
</ul>
<p><strong>Core Philosophy:</strong> Leverage the power of the data warehouse itself (pushing computations) rather than using separate processing engines</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q44">
<div class="question-title">Core components of a DBT project: Models, Tests, Seeds</div>
<div class="answer">
<p><strong>Models:</strong> SQL SELECT statements organized into transformation logic</p>
<ul>
<li>Each .sql file = one model</li>
<li>Output: view or table (depends on materialization)</li>
<li>Example: Model calculating monthly sales from raw transactions</li>
</ul>
<p><strong>Tests:</strong> Data quality checks ensuring data integrity</p>
<ul>
<li><strong>Singular Tests:</strong> Custom SQL returning failing rows if condition not met</li>
<li><strong>Generic Tests:</strong> Pre-defined tests applied via YAML (not_null, unique, accepted_values)</li>
<li>Purpose: Prevent downstream errors, ensure data reliability</li>
</ul>
<p><strong>Seeds:</strong> Static CSV files loaded as reference/lookup tables</p>
<ul>
<li>Small, infrequently-changing data (country codes, mapping tables)</li>
<li>Loaded via dbt seed command</li>
</ul>
<p><strong>Macros (Bonus):</strong> Reusable Jinja+SQL code snippets (like functions)</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q45">
<div class="question-title">Materializations in DBT - Four default types</div>
<div class="answer">
<p><strong>View (Default):</strong></p>
<ul>
<li>Mechanism: Creates SQL view</li>
<li>Pros: Always latest data, fast compile</li>
<li>Cons: Slow queries (logic runs every time)</li>
<li>Best for: Simple transformations, rarely-queried models</li>
</ul>
<p><strong>Table:</strong></p>
<ul>
<li>Mechanism: Creates persistent table with CREATE TABLE AS</li>
<li>Pros: Fast queries (pre-calculated)</li>
<li>Cons: Slow to build, consumes storage</li>
<li>Best for: Complex/frequently-queried models, large datasets</li>
</ul>
<p><strong>Incremental:</strong></p>
<ul>
<li>Mechanism: INSERT/MERGE only new/changed records</li>
<li>Pros: Very fast after initial build</li>
<li>Cons: Complex design, prone to errors</li>
<li>Best for: Large fact tables with incremental changes</li>
</ul>
<p><strong>Ephemeral:</strong></p>
<ul>
<li>Mechanism: No physical object, compiles to CTE</li>
<li>Pros: Excellent modularity</li>
<li>Cons: Not queryable directly</li>
<li>Best for: Simple intermediate cleaning steps</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q46">
<div class="question-title">Managing dependencies and referencing models in DBT</div>
<div class="answer">
<p><strong>The ref() Function:</strong> Reference models without hardcoding schema names</p>
<pre>-- Instead of FROM SCHEMA.TABLE
SELECT * FROM {{ ref('stg_orders') }}
WHERE is_valid = TRUE</pre>
<p><strong>How it Works:</strong></p>
<ul>
<li>{{ ref('stg_orders') }} replaced with correct schema/table name during compilation</li>
<li>DBT automatically knows stg_orders must be built before this model</li>
<li>Creates dependency edge in DAG</li>
</ul>
<p><strong>Automatic DAG Generation:</strong></p>
<ul>
<li>DBT scans all models and their ref() calls</li>
<li>Builds DAG showing ALL dependencies</li>
<li>dbt run executes models in correct order, ensuring source tables exist before use</li>
</ul>
<p><strong>Benefits:</strong> No manual ordering, automatic parallelization, clear dependencies</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q47">
<div class="question-title">What is Jinja in DBT - Simple example</div>
<div class="answer">
<p>Jinja is a templating language that adds programming logic to SQL. Processed before SQL reaches the warehouse.</p>
<p><strong>Capabilities:</strong> if/else statements, variables, loops, macro reuse, dynamic SQL</p>
<p><strong>Simple Example:</strong></p>
<pre>{% set limit_date = '2023-01-01' %}

SELECT * FROM {{ ref('raw_data') }}
WHERE created_at &gt;= '{{ limit_date }}'

{% if target.name == 'prod' %}
  AND is_active = TRUE
{% endif %}</pre>
<p><strong>Compiled SQL (Development):</strong></p>
<pre>SELECT * FROM my_dev_schema.raw_data
WHERE created_at &gt;= '2023-01-01'</pre>
<p><strong>Compiled SQL (Production):</strong></p>
<pre>SELECT * FROM my_prod_schema.raw_data
WHERE created_at &gt;= '2023-01-01'
  AND is_active = TRUE</pre>
<p><strong>Benefits:</strong> DRY (Don't Repeat Yourself), environment-specific logic, dynamic SQL generation</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q48">
<div class="question-title">Data Governance best practices</div>
<div class="answer">
<p><strong>Key Best Practices:</strong></p>
<ol>
<li><strong>Define Clear Ownership:</strong> Assign data owners and stewards for each domain/dataset</li>
<li><strong>Establish Data Dictionary:</strong> Document all data elements, definitions, relationships</li>
<li><strong>Implement Metadata Management:</strong> Track technical metadata (schema, lineage) and business metadata</li>
<li><strong>Quality Standards:</strong> Implement data quality rules, monitoring, and remediation processes</li>
<li><strong>Access Control:</strong> RBAC, row-level security, column-level masking</li>
<li><strong>Data Lineage:</strong> Understand data flow from source to consumption</li>
<li><strong>Compliance &amp; Auditing:</strong> Track data usage, audit logs, regulatory compliance (GDPR, HIPAA)</li>
<li><strong>Version Control:</strong> Version all code (SQL, DBT, Python) in Git</li>
<li><strong>Documentation:</strong> Auto-generate and maintain docs (DBT, Collibra, Ataccama)</li>
<li><strong>Collaboration:</strong> Foster data culture, communication between teams</li>
</ol>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q49">
<div class="question-title">Data quality testing strategies</div>
<div class="answer">
<p><strong>Common Testing Approaches:</strong></p>
<ol>
<li><strong>Schema Tests:</strong> Validate data types, constraints
                        <ul>
<li><strong>not_null:</strong> Column should not have NULL values</li>
<li><strong>unique:</strong> Column values must be distinct</li>
<li><strong>relationships:</strong> Foreign key relationships</li>
<li><strong>accepted_values:</strong> Column values from allowed set</li>
</ul>
</li>
<li><strong>Statistical Tests:</strong> Validate data distributions, outliers
                        <ul>
<li>Range checks</li>
<li>Distribution analysis</li>
<li>Outlier detection</li>
</ul>
</li>
<li><strong>Business Logic Tests:</strong> Validate business rules
                        <ul>
<li>Cross-table consistency</li>
<li>Aggregation accuracy</li>
<li>Recency checks</li>
</ul>
</li>
<li><strong>Freshness Tests:</strong> Ensure data is current
                        <ul>
<li>Last update timestamp</li>
<li>Row count expectations</li>
</ul>
</li>
</ol>
<p><strong>Tools:</strong> DBT tests, Great Expectations, Ataccama, custom SQL checks</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q50">
<div class="question-title">Building a modern data stack - Technology selection</div>
<div class="answer">
<p><strong>Typical Modern Data Stack Layers:</strong></p>
<ul>
<li><strong>Source Systems:</strong> Operational databases, SaaS applications, APIs</li>
<li><strong>Ingestion:</strong> Fivetran, Stitch, Airbyte, AWS DMS, Qlik Replicate</li>
<li><strong>Cloud Data Warehouse:</strong> Snowflake, BigQuery, Redshift</li>
<li><strong>Transformation:</strong> DBT, Spark, Python</li>
<li><strong>Data Governance:</strong> Collibra, Ataccama, DataHub</li>
<li><strong>Analytics/BI:</strong> Tableau, Power BI, Looker</li>
<li><strong>Orchestration:</strong> Airflow, Prefect, Dagster</li>
<li><strong>Reverse ETL:</strong> Hightouch, Census (sync data back to source systems)</li>
</ul>
<p><strong>Selection Criteria:</strong></p>
<ul>
<li>Scalability and performance</li>
<li>Cost efficiency</li>
<li>Ease of use / learning curve</li>
<li>Integration with existing tools</li>
<li>Community and support</li>
<li>Security and compliance requirements</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q51">
<div class="question-title">Future trends in data engineering</div>
<div class="answer">
<p><strong>Emerging Trends:</strong></p>
<ul>
<li><strong>DataOps:</strong> Applying DevOps principles to data pipelines (CI/CD, monitoring, automation)</li>
<li><strong>Real-time Analytics:</strong> Move beyond batch to streaming/real-time data pipelines</li>
<li><strong>AI/ML Integration:</strong> ML models integrated into data pipelines, automated feature engineering</li>
<li><strong>Data Mesh:</strong> Decentralized data ownership model, domain-oriented data architecture</li>
<li><strong>Lakehouse Architecture:</strong> Combining data lake flexibility with warehouse performance (Delta Lake, Iceberg, Hudi)</li>
<li><strong>Cloud-Native:</strong> Serverless data platforms, compute separation from storage</li>
<li><strong>Data Quality as First-Class:</strong> Increased focus on data quality, governance, and observability</li>
<li><strong>Self-Service Analytics:</strong> Tools enabling business users to perform analysis without technical expertise</li>
<li><strong>Reverse ETL:</strong> Syncing aggregated/enriched data back to operational systems</li>
<li><strong>Privacy-Preserving Technologies:</strong> Differential privacy, federated learning for sensitive data</li>
</ul>
<p><strong>Skills for Future:</strong> Cloud platforms (AWS/Azure/GCP), DBT, SQL, Python, Spark, containers, Kubernetes, ML basics</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q52">
<div class="question-title">Explain your end-to-end data pipeline (tools + flow)</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>In a real-world setup, my end-to-end data pipeline looks like this:</p>
<p>&gt; <strong>Source systems → Ingestion → Cloud storage → Data warehouse → Transformations → Analytics</strong></p>
<p><strong>Typical stack I’ve worked with:</strong></p>
<ul>
<li><strong>Sources:</strong> Oracle, PostgreSQL, Salesforce, REST APIs</li>
<li><strong>Ingestion:</strong> Qlik Replicate / Fivetran / Airbyte</li>
<li><strong>Landing zone:</strong> AWS S3 (raw, immutable data)</li>
<li><strong>Warehouse:</strong> Snowflake</li>
<li><strong>Transformations:</strong> dbt (staging → intermediate → marts)</li>
<li><strong>Orchestration:</strong> Airflow</li>
<li><strong>Monitoring:</strong> Airflow alerts + dbt tests</li>
<li><strong>Consumption:</strong> Power BI / Tableau</li>
</ul>
<p><strong>Key design principles:</strong></p>
<ul>
<li>Raw data is <strong>append-only</strong> and never modified</li>
<li>All transformations happen <strong>inside Snowflake</strong></li>
<li><strong>dbt</strong> owns business logic and data modeling</li>
<li><strong>Airflow</strong> manages scheduling, dependencies, and retries</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q53">
<div class="question-title">How do you use Python inside Airflow for orchestration and failure alerts?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>Airflow is used strictly for <strong>orchestration</strong>, not transformations.</p>
<p><strong>I use Python in Airflow</strong> mainly to:</p>
<ul>
<li>Define DAGs and task dependencies</li>
<li>Validate upstream conditions (file arrival, row counts)</li>
<li>Trigger dbt runs</li>
<li>Handle retries, SLAs, and alerts</li>
</ul>
<p><strong>Typical usage:</strong></p>
<ul>
<li><code>PythonOperator</code> for file availability checks and pre/post DQ checks</li>
<li><code>BashOperator</code> to trigger dbt commands</li>
<li><code>on_failure_callback</code> to send Slack/Email with DAG, task, exec date, and error</li>
</ul>
<p><strong>Failure handling approach:</strong></p>
<ul>
<li>Configured retries with exponential backoff</li>
<li>SLA monitoring for long-running tasks</li>
<li>Alerts triggered only after retries are exhausted</li>
</ul>
<p><em>Interview-ready line:</em></p>
<p><strong>“Airflow controls <em>when</em> and <em>in what order</em> things run, while dbt controls <em>how</em> the data is transformed.”</strong></p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q54">
<div class="question-title">How do you implement SCD Type 2 in dbt?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>I implement SCD Type 2 in dbt using <strong>dbt snapshots</strong>, which is the recommended and scalable approach.</p>
<p><strong>Example snapshot:</strong></p>
<pre>{% raw %}{% snapshot dim_customer_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='customer_id',
    strategy='timestamp',
    updated_at='updated_at'
  )
}}

select * from {{ ref('stg_customers') }}

{% endsnapshot %}{% endraw %}</pre>
<p><strong>What dbt handles automatically:</strong></p>
<ul>
<li>Adds <code>dbt_valid_from</code> and <code>dbt_valid_to</code> columns</li>
<li>Inserts a new row when tracked columns change</li>
<li>Closes the old record by updating <code>dbt_valid_to</code></li>
</ul>
<p><strong>Common use cases:</strong> Customer attribute history, product price changes, account/subscription status tracking</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q55">
<div class="question-title">How does dbt snapshot handle deletes?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>By default, <strong>dbt snapshots do not detect deletes</strong>.</p>
<p><strong>Default behavior:</strong></p>
<ul>
<li>If a record disappears from the source table, the snapshot keeps the last version open (<code>dbt_valid_to</code> remains NULL)</li>
</ul>
<p><strong>To handle deletes properly:</strong></p>
<pre>invalidate_hard_deletes=true</pre>
<p><strong>Result:</strong></p>
<ul>
<li>When a source row is deleted, <code>dbt_valid_to</code> is populated and the record is marked as inactive</li>
</ul>
<p><em>Interview-ready line:</em></p>
<p><strong>“dbt snapshots don’t track deletes by default; hard deletes must be explicitly enabled.”</strong></p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q56">
<div class="question-title">When and why do you use dbt seeds?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>I use <strong>dbt seeds</strong> for <strong>small, static reference data</strong> that rarely changes and should live alongside code.</p>
<p><strong>Typical examples:</strong></p>
<ul>
<li>Country or currency codes</li>
<li>Status mappings</li>
<li>Business rule lookup tables</li>
<li>SLA thresholds</li>
</ul>
<p><strong>Why seeds are useful:</strong></p>
<ul>
<li>Version-controlled using Git</li>
<li>Easy to deploy with dbt</li>
<li>No dependency on upstream systems</li>
</ul>
<p><strong>When I avoid seeds:</strong></p>
<ul>
<li>Large datasets</li>
<li>Frequently changing or transactional data</li>
</ul>
<p><em>One-liner for interviews:</em></p>
<p><strong>“dbt seeds are ideal for small, stable reference data that belongs with transformation logic.”</strong></p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q58">
<div class="question-title">dbt Materializations – Syntax &amp; When to Use</div>
<div class="answer">
<p>Materialization defines <strong>how dbt builds a model in the warehouse</strong>.</p>
<p><strong>1. View (Default)</strong></p>
<pre>{{ config(materialized='view') }}

select *
from {{ ref('stg_orders') }}</pre>
<ul>
<li><strong>Purpose:</strong> Creates a Snowflake VIEW. No data stored, always reflects latest source data.</li>
<li><strong>When to use:</strong> Staging models, light transformations, fast iteration</li>
<li><strong>Interview line:</strong> "Views are best for staging where we want zero storage and always-fresh data."</li>
</ul>
<p><strong>2. Table</strong></p>
<pre>{{ config(materialized='table') }}

select *
from {{ ref('int_sales') }}</pre>
<ul>
<li><strong>Purpose:</strong> Creates a physical table. Rebuilt completely on every run.</li>
<li><strong>When to use:</strong> Small dimension tables, final marts with low data volume</li>
<li><strong>Trade-off:</strong> Expensive for large datasets</li>
</ul>
<p><strong>3. Incremental (MOST IMPORTANT)</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}

{% if is_incremental() %}
  where updated_at &gt; (select max(updated_at) from {{ this }})
{% endif %}</pre>
<ul>
<li><strong>Purpose:</strong> Loads only new or changed data. Saves cost and time.</li>
<li><strong>When to use:</strong> Fact tables, large datasets, streaming/CDC data</li>
<li><strong>Key points:</strong> <code>is_incremental()</code> runs only after first load; <code>unique_key</code> enables merge</li>
<li><strong>Interview line:</strong> "Incremental models are mandatory for large fact tables in production."</li>
</ul>
<p><strong>4. Ephemeral</strong></p>
<pre>{{ config(materialized='ephemeral') }}

select
    order_id,
    sum(amount) as total_amount
from {{ ref('stg_payments') }}
group by order_id</pre>
<ul>
<li><strong>Purpose:</strong> No table, no view. SQL is inlined into downstream models.</li>
<li><strong>When to use:</strong> Reusable logic, intermediate calculations</li>
<li><strong>Interview line:</strong> "Ephemeral models are like SQL macros with structure."</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q59">
<div class="question-title">dbt Snapshots – Full Syntax &amp; Purpose (SCD Type 2)</div>
<div class="answer">
<p>Snapshots are used for <strong>SCD Type 2</strong> – tracking changes over time.</p>
<p><strong>Timestamp Strategy (When source has reliable updated_at)</strong></p>
<pre>{% snapshot dim_customer_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='customer_id',
    strategy='timestamp',
    updated_at='updated_at',
    invalidate_hard_deletes=true
  )
}}

select *
from {{ ref('stg_customers') }}

{% endsnapshot %}</pre>
<p><strong>What dbt adds automatically:</strong></p>
<ul>
<li><code>dbt_valid_from</code> – When the record became active</li>
<li><code>dbt_valid_to</code> – When the record expired (NULL if current)</li>
<li>History tracking with row versions</li>
</ul>
<p><strong>Check Strategy (When no reliable timestamp)</strong></p>
<pre>{% snapshot dim_product_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='product_id',
    strategy='check',
    check_cols=['price', 'status']
  )
}}

select *
from {{ ref('stg_products') }}

{% endsnapshot %}</pre>
<p><strong>Use when:</strong> No reliable timestamp; track changes in specific columns</p>
<p><strong>Key Configuration Options:</strong></p>
<ul>
<li><code>invalidate_hard_deletes=true</code> – Closes records when source row is deleted</li>
<li><code>target_schema</code> – Where snapshots are stored</li>
<li><code>unique_key</code> – Column(s) identifying a record</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q60">
<div class="question-title">dbt YAML Files – Syntax, Indentation &amp; Purpose</div>
<div class="answer">
<p>YAML defines <strong>metadata and tests</strong>, not SQL logic.</p>
<p><strong>schema.yml (Most Important YAML File)</strong></p>
<pre>version: 2

models:
  - name: stg_orders
    description: "Cleaned orders data"
    columns:
      - name: order_id
        description: "Primary key"
        tests:
          - not_null
          - unique
      - name: status
        tests:
          - accepted_values:
              values: ['pending', 'completed', 'cancelled']</pre>
<p><strong>Indentation Rules (CRITICAL – YAML breaks with wrong spacing)</strong></p>
<table>
<tr>
<th>Level</th>
<th>Indentation</th>
<th>Meaning</th>
</tr>
<tr>
<td>models:</td>
<td>0 spaces</td>
<td>Top-level key</td>
</tr>
<tr>
<td>- name:</td>
<td>2 spaces</td>
<td>Model item</td>
</tr>
<tr>
<td>columns:</td>
<td>4 spaces</td>
<td>Column metadata</td>
</tr>
<tr>
<td>tests:</td>
<td>6 spaces</td>
<td>Data quality tests</td>
</tr>
</table>
<p><strong>Interview Tip:</strong> "YAML breaks if indentation is wrong. Two spaces only – no tabs."</p>
<p><strong>Relationships Test (Foreign Key Validation)</strong></p>
<pre>- name: order_id
    description: "Foreign key to customer"
    tests:
      - relationships:
          to: ref('dim_customer')
          field: customer_id</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q61">
<div class="question-title">Source YAML, Snapshot YAML, and Seeds YAML</div>
<div class="answer">
<p><strong>Source YAML (sources.yml)</strong></p>
<pre>version: 2

sources:
  - name: sales
    database: raw
    schema: public
    tables:
      - name: orders
        description: "Raw orders table"
        columns:
          - name: order_id
            tests:
              - not_null</pre>
<ul>
<li><strong>Purpose:</strong> Define raw data sources, source freshness, source-level testing, lineage tracking</li>
<li><strong>Usage in models:</strong> <code>{{ source('sales', 'orders') }}</code></li>
</ul>
<p><strong>Snapshot YAML (snapshots.yml)</strong></p>
<pre>version: 2

snapshots:
  - name: dim_customer_snapshot
    description: "Customer SCD Type 2 history"
    columns:
      - name: customer_id
        tests:
          - not_null</pre>
<p><strong>Seeds YAML (seeds.yml)</strong></p>
<pre>version: 2

seeds:
  - name: country_codes
    description: "ISO country codes lookup table"
    columns:
      - name: country_code
        description: "2-letter ISO code"
        tests:
          - not_null
          - unique</pre>
<ul>
<li><strong>Purpose:</strong> Version-controlled static reference data (CSV files)</li>
<li><strong>To load:</strong> <code>dbt seed</code></li>
<li><strong>Usage in models:</strong> <code>{{ ref('country_codes') }}</code></li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q62">
<div class="question-title">Tests in YAML – Built-in &amp; Custom Tests</div>
<div class="answer">
<p><strong>Built-in Generic Tests</strong></p>
<pre>tests:
  - not_null  # Column must not be NULL
  - unique    # Column values must be distinct
  - accepted_values:
      values: ['A', 'I', 'U']  # Only these values allowed</pre>
<p><strong>Relationships Test (Foreign Key Validation)</strong></p>
<pre>- name: customer_id
    description: "Foreign key to dim_customer"
    tests:
      - relationships:
          to: ref('dim_customer')
          field: customer_id</pre>
<ul>
<li><strong>Purpose:</strong> Ensures referential integrity across tables</li>
<li><strong>When to use:</strong> Foreign key validation, cross-table consistency</li>
</ul>
<p><strong>Custom Singular Tests (SQL-based)</strong></p>
<p>Create a file: <code>tests/assert_no_negative_amounts.sql</code></p>
<pre>-- Should return zero rows for test to pass
select * 
from {{ ref('fact_orders') }}
where amount &lt; 0</pre>
<p><strong>Why custom tests:</strong> Business logic checks, complex validations (e.g., "order count should not drop &gt; 10% daily")</p>
<p><strong>Run tests:</strong> <code>dbt test</code></p>
<ul>
<li>Executes all YAML tests and singular tests</li>
<li>Fails build if any test fails (default behavior)</li>
<li>With flag: <code>dbt test --select stg_orders</code> (test specific model)</li>
</ul>
<p><strong>Interview Line:</strong> "Tests are the backbone of data quality in dbt – they run after every build and catch issues early."</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q63">
<div class="question-title">dbt_project.yml Configuration &amp; Interview Power Lines</div>
<div class="answer">
<p><strong>dbt_project.yml – Enforce Materialization Standards</strong></p>
<pre>name: 'my_analytics_project'
version: '1.0'
config-version: 2

models:
  my_analytics_project:
    staging:
      +materialized: view        # All staging models are views
    intermediate:
      +materialized: view
    marts:
      +materialized: table       # All mart models are tables
      
    # Fact tables override to incremental
    marts_facts:
      +materialized: incremental
      +unique_key: order_id</pre>
<p><strong>Why This Matters:</strong></p>
<ul>
<li>Enforces organizational standards without per-model config</li>
<li>New developers don't need to decide materialization</li>
<li>Prevents expensive mistakes (e.g., 500GB table as view)</li>
</ul>
<p><strong>Quick Interview Cheat Table</strong></p>
<table>
<tr>
<th>Feature</th>
<th>Used For</th>
<th>Output</th>
</tr>
<tr>
<td>view</td>
<td>Staging</td>
<td>SQL SELECT</td>
</tr>
<tr>
<td>table</td>
<td>Small dimensions</td>
<td>Physical table</td>
</tr>
<tr>
<td>incremental</td>
<td>Large facts</td>
<td>Fast updates</td>
</tr>
<tr>
<td>ephemeral</td>
<td>Reusable logic</td>
<td>Inlined CTE</td>
</tr>
<tr>
<td>snapshot</td>
<td>SCD Type 2</td>
<td>History table</td>
</tr>
<tr>
<td>schema.yml</td>
<td>Tests &amp; docs</td>
<td>Metadata</td>
</tr>
<tr>
<td>sources.yml</td>
<td>Source metadata</td>
<td>Data lineage</td>
</tr>
<tr>
<td>seeds</td>
<td>Static reference data</td>
<td>CSV → Table</td>
</tr>
</table>
<p><strong>Interview Power Lines (Use These Under Pressure)</strong></p>
<ul>
<li><strong>"dbt handles transformations, Airflow handles orchestration."</strong></li>
<li><strong>"Incremental models are critical for cost control in Snowflake."</strong></li>
<li><strong>"Snapshots are the cleanest way to implement SCD Type 2."</strong></li>
<li><strong>"YAML defines <em>what should be true</em>, SQL defines <em>how data is built</em>."</strong></li>
<li><strong>"Views for staging keep iterative work fast; tables for marts ensure performance."</strong></li>
<li><strong>"Tests catch data quality issues before they reach BI tools or dashboards."</strong></li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q64">
<div class="question-title">Advanced Incremental Strategies – MERGE vs APPEND</div>
<div class="answer">
<p>Incremental ≠ one thing. There are <strong>multiple patterns</strong>, and choosing the wrong one causes <strong>duplicates, data loss, or high cost</strong>.</p>
<p><strong>Strategy 1: MERGE (Upsert) – Most Common</strong></p>
<p><strong>When to use:</strong> CDC data, updates are possible, late-arriving data, deduplication required</p>
<p><strong>How it works:</strong> Uses MERGE INTO, updates existing rows, inserts new rows</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}</pre>
<p><strong>What dbt generates:</strong></p>
<pre>MERGE INTO target t
USING source s
ON t.order_id = s.order_id
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT</pre>
<p><strong>Interview explanation:</strong> "MERGE is used when data can change after ingestion. It guarantees idempotency and correctness."</p>
<p><strong>Strategy 2: APPEND-ONLY Incremental</strong></p>
<p><strong>When to use:</strong> Event data, immutable logs, streaming data, no updates ever</p>
<pre>{{ config(
    materialized='incremental'
) }}

select *
from {{ ref('stg_events') }}

{% if is_incremental() %}
  where event_date &gt; (select max(event_date) from {{ this }})
{% endif %}</pre>
<p><strong>Key difference:</strong> No <code>unique_key</code>, dbt does INSERT ONLY</p>
<p><strong>Interview explanation:</strong> "Append strategy is faster and cheaper but only safe when data is immutable."</p>
<p><strong>Strategy 3: Micro-Batch Incremental (Advanced)</strong></p>
<p><strong>Problem it solves:</strong> Late-arriving data, backfills, partial refresh without full rebuild</p>
<pre>{% if is_incremental() %}
  where updated_at &gt;= dateadd(day, -3, current_date)
{% endif %}</pre>
<p><strong>Use case:</strong> Refresh last N days every run</p>
<p><strong>Interview line:</strong> "Micro-batching balances correctness and cost by reprocessing a sliding window."</p>
<p><strong>Strategy 4: Delete + Insert (Hard Reset per Key)</strong></p>
<p><strong>When to use:</strong> Source sends full replacement per key, no reliable update timestamp</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='customer_id',
    incremental_strategy='delete+insert'
) }}</pre>
<p><strong>Interview explanation:</strong> "delete+insert is safer when updates are complex and partial merges aren't reliable."</p>
<p><strong>Incremental Strategy Decision Table</strong></p>
<table>
<tr>
<th>Scenario</th>
<th>Strategy</th>
</tr>
<tr>
<td>CDC / updates</td>
<td>MERGE</td>
</tr>
<tr>
<td>Logs / events</td>
<td>APPEND</td>
</tr>
<tr>
<td>Late data</td>
<td>Micro-batch</td>
</tr>
<tr>
<td>Full row replace</td>
<td>delete+insert</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q65">
<div class="question-title">Custom dbt Tests – Beyond Built-ins</div>
<div class="answer">
<p>Built-in tests are <strong>not enough</strong> in real projects. Custom tests validate business logic.</p>
<p><strong>Generic Test (Reusable)</strong></p>
<p><strong>Example:</strong> Amount must be positive</p>
<p><strong>File:</strong> <code>tests/assert_positive_amount.sql</code></p>
<pre>select *
from {{ model }}
where {{ column_name }} &lt;= 0</pre>
<p><strong>YAML usage:</strong></p>
<pre>columns:
  - name: amount
    tests:
      - assert_positive_amount</pre>
<p><strong>Interview explanation:</strong> "Generic tests are reusable across models and columns."</p>
<p><strong>Singular Test (Business Rule Test)</strong></p>
<p><strong>Example:</strong> Orders should have payments</p>
<p><strong>File:</strong> <code>tests/orders_without_payments.sql</code></p>
<pre>select o.order_id
from {{ ref('fct_orders') }} o
left join {{ ref('fct_payments') }} p
  on o.order_id = p.order_id
where p.order_id is null</pre>
<p><strong>Interview explanation:</strong> "Singular tests validate complex business assumptions, not just column constraints."</p>
<p><strong>Severity Levels (VERY IMPORTANT)</strong></p>
<pre>tests:
  - not_null:
      severity: warn</pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li><code>error</code> → pipeline fails</li>
<li><code>warn</code> → alert but continue</li>
</ul>
<p><strong>Interview line:</strong> "Not all data issues should break pipelines; severity controls blast radius."</p>
<p><strong>Snapshot-Specific Tests</strong></p>
<pre>tests:
  - dbt_utils.expression_is_true:
      expression: "dbt_valid_to &gt; dbt_valid_from"</pre>
<p><strong>Key Takeaway:</strong> Custom tests catch production bugs early. Integration tests are the last line of defense.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q66">
<div class="question-title">Enterprise-Grade dbt Folder Structure</div>
<div class="answer">
<p>Bad structure = unmaintainable project. This is <strong>conversion-critical</strong>.</p>
<p><strong>Recommended Enterprise Structure</strong></p>
<pre>dbt_project/
│
├── models/
│   ├── staging/
│   │   ├── sales/
│   │   │   ├── stg_orders.sql
│   │   │   ├── stg_payments.sql
│   │   │   └── schema.yml
│   │
│   ├── intermediate/
│   │   ├── int_order_payments.sql
│   │   └── schema.yml
│   │
│   ├── marts/
│   │   ├── finance/
│   │   │   ├── fct_revenue.sql
│   │   │   ├── dim_customer.sql
│   │   │   └── schema.yml
│
├── snapshots/
│   ├── dim_customer_snapshot.sql
│
├── tests/
│   ├── assert_positive_amount.sql
│
├── seeds/
│   ├── country_codes.csv
│
├── macros/
│   ├── generate_surrogate_key.sql
│
├── dbt_project.yml</pre>
<p><strong>Layer Responsibilities (Interview Gold)</strong></p>
<p><strong>Staging (stg_)</strong></p>
<ul>
<li>1-to-1 with source</li>
<li>Rename columns</li>
<li>Type casting</li>
<li>No joins</li>
</ul>
<p>"Staging models are clean mirrors of source data."</p>
<p><strong>Intermediate (int_)</strong></p>
<ul>
<li>Joins</li>
<li>Deduplication</li>
<li>Business prep logic</li>
</ul>
<p>"Intermediate models simplify downstream marts."</p>
<p><strong>Marts (fct_, dim_)</strong></p>
<ul>
<li>Business-ready</li>
<li>KPI definitions</li>
<li>Analytics layer</li>
</ul>
<p>"Marts are the contract with BI and consumers."</p>
<p><strong>Enforcing Standards via dbt_project.yml</strong></p>
<pre>models:
  my_project:
    staging:
      +materialized: view
    intermediate:
      +materialized: ephemeral
    marts:
      +materialized: incremental</pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Prevents mistakes</li>
<li>Enforces architecture</li>
<li>Scales across teams</li>
</ul>
<p><strong>Final Interview Power Statements</strong></p>
<ul>
<li>"Incremental strategy selection depends on data mutability."</li>
<li>"MERGE ensures idempotency in CDC pipelines."</li>
<li>"Custom tests validate business truth, not just schema."</li>
<li>"Folder structure enforces responsibility boundaries."</li>
<li>"dbt scales through convention, not configuration."</li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q67">
<div class="question-title">Production dbt Failures &amp; Root Causes (What Seniors Face)</div>
<div class="answer">
<p><strong>These scenarios separate junior users from senior owners. Interviewers love failure stories.</strong></p>
<p><strong>Failure 1: Incremental Model Creating Duplicates</strong></p>
<p><strong>Root Cause:</strong> Missing or incorrect <code>unique_key</code>; append strategy used where updates exist</p>
<p><strong>Symptom:</strong> Row counts grow unexpectedly, KPI inflation, duplicate primary keys</p>
<p><strong>Bad Pattern:</strong></p>
<pre>{{ config(materialized='incremental') }}
select * from source</pre>
<p><strong>Fix:</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}</pre>
<p><strong>Interview Explanation:</strong> "If updates are possible, append-only incrementals are dangerous. MERGE is mandatory."</p>
<p><strong>Failure 2: Late-Arriving Data Missing in Incremental Loads</strong></p>
<p><strong>Root Cause:</strong> Filter based only on <code>max(updated_at)</code>; source sends delayed records</p>
<p><strong>Symptom:</strong> Historical gaps, inconsistent reports</p>
<p><strong>Bad Pattern:</strong></p>
<pre>where updated_at &gt; (select max(updated_at) from {{ this }})</pre>
<p><strong>Fix - Micro-Batching:</strong></p>
<pre>{% if is_incremental() %}
  where updated_at &gt;= dateadd(day, -3, current_date)
{% endif %}</pre>
<p><strong>Interview Line:</strong> "We reprocess a rolling window to handle late-arriving data safely."</p>
<p><strong>Failure 3: dbt Snapshot Growing Infinitely</strong></p>
<p><strong>Root Cause:</strong> Using <code>check</code> strategy on volatile columns (timestamps, operational noise)</p>
<p><strong>Symptom:</strong> Snapshot table explodes, performance degradation</p>
<p><strong>Fix:</strong> Track <strong>only business-relevant columns</strong></p>
<pre>check_cols=['status', 'tier']</pre>
<p><strong>Interview Line:</strong> "Snapshots should track business change, not operational noise."</p>
<p><strong>Failure 4: Pipelines Failing Due to Non-Critical Data Issues</strong></p>
<p><strong>Root Cause:</strong> All tests set to <code>severity: error</code></p>
<p><strong>Symptom:</strong> Frequent production failures, alert fatigue</p>
<p><strong>Fix:</strong></p>
<pre>tests:
  - not_null:
      severity: warn</pre>
<p><strong>Interview Line:</strong> "Not all data quality issues deserve to break pipelines."</p>
<p><strong>Failure 5: dbt Models Rebuilt Accidentally in Production</strong></p>
<p><strong>Root Cause:</strong> Using <code>table</code> materialization for large datasets</p>
<p><strong>Symptom:</strong> Snowflake cost spike, long downtimes</p>
<p><strong>Fix:</strong> Use <code>incremental</code> and enforce via <code>dbt_project.yml</code></p>
<pre>marts:
  +materialized: incremental</pre>
<p><strong>Failure 6: Broken Downstream Models Due to Schema Changes</strong></p>
<p><strong>Root Cause:</strong> Source column renamed/dropped; no contract enforcement</p>
<p><strong>Fix:</strong> Source tests + freshness checks; staging layer isolation</p>
<p><strong>Interview Line:</strong> "Staging models act as shock absorbers for upstream changes."</p>
<p><strong>Key Takeaway:</strong> Most production issues stem from wrong materialization choice or missing constraints. Prevention is cheaper than firefighting.</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q68">
<div class="question-title">Senior-Level dbt Cheat Sheet &amp; Interview Power Lines</div>
<div class="answer">
<p><strong>Memorize this table before every interview.</strong></p>
<p><strong>Materialization Selection Grid</strong></p>
<table>
<tr>
<th>Type</th>
<th>Use Case</th>
<th>Risk</th>
</tr>
<tr>
<td>view</td>
<td>Staging</td>
<td>None (zero storage)</td>
</tr>
<tr>
<td>table</td>
<td>Small dimensions</td>
<td>Full rebuild cost</td>
</tr>
<tr>
<td>incremental</td>
<td>Large facts</td>
<td>Wrong strategy = data loss</td>
</tr>
<tr>
<td>ephemeral</td>
<td>Reusable logic</td>
<td>Harder debugging</td>
</tr>
<tr>
<td>snapshot</td>
<td>SCD Type 2</td>
<td>Size explosion if misused</td>
</tr>
</table>
<p><strong>Incremental Strategy Selection</strong></p>
<table>
<tr>
<th>Data Pattern</th>
<th>Strategy</th>
</tr>
<tr>
<td>Immutable events</td>
<td>Append</td>
</tr>
<tr>
<td>Updates possible</td>
<td>Merge</td>
</tr>
<tr>
<td>Late data</td>
<td>Micro-batch</td>
</tr>
<tr>
<td>Full row replacement</td>
<td>delete+insert</td>
</tr>
</table>
<p><strong>Snapshot Strategy Decision</strong></p>
<table>
<tr>
<th>Situation</th>
<th>Strategy</th>
</tr>
<tr>
<td>Reliable updated_at</td>
<td>timestamp</td>
</tr>
<tr>
<td>No timestamp</td>
<td>check</td>
</tr>
<tr>
<td>Need delete tracking</td>
<td>invalidate_hard_deletes</td>
</tr>
</table>
<p><strong>Testing Strategy Framework</strong></p>
<table>
<tr>
<th>Test Type</th>
<th>Purpose</th>
</tr>
<tr>
<td>not_null</td>
<td>Mandatory keys</td>
</tr>
<tr>
<td>unique</td>
<td>Primary keys</td>
</tr>
<tr>
<td>relationships</td>
<td>Referential integrity</td>
</tr>
<tr>
<td>singular tests</td>
<td>Business rules</td>
</tr>
<tr>
<td>severity: warn</td>
<td>Non-blocking checks</td>
</tr>
</table>
<p><strong>Folder Responsibility Contract</strong></p>
<table>
<tr>
<th>Layer</th>
<th>Responsibility</th>
</tr>
<tr>
<td>staging</td>
<td>Clean + type cast (1-to-1 with source)</td>
</tr>
<tr>
<td>intermediate</td>
<td>Join + deduplicate (business prep)</td>
</tr>
<tr>
<td>marts</td>
<td>Business metrics (analytics layer)</td>
</tr>
<tr>
<td>snapshots</td>
<td>History tracking (SCD Type 2)</td>
</tr>
<tr>
<td>seeds</td>
<td>Static reference data</td>
</tr>
</table>
<p><strong>dbt Project Guardrails (dbt_project.yml)</strong></p>
<pre>staging:
  +materialized: view
intermediate:
  +materialized: ephemeral
marts:
  +materialized: incremental</pre>
<p><strong>YAML Golden Rules</strong></p>
<ul>
<li>2-space indentation only (never tabs)</li>
<li>YAML = expectations (what should be true)</li>
<li>SQL = logic (how data is built)</li>
<li>Tests are trust guarantees</li>
</ul>
<p><strong>Senior Interview Power Lines (Memorize These)</strong></p>
<ul>
<li><strong>"Incremental strategy depends on data mutability – know your source."</strong></li>
<li><strong>"Snapshots track business change, not operational noise."</strong></li>
<li><strong>"dbt scales through convention, not ad-hoc configurations."</strong></li>
<li><strong>"Tests define trust; they catch issues before BI and dashboards do."</strong></li>
<li><strong>"Staging layers isolate upstream chaos; marts define business truth."</strong></li>
<li><strong>"MERGE ensures idempotency in production pipelines."</strong></li>
</ul>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q69">
<div class="question-title">Advanced dbt Macros &amp; Jinja Templating Patterns</div>
<div class="answer">
<p><strong>Macros are reusable SQL/Jinja snippets. They're the difference between repetitive code and elegant architecture.</strong></p>
<p><strong>Pattern 1: Surrogate Key Generation (Enterprise Standard)</strong></p>
<pre>{% macro surrogate_key(field_list) %}
    {% set fields = field_list|join(", ") %}
    md5(concat({{fields}}))
{% endmacro %}</pre>
<p><strong>Usage in model:</strong></p>
<pre>select
    {{ surrogate_key(['customer_id', 'order_date']) }} as sk_customer_order,
    *
from {{ ref('stg_orders') }}</pre>
<p><strong>Why this matters:</strong> Ensures consistent hashing across all models without writing MD5 repeatedly</p>
<p><strong>Pattern 2: Dynamic Column Generation</strong></p>
<pre>{% macro generate_staging_columns(source_table) %}
    {% set columns = run_query("SELECT column_name FROM information_schema.columns WHERE table_name = '" ~ source_table ~ "'") %}
    {% for col in columns %}
        cast({{ col.column_name }} as string) as {{ col.column_name|lower }}{{ "," if not loop.last }}
    {% endfor %}
{% endmacro %}</pre>
<p><strong>Interview power line:</strong> "Macros eliminate manual column enumeration and allow code to adapt to schema changes."</p>
<p><strong>Pattern 3: Conditional Logic Based on Target Environment</strong></p>
<pre>{% macro limit_data() %}
    {% if target.name == 'dev' %}
        where date_trunc('day', created_at) &gt;= current_date - 30
    {% endif %}
{% endmacro %}</pre>
<p><strong>Use case:</strong> Dev uses 30 days, prod uses all data. Single model config, multiple behaviors.</p>
<p><strong>Pattern 4: Testing Helper Macros</strong></p>
<pre>{% macro assert_column_not_null(model, column_name) %}
    select *
    from {{ model }}
    where {{ column_name }} is null
{% endmacro %}</pre>
<p><strong>Called in singular test (tests/check_order_id.sql):</strong></p>
<pre>{{ assert_column_not_null(ref('fct_orders'), 'order_id') }}</pre>
<p><strong>Advanced Jinja Patterns</strong></p>
<ul>
<li><code>run_query()</code> – Executes SQL during parsing (only in execute block)</li>
<li><code>fromjson()</code> – Parse JSON in Jinja</li>
<li><code>tojson()</code> – Convert to JSON</li>
<li><code>safe_divide()</code> – Built-in macro to avoid division by zero</li>
</ul>
<p><strong>Interview line:</strong> "Macros reduce duplication by centralizing transformation logic. They're the foundation of scalable dbt projects."</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q70">
<div class="question-title">dbt Exposures &amp; Metrics – Connecting to BI &amp; Analytics</div>
<div class="answer">
<p><strong>Exposures define downstream BI artifacts (dashboards, reports) that depend on dbt models. They enable end-to-end lineage tracking.</strong></p>
<p><strong>Exposures YAML Structure (metrics.yml or exposures.yml)</strong></p>
<pre>version: 2

exposures:
  - name: executive_dashboard
    type: dashboard
    maturity: production
    owner:
      name: Analytics Lead
      email: analytics@company.com
    description: "Executive KPI dashboard"
    depends_on:
      - ref('fct_revenue')
      - ref('dim_customer')
    url: "https://looker.company.com/dashboards/executive"
    tags:
      - executive
      - critical</pre>
<p><strong>Benefits of Exposures:</strong></p>
<ul>
<li>dbt knows which dashboards depend on which models</li>
<li><code>dbt ls --select state:new</code> shows impacted downstream dashboards</li>
<li>Documentation becomes bi-directional</li>
<li>Stakeholder visibility in dbt DAG</li>
</ul>
<p><strong>dbt Metrics (Semantic Layer)</strong></p>
<p><strong>Purpose:</strong> Define business metrics once, use everywhere (BI tools, APIs, reports)</p>
<pre>metrics:
  - name: total_revenue
    description: "Sum of all orders"
    calculation_method: sum
    expression: order_amount
    timestamp: order_date
    time_grains: [day, month, year]
    dimensions:
      - customer_id
      - region
    meta:
      owner: "Finance"
      sla: "daily"</pre>
<p><strong>How it works:</strong></p>
<ul>
<li>BI tool queries dbt Semantic Layer API</li>
<li>dbt compiles metric to SQL</li>
<li>Single source of truth for business definitions</li>
</ul>
<p><strong>Interview power line:</strong> "Exposures create accountability by showing downstream blast radius. Metrics eliminate metric drift across teams."</p>
<p><strong>Advanced: Custom Properties for Governance</strong></p>
<pre>meta:
  owner: analytics_team
  pii: true
  retention_days: 90
  tags: [critical, gdpr-sensitive]</pre>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q71">
<div class="question-title">dbt on Snowflake: Performance Optimization &amp; Run Config</div>
<div class="answer">
<p><strong>Snowflake-specific tuning that senior engineers use in production to reduce cost and runtime.</strong></p>
<p><strong>1. Warehouse Sizing per Model Type</strong></p>
<pre>models:
  my_project:
    staging:
      +snowflake_warehouse: xs_wh     # Light transformations
    intermediate:
      +snowflake_warehouse: sm_wh
    marts:
      +snowflake_warehouse: lg_wh     # Heavy aggregations</pre>
<p><strong>Why:</strong> Prevents expensive large warehouse for simple selects</p>
<p><strong>2. Query Tags for Cost Attribution</strong></p>
<pre>{{ config(
    tags=['finance', 'daily'],
    query_tag='dbt_incremental_fact_revenue'
) }}</pre>
<p><strong>Result:</strong> Snowflake's query history shows exact cost per model</p>
<p><strong>3. Pre- and Post-Hooks for Cleanup</strong></p>
<pre>{{ config(
    pre_hook="ALTER SESSION SET TIMEZONE = 'UTC'",
    post_hook="DELETE FROM {{ table }} WHERE dbt_valid_to is not null"
) }}</pre>
<p><strong>4. Clustering Keys for Incremental Models</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    cluster_by=['order_date', 'customer_id']
) }}</pre>
<p><strong>Result:</strong> Faster scans on large fact tables</p>
<p><strong>5. Transient Tables for Intermediate Results</strong></p>
<pre>{{ config(
    materialized='table',
    transient=true  # Auto-drops after 24 hours if not queried
) }}</pre>
<p><strong>Use case:</strong> Bulk loading temporary data that's refreshed daily</p>
<p><strong>6. Execute Batch Loading with Dynamic SQL</strong></p>
<pre>{% if execute %}
    {% set batch_size = 10000 %}
    {% set total_rows = run_query("SELECT COUNT(*) FROM source").columns[0][0] %}
    {% for offset in range(0, total_rows, batch_size) %}
        INSERT INTO target SELECT * FROM source LIMIT {{ batch_size }} OFFSET {{ offset }};
    {% endfor %}
{% endif %}</pre>
<p><strong>Advanced: dbt Config per Profile</strong></p>
<pre>profiles.yml:
dev:
  target: dev
  outputs:
    dev:
      type: snowflake
      warehouse: xs_wh
      threads: 1

prod:
  target: prod
  outputs:
    prod:
      type: snowflake
      warehouse: xl_wh
      threads: 8</pre>
<p><strong>Interview line:</strong> "Small warehouses for dev, large for prod, clustering for fact tables, transient for temps. This cuts costs by 40%."</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q72">
<div class="question-title">Data Contracts &amp; Dynamic Testing in dbt</div>
<div class="answer">
<p><strong>Contracts enforce schema guarantees between producers and consumers. They detect breaking changes before they reach dashboards.</strong></p>
<p><strong>Enforcing Contracts via schema.yml</strong></p>
<pre>models:
  - name: fct_revenue
    description: "Revenue facts – daily granularity"
    config:
      contract:
        enforced: true
    columns:
      - name: order_id
        description: "Foreign key to orders"
        data_type: integer
        constraints:
          - type: not_null
          - type: unique
      - name: order_amount
        description: "Amount in USD"
        data_type: numeric(10, 2)
        constraints:
          - type: not_null
          - type: check
            expression: "order_amount &gt; 0"</pre>
<p><strong>What enforced contracts do:</strong></p>
<ul>
<li>dbt fails build if column order changes</li>
<li>dbt fails if data types don't match</li>
<li>dbt fails if NOT NULL dropped</li>
<li>Prevents accidental schema breaking changes</li>
</ul>
<p><strong>Dynamic Testing Pattern (Anomaly Detection)</strong></p>
<pre>{% macro anomaly_detection(model, column_name, threshold=2.5) %}
    with baseline_stats as (
        select
            avg({{ column_name }}) as avg_val,
            stddev_pop({{ column_name }}) as stddev_val
        from {{ model }}
        where date_trunc('day', created_at) &gt;= current_date - 30
    ),
    today_stats as (
        select
            avg({{ column_name }}) as avg_val
        from {{ model }}
        where date_trunc('day', created_at) = current_date
    )
    select *
    from today_stats, baseline_stats
    where abs((today_stats.avg_val - baseline_stats.avg_val) / baseline_stats.stddev_val) &gt; {{ threshold }}
{% endmacro %}</pre>
<p><strong>Use in test:</strong></p>
<pre>{{ anomaly_detection(ref('fct_revenue'), 'order_amount', 3.0) }}</pre>
<p><strong>Result:</strong> Detects unusual spikes/drops in amounts (e.g., bot traffic, data quality issues)</p>
<p><strong>Mutual Exclusivity Testing</strong></p>
<pre>with exclusive_check as (
    select *
    from {{ ref('fct_orders') }}
    where status IN ('completed', 'cancelled', 'pending')
      and status IS NULL
)
select * from exclusive_check</pre>
<p><strong>Interview line:</strong> "Contracts catch schema breaking changes at parse time. Dynamic tests catch data anomalies at execution time."</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q73">
<div class="question-title">CI/CD Workflows in dbt Cloud &amp; Git-Driven Development</div>
<div class="answer">
<p><strong>Modern dbt development = Git + dbt Cloud + Automated Testing. This is how seniors ship reliable changes.</strong></p>
<p><strong>Git Workflow in dbt Projects</strong></p>
<pre>Workflow:
1. Create feature branch: git checkout -b feature/new-metric
2. Develop locally: dbt run -s models/marts/metrics
3. Test locally: dbt test -s models/marts/metrics
4. Commit: git commit -m "Add revenue metric for Q1"
5. Push &amp; Open PR: git push origin feature/new-metric
6. dbt Cloud runs automated tests on PR
7. Merge to main after approval
8. dbt Cloud runs full CI/CD pipeline in prod</pre>
<p><strong>dbt Cloud PR Checks (CI/CD)</strong></p>
<pre>dbt_project.yml:
vars:
  dbt_environment: "{{ env_var('DBT_ENV', 'dev') }}"</pre>
<p><strong>What happens on PR:</strong></p>
<ul>
<li>dbt parses all models (catches syntax errors early)</li>
<li>Runs <code>dbt test</code> on changed models + downstream</li>
<li>Produces diff report (new rows, changed columns)</li>
<li>Shows estimated cost impact</li>
<li>Blocks merge if tests fail</li>
</ul>
<p><strong>State-Based Testing (Only Check What Changed)</strong></p>
<pre>dbt test --select state:modified+
# Tests only changed models + downstream dependencies
# Saves time vs full suite</pre>
<p><strong>Behind the scenes:</strong> dbt tracks manifest.json (DAG) in Git, compares branches</p>
<p><strong>Deployment Best Practice: Slim CI</strong></p>
<pre>dbt run --select state:modified+ --threads 8
dbt test --select state:modified+ --threads 8</pre>
<p><strong>Result:</strong> 30-second CI pipeline instead of 10-minute full run</p>
<p><strong>Production Deployment Strategy</strong></p>
<pre>Deployment Job in dbt Cloud:
1. dbt run --select state:modified+
2. dbt test
3. dbt snapshot (if needed)
4. Rollback if tests fail (automated)</pre>
<p><strong>Advanced: Webhook Triggers</strong></p>
<pre>Webhook → dbt Cloud Job when:
- Upstream data loaded to Snowflake
- Schedule (daily 2 AM UTC)
- Manual trigger from dashboard</pre>
<p><strong>Team Guardrails</strong></p>
<ul>
<li>No one can edit prod without PR approval</li>
<li>dbt Cloud enforces PR checks</li>
<li>Secrets in environment variables (never in code)</li>
<li>Audit trail in Git history</li>
</ul>
<p><strong>Interview power line:</strong> "State-based CI testing catches breaking changes in seconds. dbt Cloud makes data pipelines as reliable as software deployments."</p>
<p><strong>Senior gotcha:</strong> "Developers think 'dbt run' works locally, then fail in PR because of missing upstream data. Always test against prod database in CI."</p>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div><div class="question" id="q74">
<div class="question-title">dbt Pre-Hooks &amp; Post-Hooks – Advanced Execution Control</div>
<div class="answer">
<p><strong>Pre-hooks and post-hooks execute arbitrary SQL before/after model runs. They're essential for setup, cleanup, and auditing in production pipelines.</strong></p>
<p><strong>What Are Pre-Hooks &amp; Post-Hooks?</strong></p>
<ul>
<li><strong>Pre-hook:</strong> SQL that runs BEFORE a model is built</li>
<li><strong>Post-hook:</strong> SQL that runs AFTER a model is built successfully</li>
</ul>
<p><strong>Hooks don't run if the model build fails (unless on-first-run is set).</strong></p>
<p><strong>Pattern 1: Timezone Configuration (Pre-Hook)</strong></p>
<p><strong>Problem:</strong> Different teammates have different session timezones, causing inconsistent date calculations</p>
<pre>{{ config(
    pre_hook="ALTER SESSION SET TIMEZONE = 'UTC'",
    post_hook="ALTER SESSION SET TIMEZONE = 'America/New_York'"
) }}

select
    current_timestamp as event_time,  -- Always UTC
    order_date,
    amount
from {{ ref('stg_orders') }}</pre>
<p><strong>Result:</strong> All date arithmetic runs in UTC regardless of user timezone</p>
<p><strong>Pattern 2: Grant Permissions (Post-Hook)</strong></p>
<p><strong>Problem:</strong> BI team can't query new marketing mart models</p>
<pre>{{ config(
    materialized='table',
    post_hook="GRANT SELECT ON {{ this }} TO ROLE analytics_viewer"
) }}

select
    customer_id,
    total_orders,
    ltv
from {{ ref('int_customer_metrics') }}</pre>
<p><strong>Result:</strong> Model automatically grants read access to analytics_viewer when built</p>
<p><strong>Pattern 3: Audit Logging (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Track when models are refreshed for compliance/SLA monitoring</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    post_hook="INSERT INTO audit_log (table_name, load_timestamp, row_count, status) 
               SELECT '{{ this.name }}', current_timestamp, COUNT(*), 'SUCCESS' FROM {{ this }}"
) }}

select * from {{ ref('stg_orders') }}</pre>
<p><strong>Audit log result:</strong></p>
<pre>table_name     load_timestamp              row_count  status
fct_orders     2025-02-08 10:15:30.123     1,245,678  SUCCESS
fct_orders     2025-02-08 09:15:05.456     1,244,932  SUCCESS</pre>
<p><strong>Pattern 4: Data Quality Check (Post-Hook with Conditional Fail)</strong></p>
<p><strong>Problem:</strong> Need to fail model if row count drops &gt; 10% from yesterday</p>
<pre>{{ config(
    materialized='table',
    post_hook=[
        "INSERT INTO dbt_audit_checks (model_name, check_type, status, details)
         SELECT 
            '{{ this.name }}',
            'row_count_validation',
            CASE 
                WHEN (SELECT COUNT(*) FROM {{ this }}) &lt; (SELECT COUNT(*) FROM {{ this }}_prev) * 0.9
                THEN 'FAILED'
                ELSE 'PASSED'
            END,
            'Expected ' || (SELECT COUNT(*) FROM {{ this }}_prev) || ' rows, got ' || (SELECT COUNT(*) FROM {{ this }})
        "
    ]
) }}

select * from {{ ref('fct_revenue') }}</pre>
<p><strong>Pattern 5: Snapshot &amp; Archive (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Automatically snapshot fact table before incremental refresh (backup safety)</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    post_hook="CREATE TABLE {{ this }}_snapshot_{{ today() }} AS SELECT * FROM {{ this }}"
) }}</pre>
<p><strong>Result:</strong> Automatic daily backups (fct_orders_snapshot_2025-02-08, etc.)</p>
<p><strong>Pattern 6: Cluster Keys Management (Pre-Hook)</strong></p>
<p><strong>Problem:</strong> Need to reclustered table without rebuilding data</p>
<pre>{{ config(
    materialized='table',
    pre_hook="ALTER TABLE {{ this }} CLUSTER BY (order_date, customer_id)",
    post_hook="SELECT SYSTEM$CLUSTERING_INFORMATION('{{ this }}')"
) }}</pre>
<p><strong>Why:</strong> Clustering improves scan performance on large tables by 3-5x</p>
<p><strong>Pattern 7: Dynamic Statistics Update (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Update table statistics for query optimizer (Snowflake performance)</p>
<pre>{{ config(
    materialized='table',
    post_hook="ANALYZE TABLE {{ this }} COMPUTE STATS"
) }}</pre>
<p><strong>Why:</strong> Outdated stats lead to poor query plans; forces re-analysis</p>
<p><strong>Hook Execution Order</strong></p>
<pre>1. Pre-hook (before model build)
2. Model SQL executes
3. Post-hook (after successful build)
4. Cleanup (drop old table if full refresh)</pre>
<p><strong>Multiple Hooks (Array Syntax)</strong></p>
<p><strong>You can chain multiple post-hooks:</strong></p>
<pre>{{ config(
    post_hook=[
        "GRANT SELECT ON {{ this }} TO ROLE analytics",
        "INSERT INTO audit_log VALUES ('{{ this.name }}', current_timestamp)",
        "CALL refresh_downstream_views('{{ this.name }}')"
    ]
) }}</pre>
<p><strong>They execute in order; if any fails, subsequent hooks don't run.</strong></p>
<p><strong>Conditional Hooks (if-then Logic)</strong></p>
<p><strong>Run hooks only in production:</strong></p>
<pre>{% if target.name == 'prod' %}
    {{ config(
        post_hook="GRANT SELECT ON {{ this }} TO ROLE bi_team"
    ) }}
{% endif %}</pre>
<p><strong>Common pattern:</strong> Permissions+grants in prod only, skip in dev</p>
<p><strong>Hook Access to dbt Context</strong></p>
<p><strong>Inside hooks, you can access:</strong></p>
<ul>
<li><code>{{ this }}</code> – Current table/view name</li>
<li><code>{{ this.name }}</code> – Table name only</li>
<li><code>{{ target.name }}</code> – Environment (dev/prod)</li>
<li><code>{{ env_var('VAR_NAME') }}</code> – Environment variables</li>
<li><code>current_timestamp</code> – Timestamp function</li>
</ul>
<p><strong>Anti-Patterns to Avoid</strong></p>
<ul>
<li>Don't grant permissions in pre-hook (table doesn't exist yet)</li>
<li>Don't query {{ this }} in pre-hook (old table still exists)</li>
<li>Don't fail tests in post-hooks (use dbt test instead)</li>
<li>Avoid expensive operations in post-hooks (slows down CI/CD)</li>
</ul>
<p><strong>Interview Power Lines</strong></p>
<ul>
<li><strong>"Pre/post-hooks automate operational setup without modifying SQL logic."</strong></li>
<li><strong>"Post-hook grants make permission management scale with new models."</strong></li>
<li><strong>"Audit logging via post-hooks creates compliance trails automatically."</strong></li>
<li><strong>"Hooks can cluster tables and update statistics without full rebuild."</strong></li>
</ul>
<p><strong>Real Production Example: Complete Flow</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    pre_hook=[
        "ALTER SESSION SET TIMEZONE = 'UTC'",
        "ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD'"
    ],
    post_hook=[
        "GRANT SELECT ON {{ this }} TO ROLE analytics_viewer",
        "INSERT INTO dbt_audit_log (model, loaded_at, rows) SELECT '{{ this.name }}', current_timestamp, COUNT(*) FROM {{ this }}",
        "ALTER TABLE {{ this }} CLUSTER BY (order_date, customer_id)",
        "CALL refresh_downstream_materialized_views('{{ this.name }}')"
    ]
) }}

select
    order_id,
    order_date,
    customer_id,
    amount
from {{ ref('stg_orders') }}
where order_date &gt;= '2025-01-01'</pre>
<a class='\"back-to-toc\"' href='\"#table-of-contents\"'>↑ Back to Contents</a>
<a class="back-to-toc" href="#toc">⬆ Back to Contents</a>
</div>
</div></div></body></html>