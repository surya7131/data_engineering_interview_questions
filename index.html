<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Complete Snowflake &amp; Data Engineering Interview Guide - 74 Questions</title>

<link href="style.css" rel="stylesheet"/></head>
<body>
<div class="container">
<header>
<h1>Complete Snowflake &amp; Data Engineering Interview Guide</h1>
<p class="subtitle">Comprehensive Questions with Detailed Answers</p>
</header>
<div class="toc-container" id="toc">
<h2>ðŸ“˜ Contents</h2>
<ul>
<li><a href="#q1">Q1: Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</a></li>
<li><a href="#q2">Q2: Snowflake Architecture</a></li>
<li><a href="#q3">Q3: Time-Travel with scenarios like offset, query_id and timestamp</a></li>
<li><a href="#q4">Q4: Optimize the query performance and how you do it</a></li>
<li><a href="#q5">Q5: What is partition and what is micro-partitions</a></li>
<li><a href="#q6">Q6: SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</a></li>
<li><a href="#q7">Q7: How you ingested JSON data into Snowflake, what steps you perform</a></li>
<li><a href="#q8">Q8: If the file size is 50GB, how you will ingest the data</a></li>
<li><a href="#q9">Q9: Internal and External stages in Snowflake - Uses for each</a></li>
<li><a href="#q10">Q10: How you schedule a data pipeline in Snowflake</a></li>
<li><a href="#q11">Q11: Stored Procedure</a></li>
<li><a href="#q12">Q12: User Defined Functions (UDF)</a></li>
<li><a href="#q13">Q13: What is Git</a></li>
<li><a href="#q14">Q14: What is QUALIFY in Snowflake - Why it's used</a></li>
<li><a href="#q15">Q15: Benefits of Snowflake</a></li>
<li><a href="#q16">Q16: What is CDC (Change Data Capture) - Is it tool or term</a></li>
<li><a href="#q17">Q17: Python questions - List and Tuple</a></li>
<li><a href="#q18">Q18: AVRO, Parquet, ORC file formats - Uses</a></li>
<li><a href="#q19">Q19: Difference between CTE and Temp table - Where to use</a></li>
<li><a href="#q20">Q20: Difference between Transient and Dynamic and Dynamic Transient tables</a></li>
<li><a href="#q21">Q21: How you will handle schema changes in the downstream</a></li>
<li><a href="#q22">Q22: Ephemeral vs Permanent tables</a></li>
<li><a href="#q23">Q23: How to implement CDC without ETL tools in Snowflake</a></li>
<li><a href="#q24">Q24: What is DBT and uses - Execution plan</a></li>
<li><a href="#q25">Q25: Why continuous data load used - Use cases</a></li>
<li><a href="#q26">Q26: Explain end-to-end data pipeline - What are the logics you followed</a></li>
<li><a href="#q27">Q27: Why implement CDC instead of other approaches - Use case</a></li>
<li><a href="#q28">Q28: Snowflake Tasks - Troubleshooting failed queries and performance improvement</a></li>
<li><a href="#q29">Q29: What is Zero Copy clone - Uses</a></li>
<li><a href="#q30">Q30: Convert timestamp from one timezone to another</a></li>
<li><a href="#q31">Q31: Clustering Keys in Snowflake</a></li>
<li><a href="#q32">Q32: How to monitor Snowflake performance</a></li>
<li><a href="#q33">Q33: Attacama and Collibra - Uses and Differences</a></li>
<li><a href="#q34">Q34: How to optimize long running queries - How to reduce time</a></li>
<li><a href="#q35">Q35: What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</a></li>
<li><a href="#q36">Q36: Max Salary for each department and 4th max salary for each department</a></li>
<li><a href="#q37">Q37: Swap the gender value for a table - Male to Female, Female to Male</a></li>
<li><a href="#q38">Q38: DBT Project Architecture</a></li>
<li><a href="#q39">Q39: Difference between UPSERT and MERGE</a></li>
<li><a href="#q40">Q40: Data Masking and Masking Policy</a></li>
<li><a href="#q41">Q41: Difference between Hashing and Encryption</a></li>
<li><a href="#q42">Q42: Snowflake storage integration with AWS S3</a></li>
<li><a href="#q43">Q43: What is DBT and what problem does it solve in modern data stack</a></li>
<li><a href="#q44">Q44: Core components of a DBT project: Models, Tests, Seeds</a></li>
<li><a href="#q45">Q45: Materializations in DBT - Four default types</a></li>
<li><a href="#q46">Q46: Managing dependencies and referencing models in DBT</a></li>
<li><a href="#q47">Q47: What is Jinja in DBT - Simple example</a></li>
<li><a href="#q48">Q48: Data Governance best practices</a></li>
<li><a href="#q49">Q49: Data quality testing strategies</a></li>
<li><a href="#q50">Q50: Building a modern data stack - Technology selection</a></li>
<li><a href="#q51">Q51: Future trends in data engineering</a></li>
<li><a href="#q52">Q52: Explain your end-to-end data pipeline (tools + flow)</a></li>
<li><a href="#q53">Q53: How do you use Python inside Airflow for orchestration and failure alerts?</a></li>
<li><a href="#q54">Q54: How do you implement SCD Type 2 in dbt?</a></li>
<li><a href="#q55">Q55: How does dbt snapshot handle deletes?</a></li>
<li><a href="#q56">Q56: When and why do you use dbt seeds?</a></li>
<li><a href="#q57">Q57: SQL Challenge â€“ Orders and Payments</a></li>
<li><a href="#q58">Q58: dbt Materializations â€” Syntax &amp; When to Use</a></li>
<li><a href="#q59">Q59: dbt Snapshots â€” Syntax &amp; Purpose</a></li>
<li><a href="#q60">Q60: dbt YAML Files â€” Syntax, Indentation &amp; Purpose</a></li>
<li><a href="#q61">Q61: Tests in YAML â€” Built-in &amp; Relationships</a></li>
<li><a href="#q62">Q62: Materialization Configuration via dbt_project.yml</a></li>
<li><a href="#q63">Q63: Quick Interview Cheat Table &amp; Power Lines</a></li>
<li><a href="#q64">Q64: Advanced Incremental Strategies â€“ MERGE vs APPEND</a></li>
<li><a href="#q65">Q65: Custom dbt Tests â€“ Beyond Built-ins</a></li>
<li><a href="#q66">Q66: Enterprise-Grade dbt Folder Structure</a></li>
<li><a href="#q67">Q67: Production dbt Failures &amp; Root Causes (What Seniors Face)</a></li>
<li><a href="#q68">Q68: Senior-Level dbt Cheat Sheet &amp; Interview Power Lines</a></li>
<li><a href="#q69">Q69: Advanced dbt Macros &amp; Jinja Templating Patterns</a></li>
<li><a href="#q70">Q70: dbt Exposures &amp; Metrics â€“ Connecting to BI &amp; Analytics</a></li>
<li><a href="#q71">Q71: dbt on Snowflake: Performance Optimization &amp; Run Config</a></li>
<li><a href="#q72">Q72: Data Contracts &amp; Dynamic Testing in dbt</a></li>
<li><a href="#q73">Q73: CI/CD Workflows in dbt Cloud &amp; Git-Driven Development</a></li>
<li><a href="#q74">Q74: dbt Pre-Hooks &amp; Post-Hooks â€“ Advanced Execution Control</a></li>
</ul>
</div>
<div class="question" id="q1">
<div class="question-title">Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</div>
<div class="answer">
<p>This requires interpreting the start and end events to calculate the duration. Assuming Date_timestamp is a DATETIME or TIMESTAMP column.</p>
<pre>WITH EventSequence AS (
    SELECT
        Date_timestamp,
        progress,
        LAG(Date_timestamp) OVER (ORDER BY Date_timestamp) AS Previous_Timestamp,
        LAG(progress) OVER (ORDER BY Date_timestamp) AS Previous_Progress
    FROM your_table_name
),
ComputeDurations AS (
    SELECT
        Date_timestamp,
        progress,
        Previous_Timestamp,
        Previous_Progress,
        CASE
            WHEN progress = 'end' AND Previous_Progress = 'start'
            THEN DATEDIFF(SECOND, Previous_Timestamp, Date_timestamp)
            ELSE 0
        END AS Duration_Seconds
    FROM EventSequence
    WHERE progress = 'end' AND Previous_Progress = 'start'
)
SELECT
    SUM(Duration_Seconds) / 3600.0 AS Total_Compute_Hours_Overall,
    TO_DATE(Date_timestamp) AS Compute_Date,
    SUM(Duration_Seconds) / 3600.0 AS Daily_Compute_Hours
FROM ComputeDurations
GROUP BY TO_DATE(Date_timestamp)
ORDER BY Compute_Date;</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>EventSequence:</strong> This CTE assigns the Previous_Timestamp and Previous_Progress to each row</li>
<li><strong>ComputeDurations:</strong> Calculates the Duration_Seconds only for valid end events preceded by a start event</li>
<li><strong>Final SELECT:</strong> Sums durations to get Total and groups by date for Daily_Compute_Hours</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q2">
<div class="question-title">Snowflake Architecture</div>
<div class="answer">
<p>Snowflake's architecture is a unique multi-cluster shared data architecture. It separates compute and storage, allowing them to scale independently, and includes a cloud services layer for management.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Database Storage:</strong> Data is reorganized into a columnar, compressed format, encrypted, and stored in micro-partitions. This storage layer is elastic and can scale dynamically.</li>
<li><strong>Query Processing (Compute Layer):</strong> Consists of virtual warehouses - independent compute clusters that execute queries without sharing compute resources.</li>
<li><strong>Cloud Services Layer:</strong> The brain of Snowflake, coordinating all activities including authentication, metadata management, query optimization, infrastructure management, and transaction management.</li>
</ul>
<p><strong>Analogy:</strong> Imagine a library where storage is the books on shelves, compute is the reading rooms, and cloud services is the librarian system managing everything.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q3">
<div class="question-title">Time-Travel with scenarios like offset, query_id and timestamp</div>
<div class="answer">
<p>Snowflake's Time Travel feature allows you to query historical data that has been changed or deleted within a specified retention period (default 1 day, up to 90 days for Enterprise Edition).</p>
<p><strong>Using AT (OFFSET):</strong> Query data from a specific point in time relative to current time (negative offset for past).</p>
<pre>-- Current data
SELECT * FROM my_table;

-- Data from 5 minutes (300 seconds) ago
SELECT * FROM my_table AT (OFFSET =&gt; -300);</pre>
<p><strong>Using AT (TIMESTAMP):</strong> Query data as it existed at an exact past timestamp.</p>
<pre>SELECT * FROM my_table AT (TIMESTAMP =&gt; '2025-06-25 10:30:00'::TIMESTAMP_LTZ);</pre>
<p><strong>Using BEFORE (STATEMENT):</strong> Query data immediately before a specific DML statement executed.</p>
<pre>CREATE TABLE my_table_recovery AS
SELECT * FROM my_table BEFORE (STATEMENT =&gt; '123abc456def');</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q4">
<div class="question-title">Optimize the query performance and how you do it</div>
<div class="answer">
<p><strong>Key Optimization Strategies:</strong></p>
<ol>
<li><strong>Choose the Right Virtual Warehouse Size:</strong> Larger warehouses provide more compute power and memory. Use larger warehouses for complex queries on large datasets.</li>
<li><strong>Cluster Your Tables:</strong> Define clustering keys on columns frequently used in WHERE clauses, JOIN conditions, or GROUP BY clauses.</li>
<li><strong>Use Materialized Views:</strong> Pre-compute and store results of complex queries, automatically updating when base tables change.</li>
<li><strong>Query Pruning:</strong> Ensure your WHERE clauses are effective. Filter on clustered columns or columns with high cardinality.</li>
<li><strong>Effective Caching:</strong> Leverage Snowflake's Result Cache and Warehouse Cache by running the same queries multiple times.</li>
<li><strong>Avoid Anti-Patterns:</strong> Don't use SELECT *, avoid correlated subqueries, eliminate unnecessary ORDER BY or DISTINCT.</li>
<li><strong>Monitor Query Profile:</strong> Use Snowflake's Query Profile to identify bottlenecks and optimize accordingly.</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q5">
<div class="question-title">What is partition and what is micro-partitions</div>
<div class="answer">
<p><strong>Partition (General Database Concept):</strong> A strategy to divide a large table into smaller, manageable pieces based on a specified column (e.g., date, region). Usually managed explicitly by the database administrator.</p>
<p><strong>Micro-partitions (Snowflake Specific):</strong> Snowflake automatically organizes all data into immutable, compressed, columnar units typically ranging from 50 MB to 500 MB. This is automatic and transparent.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>Automatic and Transparent - handled by Snowflake</li>
<li>Rich metadata stored about each micro-partition (value ranges, distinct values, null counts)</li>
<li>Query Pruning - metadata allows Snowflake to skip irrelevant micro-partitions</li>
<li>Clustering - you can define clustering keys to optimize physical co-location of data</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q6">
<div class="question-title">SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</div>
<div class="answer">
<p><strong>SCD Type 0: Retain Original</strong> - No changes tracked. Dimension attribute value never changes.</p>
<p><strong>SCD Type 1: Overwrite</strong> - New data overwrites old data. History is not preserved.</p>
<p><strong>SCD Type 2: Add New Row</strong> - New row added with effective date ranges. History is fully preserved.</p>
<pre>Initial: EmployeeID: 201, Department: Sales, StartDate: 2020-01-01, EndDate: 9999-12-31, IsCurrent: TRUE
After Change: Department changes to Marketing
Result: 
- Old row: EndDate: 2025-05-31, IsCurrent: FALSE
- New row: Department: Marketing, StartDate: 2025-06-01, IsCurrent: TRUE</pre>
<p><strong>SCD Type 3: Add New Attribute</strong> - New column added for previous value. Partial history.</p>
<p><strong>SCD Type 4: Use History Table</strong> - Main table holds current (Type 1), separate history table stores all past versions (Type 2).</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q7">
<div class="question-title">How you ingested JSON data into Snowflake, what steps you perform</div>
<div class="answer">
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>Prepare JSON Data:</strong> Ensure well-formed JSON files (individual objects or newline-delimited)</li>
<li><strong>Stage the Files:</strong> Use Internal Stage (PUT command) or External Stage (S3, Azure, GCP)</li>
<li><strong>Create File Format:</strong> Define how to interpret JSON files
                        <pre>CREATE FILE FORMAT json_file_format
    TYPE = 'JSON'
    STRIP_OUTER_ARRAY = TRUE
    NULL_IF = ('', 'NULL');</pre>
</li>
<li><strong>Create Target Table:</strong> Either single VARIANT column or structured columns
                        <pre>CREATE TABLE raw_json_data (
    id INT,
    json_payload VARIANT,
    load_timestamp TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);</pre>
</li>
<li><strong>Copy Data:</strong> Use COPY INTO to load data from stage
                        <pre>COPY INTO raw_json_data
FROM @my_external_json_stage/
FILE_FORMAT = json_file_format
ON_ERROR = 'CONTINUE';</pre>
</li>
<li><strong>Query &amp; Transform:</strong> Use VARIANT functions to extract and transform data</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q8">
<div class="question-title">If the file size is 50GB, how you will ingest the data</div>
<div class="answer">
<p><strong>For 50GB files, the approach is:</strong></p>
<ol>
<li><strong>Stage the File:</strong> Use External Stage (S3, Azure, GCP) - faster and more reliable than internal stage</li>
<li><strong>Use Large Virtual Warehouse:</strong> Use at least X-Large or larger (2X-Large, 3X-Large) for parallel processing and sufficient memory</li>
<li><strong>Execute COPY INTO:</strong>
<pre>USE WAREHOUSE MY_LARGE_WAREHOUSE;

COPY INTO your_target_table
FROM @my_external_stage/your_50gb_file.csv
FILE_FORMAT = (FORMAT_NAME = my_csv_file_format)
ON_ERROR = 'CONTINUE'
PURGE = TRUE;</pre>
</li>
<li><strong>Key Considerations:</strong>
<ul>
<li>Use columnar formats (Parquet, ORC) if possible</li>
<li>Ensure good network bandwidth</li>
<li>Monitor COPY_HISTORY for status</li>
<li>COPY INTO is inherently parallel - larger warehouse leverages this</li>
</ul>
</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q9">
<div class="question-title">Internal and External stages in Snowflake - Uses for each</div>
<div class="answer">
<p><strong>Internal Stages (Snowflake-managed Storage):</strong></p>
<ul>
<li>User stage: @~/</li>
<li>Table stage: @%table_name</li>
<li>Named stage: CREATE STAGE my_internal_stage</li>
<li>Uses: Quick data loading, secure data transfer, temporary files</li>
</ul>
<p><strong>External Stages (User-managed Cloud Storage):</strong></p>
<ul>
<li>Points to AWS S3, Azure Blob, Google Cloud Storage</li>
<li>Uses: Large-scale automated loads, data lake integration, Snowpipe, continuous loading</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Internal Stage</th>
<th>External Stage</th>
</tr>
<tr>
<td>Storage</td>
<td>Snowflake managed</td>
<td>Your cloud storage</td>
</tr>
<tr>
<td>File Access</td>
<td>PUT / GET commands</td>
<td>COPY INTO</td>
</tr>
<tr>
<td>Cost</td>
<td>Included in Snowflake</td>
<td>Cloud provider costs</td>
</tr>
<tr>
<td>Ideal Use</td>
<td>Ad-hoc, smaller loads</td>
<td>Large-scale automated</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q10">
<div class="question-title">How you schedule a data pipeline in Snowflake</div>
<div class="answer">
<p><strong>Snowflake Tasks (Native):</strong></p>
<pre>-- Create warehouse for tasks
CREATE WAREHOUSE ETL_WH WAREHOUSE_SIZE = 'XSMALL';

-- 1. Root Task (scheduled)
CREATE TASK load_raw_data
  WAREHOUSE = ETL_WH
  SCHEDULE = 'USING CRON 0 9 * * * Asia/Kolkata'
  AS
  COPY INTO RAW_TABLE FROM @my_external_stage/raw_files/;

-- 2. Child Task (depends on load_raw_data)
CREATE TASK transform_data
  WAREHOUSE = ETL_WH
  AFTER load_raw_data
  AS
  INSERT INTO STAGING_TABLE SELECT ... FROM RAW_TABLE;

-- Enable tasks
ALTER TASK load_raw_data RESUME;
ALTER TASK transform_data RESUME;</pre>
<p><strong>External Orchestration Tools:</strong> Apache Airflow, AWS Step Functions, Azure Data Factory, Control-M, Autosys - for complex cross-platform pipelines.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q11">
<div class="question-title">Stored Procedure</div>
<div class="answer">
<p>A Stored Procedure is a set of SQL statements and procedural logic compiled and stored in the database. It can be executed by calling its name with input parameters.</p>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Encapsulation of complex logic</li>
<li>Reusability - write once, run many times</li>
<li>Parameterization - accepts input parameters</li>
<li>Better performance - pre-compiled</li>
<li>Enhanced security - grant permissions on procedure, not underlying tables</li>
</ul>
<pre>CREATE PROCEDURE calculate_daily_sales(sales_date DATE)
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE
    total_sales DECIMAL(18, 2);
BEGIN
    DELETE FROM DAILY_SALES_SUMMARY WHERE summary_date = :sales_date;
    SELECT SUM(amount) INTO total_sales FROM RAW_SALES_DATA WHERE sale_date = :sales_date;
    INSERT INTO DAILY_SALES_SUMMARY VALUES (:sales_date, :total_sales);
    IF (total_sales IS NULL) THEN
        RETURN 'No sales data found for ' || :sales_date;
    ELSE
        RETURN 'Successfully summarized for ' || :sales_date;
    END IF;
END;
$$;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q12">
<div class="question-title">User Defined Functions (UDF)</div>
<div class="answer">
<p>A User Defined Function is a custom function that performs specific operations, similar to built-in functions. UDFs encapsulate logic that can be reused within SQL queries.</p>
<p><strong>Types:</strong></p>
<ul>
<li>Scalar UDFs - return single value per input row</li>
<li>Table UDFs (UDTFs) - return set of rows per input row</li>
</ul>
<p><strong>Scalar UDF Example:</strong></p>
<pre>CREATE FUNCTION calculate_net_sales(sales_amount DECIMAL(10,2), return_amount DECIMAL(10,2))
RETURNS DECIMAL(10,2)
AS
$$
    sales_amount - COALESCE(return_amount, 0)
$$;

SELECT order_id, calculate_net_sales(sales_amount, return_amount) AS net_sales
FROM daily_transactions;</pre>
<p><strong>Languages Supported:</strong> SQL, JavaScript, Python, Java, Scala (via Snowpark)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q13">
<div class="question-title">What is Git</div>
<div class="answer">
<p>Git is a free, open-source distributed version control system (DVCS) designed to handle projects of any size. It tracks changes in source code and other files during software development, enabling collaborative work and complete change history.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Distributed:</strong> Every developer has a complete copy of the entire repository with full history. No single point of failure</li>
<li><strong>Version Control:</strong> Tracks every change with author, timestamp, and message. Complete history allows reverting to any previous state</li>
<li><strong>Branches:</strong> Parallel lines of development (main, develop, feature branches) allowing teams to work independently</li>
<li><strong>Commits:</strong> Snapshots of the entire repository at specific points with unique SHA-1 hash identifiers</li>
<li><strong>Repository:</strong> Complete collection of files, branches, and entire change history (can be local or on remote servers like GitHub)</li>
<li><strong>Staging Area (Index):</strong> Intermediate area where you select which changes to include in the next commit</li>
</ul>
<p><strong>Typical Workflow:</strong></p>
<pre>git clone https://github.com/user/repo.git  # Clone remote repo
git branch feature/new-analysis              # Create feature branch
git checkout feature/new-analysis             # Switch to feature branch
git add analysis_script.sql                   # Stage changes
git commit -m "Add new sales analysis"        # Commit with message
git push origin feature/new-analysis          # Push to remote
# Create pull request on GitHub for review
git checkout main                             # Switch back to main
git pull origin main                          # Get latest main
git merge feature/new-analysis                # Merge feature branch</pre>
<p><strong>For Data Teams:</strong> Git is essential for version controlling DBT projects, SQL scripts, Python ETL code, and documentation. Enables code review, audit trails, and rollback capabilities</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q14">
<div class="question-title">What is QUALIFY in Snowflake - Why it's used</div>
<div class="answer">
<p>QUALIFY is a clause unique to Snowflake that filters results of window functions without needing to wrap queries in subqueries or CTEs.</p>
<p><strong>Example: Find the 2nd highest paid employee in each department</strong></p>
<p><strong>With QUALIFY (Simpler):</strong></p>
<pre>SELECT employee_id, employee_name, department, salary
FROM employees
QUALIFY RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 2;</pre>
<p><strong>Without QUALIFY (requires CTE):</strong></p>
<pre>WITH EmployeeRank AS (
    SELECT ..., RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rnk
    FROM employees
)
SELECT * FROM EmployeeRank WHERE rnk = 2;</pre>
<p><strong>Benefits:</strong> Simplicity, readability, potential performance improvements, direct filtering on window function results</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q15">
<div class="question-title">Benefits of Snowflake</div>
<div class="answer">
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Separation of Compute and Storage:</strong> Scale independently - add storage without compute cost. Run small XS warehouse for lightweight queries, large 3X warehouse for complex jobs. Only pay for compute when warehouse is running</li>
<li><strong>Elasticity and Scalability:</strong> Instantly resize warehouses (XS to 3X in seconds) or auto-scale with multi-cluster warehouses for concurrency</li>
<li><strong>Near-Zero Management:</strong> Fully managed - Snowflake handles patching, updates, hardware provisioning, index maintenance. No DBAs needed for infrastructure</li>
<li><strong>Support for Semi-Structured Data:</strong> Native VARIANT type handles JSON/Avro/Parquet natively. Query nested data with dot notation without flattening</li>
<li><strong>Concurrency:</strong> Multiple independent virtual warehouses access same data simultaneously without locking or contention. BI team on WH1, ETL team on WH2, no impact</li>
<li><strong>Data Sharing:</strong> Secure, instantaneous live data sharing with other Snowflake accounts (same or different regions). Share without copying data</li>
<li><strong>Time Travel &amp; Fail-safe:</strong> Query data as it existed 1-90 days ago. Accidentally dropped table? UNDROP within retention period. 7-day fail-safe for disaster recovery</li>
<li><strong>Performance Optimization:</strong> Automatic micro-partitioning with intelligent pruning, multi-layer caching (result + warehouse cache), optional clustering keys</li>
<li><strong>Security &amp; Compliance:</strong> AES-256 encryption at rest/transit, multi-factor MFA, RBAC + object-level privileges, row/column masking, SOC2/PCI/HIPAA/GDPR certified</li>
<li><strong>Ecosystem Integration:</strong> Native connectors for Tableau, Power BI, Looker. DBT integration, Spark through connectors, Python/Pandas via snowpark_python</li>
</ul>
<p><strong>Cost Efficiency Example:</strong> Traditional data warehouse requires expensive hardware, undergoes periods of low utilization (pay for unused capacity). Snowflake: suspend warehouse when not used, pay only for storage. Scale up for batch jobs, down immediately after</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q16">
<div class="question-title">What is CDC (Change Data Capture) - Is it tool or term</div>
<div class="answer">
<p>CDC is primarily a <strong>term and technique</strong> (not a single tool). It identifies and captures changes (INSERT, UPDATE, DELETE) made to data in a source database and delivers those changes to a target system efficiently instead of reprocessing entire datasets.</p>
<p><strong>Common Mechanisms:</strong></p>
<ul>
<li><strong>Timestamp-Based (Polling):</strong> Periodic queries find rows where last_updated_timestamp &gt; previous_check_time. Simple but can miss deletes, slow for large tables, unreliable if timestamps aren't consistent</li>
<li><strong>Log-Based (Most Robust):</strong> Read database transaction logs (MySQL binlog, Oracle redo logs, SQL Server transaction log). Captures ALL changes in real-time, no impact on source DB, handles deletes</li>
<li><strong>Trigger-Based:</strong> Database triggers fire on INSERT/UPDATE/DELETE, write change details to change table. Real-time but adds overhead to source system performance</li>
<li><strong>Hash-Based:</strong> Calculate hash of row, compare snapshots to find differences. Works without source modifications but resource-intensive for large datasets</li>
</ul>
<p><strong>Why CDC Matters:</strong></p>
<pre>-- Without CDC (Full Load Every Time) - INEFFICIENT
COPY INTO warehouse_sales FROM source_db  -- 100GB table
-- Takes 2 hours, processes entire table even if only 1GB changed
-- Heavy impact on source system

-- With CDC (Only Changes) - EFFICIENT
COPY INTO warehouse_sales FROM change_stream  -- Only 1GB changed
-- Takes 5 minutes, minimal source system impact
-- Near real-time data availability</pre>
<p><strong>Popular CDC Tools:</strong> Qlik Replicate (log-based), Fivetran (managed CDC), Debezium (Kafka-based open source), Oracle GoldenGate (enterprise), AWS DMS, Apache Hudi/Iceberg (data lake CDC)</p>
<p><strong>Real Use Cases:</strong> Real-time analytics dashboards, microservices data synchronization, database migration with zero downtime, maintaining dimensional data in data warehouse</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q17">
<div class="question-title">Python questions - List and Tuple</div>
<div class="answer">
<p><strong>List []: Mutable (changeable)</strong> - ordered collection that can be modified after creation</p>
<pre>my_list = [10, "apple", 3.14, True, [1, 2, 3]]  # Can contain mixed types, even nested lists
print(my_list[1])              # Output: apple (zero-indexed)
my_list[0] = 20                # Modify element
my_list.append("banana")       # Add element
my_list.extend([4, 5, 6])      # Add multiple elements
my_list.insert(1, "orange")    # Insert at specific position
my_list.remove("apple")        # Remove by value
popped = my_list.pop()         # Remove and return last element
my_list.sort()                 # Sort in-place
my_list.clear()                # Remove all elements

# Common operations
for item in my_list:
    print(item)  # Iterate

sliced = my_list[1:4]  # Slicing creates new list
reversed_list = my_list[::-1]  # Reverse</pre>
<p><strong>Tuple (): Immutable (unchangeable)</strong> - ordered collection that cannot be modified after creation</p>
<pre>my_tuple = (10, "apple", 3.14, True, (1, 2, 3))  # Can contain mixed types
print(my_tuple[1])           # Output: apple
# my_tuple[0] = 20           # TypeError! Tuples are immutable

# Single element tuple REQUIRES trailing comma
single_tuple = (5,)          # Correct
wrong_single = (5)           # This is just an int, not a tuple!

# Tuple packing and unpacking
coordinates = 10, 20, 30     # Automatic packing into tuple
x, y, z = coordinates        # Unpacking tuple to variables

# Tuples can be used as dictionary keys (lists cannot)
my_dict = {(0, 0): "origin", (1, 1): "diagonal"}

# Operations (read-only)
print(my_tuple.count(10))     # Count occurrences
print(my_tuple.index("apple"))  # Find index
print(len(my_tuple))          # Length</pre>
<table>
<tr>
<th>Feature</th>
<th>List</th>
<th>Tuple</th>
</tr>
<tr>
<td>Mutability</td>
<td>Mutable (changeable)</td>
<td>Immutable (fixed)</td>
</tr>
<tr>
<td>Performance</td>
<td>Slower (tracking changes)</td>
<td>Faster (fixed size)</td>
</tr>
<tr>
<td>Memory</td>
<td>More memory overhead</td>
<td>Less memory usage</td>
</tr>
<tr>
<td>As Dict Key</td>
<td>No - not hashable</td>
<td>Yes - if elements are hashable</td>
</tr>
<tr>
<td>Best Use Case</td>
<td>Dynamic data, frequent changes</td>
<td>Fixed data, constant values</td>
</tr>
<tr>
<td>Return from Function</td>
<td>N/A</td>
<td>Yes - functions return tuples</td>
</tr>
</table>
<p><strong>In Data Engineering:</strong> Tuples used for immutable data records, fixed field orders. Lists used for accumulating/processing data. DBT source definitions, data lineage metadata often use tuples</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q18">
<div class="question-title">AVRO, Parquet, ORC file formats - Uses</div>
<div class="answer">
<p><strong>AVRO (Apache Avro):</strong> Row-oriented data serialization format with self-describing schema</p>
<ul>
<li><strong>Schema Definition:</strong> Schema stored in JSON within each file (or managed externally via schema registry)</li>
<li><strong>Excellent Schema Evolution:</strong> Can add/remove/modify fields with clear rules for backward/forward compatibility</li>
<li><strong>Size:</strong> Compact binary format after serialization</li>
<li><strong>Uses:</strong>
<ul>
<li>Real-time streaming with Apache Kafka (Confluent)</li>
<li>Inter-process communication between services</li>
<li>Data archival where schema changes occur over time</li>
<li>CDC platforms like Debezium</li>
</ul>
</li>
<li><strong>Limitations:</strong> Row-based format, less efficient for OLAP queries (must read many rows to get one column)</li>
</ul>
<p><strong>Parquet (Apache Parquet):</strong> Columnar storage format optimized for analytics</p>
<ul>
<li><strong>Column Storage:</strong> Data organized by column, not row. Reading one column doesn't require reading others</li>
<li><strong>Compression:</strong> Column-level compression (RLE, dictionary encoding). Achieves 10x+ compression on repetitive columns</li>
<li><strong>Query Performance:</strong> Queries scanning specific columns are 10-100x faster than row-oriented formats</li>
<li><strong>Uses:</strong>
<ul>
<li>Data lakes (AWS S3, Azure ADLS) - primary format for analytics</li>
<li>Spark DataFrames and Pandas - native support</li>
<li>Snowflake, BigQuery, Redshift - optimized for Parquet</li>
<li>Machine Learning datasets - efficient feature access</li>
</ul>
</li>
<li><strong>Overhead:</strong> Largest metadata overhead, slower for row-by-row access but excellent for batch analytics</li>
</ul>
<p><strong>ORC (Optimized Row Columnar):</strong> Columnar format developed by Hortonworks for Hadoop ecosystem</p>
<ul>
<li><strong>Predicate Pushdown:</strong> Filtering applied during read from disk, not after. Scans only relevant stripes</li>
<li><strong>Type Awareness:</strong> Understanding of data types enables better compression and optimizations</li>
<li><strong>Uses:</strong>
<ul>
<li>Apache Hive (OLAP queries on Hadoop)</li>
<li>Data warehousing on Hadoop clusters</li>
<li>Spark SQL workloads in Hadoop ecosystems</li>
</ul>
</li>
<li><strong>Comparison:</strong> ORC typically smaller than Parquet (better compression) but less ecosystem support outside Hadoop</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>AVRO</th>
<th>Parquet</th>
<th>ORC</th>
</tr>
<tr>
<td>Storage Type</td>
<td>Row-oriented</td>
<td>Columnar</td>
<td>Columnar</td>
</tr>
<tr>
<td>Schema Evolution</td>
<td>Excellent (reader/writer schemas)</td>
<td>Good (backward compatible)</td>
<td>Good</td>
</tr>
<tr>
<td>Compression Ratio</td>
<td>Moderate</td>
<td>Very Good</td>
<td>Best</td>
</tr>
<tr>
<td>Query Performance</td>
<td>Good for full row reads</td>
<td>Excellent for column subset</td>
<td>Excellent for analytic queries</td>
</tr>
<tr>
<td>Ecosystem Support</td>
<td>Kafka, Pulsar, Streaming</td>
<td>Universal (Cloud DWs, Spark)</td>
<td>Hadoop/Hive focused</td>
</tr>
<tr>
<td>Best For</td>
<td>Event streaming, CDC</td>
<td>Data lakes, analytics</td>
<td>Hadoop, Hive queries</td>
</tr>
</table>
<p><strong>Recommendation for Modern Data Stack:</strong> Use Parquet for data lakes (S3/ADLS); Use Avro for streaming pipelines and CDC; ORC if using Hadoop ecosystem</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q19">
<div class="question-title">Difference between CTE and Temp table - Where to use</div>
<div class="answer">
<p><strong>CTE (Common Table Expression) - WITH Clause:</strong> Temporary named result set defined within a single query. Often called "WITH clause" or subquery alternative</p>
<pre>-- Multiple CTEs in one query
WITH 
-- CTE 1: Get top customers
TopCustomers AS (
    SELECT customer_id, SUM(order_amount) AS total_amount
    FROM orders 
    GROUP BY customer_id 
    ORDER BY total_amount DESC 
    LIMIT 100
),
-- CTE 2: Get their recent orders
RecentOrders AS (
    SELECT tc.customer_id, o.order_id, o.order_date, o.amount
    FROM TopCustomers tc
    JOIN orders o ON tc.customer_id = o.customer_id
    WHERE o.order_date &gt;= CURRENT_DATE - 30
)
-- Main query uses CTEs
SELECT customer_id, COUNT(*) as order_count, AVG(amount) as avg_amount
FROM RecentOrders
GROUP BY customer_id;</pre>
<p><strong>Temp Table (TEMPORARY Table):</strong> Session-scoped table that persists in database until session ends. Can be used across multiple queries in same session</p>
<pre>-- Create temp table that exists for this session
CREATE TEMPORARY TABLE top_customers AS
SELECT customer_id, SUM(order_amount) AS total_amount
FROM orders 
GROUP BY customer_id 
ORDER BY total_amount DESC 
LIMIT 100;

-- Use temp table in Query 1
SELECT * FROM top_customers WHERE total_amount &gt; 10000;

-- Use same temp table in Query 2
SELECT tc.customer_id, COUNT(*) 
FROM top_customers tc
JOIN orders o ON tc.customer_id = o.customer_id
GROUP BY tc.customer_id;

-- Query 3 still has access to temp table
INSERT INTO analytics_summary
SELECT * FROM top_customers;

-- Eventually session ends, temp table auto-drops</pre>
<table>
<tr>
<th>Feature</th>
<th>CTE (WITH)</th>
<th>Temporary Table</th>
</tr>
<tr>
<td>Scope</td>
<td>Single query only</td>
<td>Entire session</td>
</tr>
<tr>
<td>Persistence</td>
<td>Logical, not stored</td>
<td>Physical table in database</td>
</tr>
<tr>
<td>Reusability</td>
<td>Within same query</td>
<td>Across multiple queries</td>
</tr>
<tr>
<td>Materialization</td>
<td>Optimizer decides (often inlined)</td>
<td>Always materialized</td>
</tr>
<tr>
<td>Storage Cost</td>
<td>None</td>
<td>Counts toward table storage</td>
</tr>
<tr>
<td>Performance</td>
<td>Can be inlined (fast) or materialized</td>
<td>Predictable, always fast for reads</td>
</tr>
<tr>
<td>When to Use</td>
<td>Breaking down complex queries, single-use logic</td>
<td>Expensive operation reused many times in session</td>
</tr>
</table>
<p><strong>When to Use CTEs:</strong></p>
<ul>
<li>Breaking down complex queries into readable parts</li>
<li>Recursive queries (hierarchical data like org charts)</li>
<li>Logical organization without needing storage</li>
</ul>
<p><strong>When to Use Temp Tables:</strong></p>
<ul>
<li>Expensive calculation reused multiple times in session</li>
<li>Multi-step ETL processes in stored procedures</li>
<li>Needing to add indexes or optimize for specific access patterns</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q20">
<div class="question-title">Difference between Transient and Dynamic and Dynamic Transient tables</div>
<div class="answer">
<p><strong>TRANSIENT Table:</strong> Designed specifically for temporary data that doesn't require long-term protection</p>
<ul>
<li><strong>Time Travel:</strong> Default 0 days (configurable up to 1 day max) - cannot query historical versions</li>
<li><strong>Fail-safe:</strong> None (no 7-day recovery window)</li>
<li><strong>Storage Cost:</strong> Approximately 50% lower than permanent tables</li>
<li><strong>Creation:</strong> CREATE TRANSIENT TABLE my_table (...);</li>
<li><strong>Uses:</strong> Staging tables for daily loads, intermediate ETL results, temporary aggregations</li>
</ul>
<p><strong>Example Use Case:</strong></p>
<pre>-- Daily staging table for raw data
CREATE OR REPLACE TRANSIENT TABLE raw_daily_sales AS
COPY INTO FROM @s3_stage/daily_sales/
FILE_FORMAT = (TYPE = CSV);

-- Transform and move to permanent table
INSERT INTO fact_sales
SELECT * FROM raw_daily_sales
WHERE processing_complete = TRUE;

-- raw_daily_sales can be safely dropped - data is in fact_sales</pre>
<p><strong>Note:</strong> "Dynamic" and "Dynamic Transient" are not standard Snowflake table types. They may refer to:</p>
<ul>
<li><strong>Dynamic Data Loading:</strong> Frequently updated data with continuous CDC</li>
<li><strong>Project-specific Naming:</strong> Custom conventions for different use cases</li>
</ul>
<p>In production, most tables are either <strong>Permanent</strong> (for analytics) or <strong>Transient</strong> (for staging). See Question 22 for Ephemeral/Temporary tables comparison.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q21">
<div class="question-title">How you will handle schema changes in the downstream</div>
<div class="answer">
<p><strong>Strategies for Handling Schema Changes:</strong></p>
<ol>
<li><strong>Communication &amp; Collaboration:</strong> Establish data governance processes, cross-functional meetings, maintain documentation</li>
<li><strong>Flexible Data Types:</strong> Use VARIANT columns for semi-structured data resilience to schema changes</li>
<li><strong>Schema-on-Read:</strong> Apply schema at query time, not load time (data lakes with Parquet/ORC)</li>
<li><strong>Additive Schema Changes (Preferred):</strong> Add new columns rather than renaming/deleting/changing types</li>
<li><strong>Soft Deletes/Deprecation:</strong> Mark columns as deprecated before removal</li>
<li><strong>Versioning Tables/Views:</strong> Create views over base tables to maintain consistent interface
                        <pre>-- Original view
CREATE VIEW v_customer AS SELECT customer_id, email FROM raw_customer;

-- Source table changes: 'email' becomes 'primary_contact_email'
-- Update view to maintain compatibility
CREATE VIEW v_customer AS SELECT customer_id, primary_contact_email AS email FROM raw_customer;</pre>
</li>
<li><strong>Data Contracts/Schema Registries:</strong> Define formal contracts using Avro/Protobuf</li>
<li><strong>Impact Analysis &amp; Testing:</strong> Understand dependencies, test in dev/test before prod</li>
<li><strong>Graceful Degradation:</strong> Implement logic that handles missing columns gracefully</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q22">
<div class="question-title">Ephemeral vs Permanent tables</div>
<div class="answer">
<p><strong>Permanent Table (Default):</strong> Standard table type with full data durability</p>
<ul>
<li>Time Travel: 1-90 days</li>
<li>Fail-safe: 7 days</li>
<li>Persistence: Until explicitly dropped</li>
<li>Cost: Highest storage cost</li>
<li>Uses: Core data, historical records</li>
</ul>
<p><strong>Ephemeral (TEMPORARY Table):</strong> Session-scoped table</p>
<ul>
<li>Persistence: Auto-dropped at session end</li>
<li>Time Travel: Session-bound (1 day max)</li>
<li>Fail-safe: None</li>
<li>Cost: Lowest storage cost</li>
<li>Uses: Ad-hoc analysis, session-specific work</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Permanent</th>
<th>TEMPORARY (Ephemeral)</th>
</tr>
<tr>
<td>Persistence</td>
<td>Until dropped</td>
<td>Session-scoped</td>
</tr>
<tr>
<td>Time Travel</td>
<td>1-90 days</td>
<td>Session-bound</td>
</tr>
<tr>
<td>Fail-safe</td>
<td>7 days</td>
<td>None</td>
</tr>
<tr>
<td>Cost</td>
<td>Highest</td>
<td>Lowest</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q23">
<div class="question-title">How to implement CDC without ETL tools in Snowflake</div>
<div class="answer">
<p><strong>Strategy 1: Timestamp-Based CDC</strong></p>
<pre>CREATE TABLE staging_data (
    id INT, data VARCHAR, last_modified TIMESTAMP
);

MERGE INTO target_table AS t
USING (SELECT * FROM staging_data WHERE last_modified &gt; :last_load_time) AS s
ON t.id = s.id
WHEN MATCHED THEN UPDATE SET t.data = s.data
WHEN NOT MATCHED THEN INSERT VALUES (s.id, s.data);</pre>
<p><strong>Strategy 2: Snowflake Streams (Recommended):</strong></p>
<pre>CREATE STREAM my_stream ON TABLE source_table;

-- Query stream - tracks inserts, updates, deletes
SELECT * FROM my_stream;
-- METADATA$ACTION shows 'INSERT' or 'DELETE'
-- METADATA$ISUPDATE shows if row is part of UPDATE

-- Consume changes atomically
MERGE INTO target_table AS t
USING my_stream AS s
ON t.id = s.id
WHEN MATCHED AND s.METADATA$ACTION='DELETE' THEN DELETE
WHEN NOT MATCHED AND s.METADATA$ACTION='INSERT' THEN INSERT ...;</pre>
<p><strong>Benefits of Streams:</strong> Exactly-once processing, captures all DML, low performance impact</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q24">
<div class="question-title">What is DBT and uses - Execution plan</div>
<div class="answer">
<p>DBT (Data Build Tool) is an open-source framework that enables data analysts to transform data in their warehouse using SQL and software engineering best practices. It focuses on the T in ELT.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>SQL-centric transformations</li>
<li>Modularity and reusability</li>
<li>Version control with Git</li>
<li>Built-in data quality tests</li>
<li>Auto-generated documentation</li>
<li>Dependency graph (DAG)</li>
<li>Jinja templating for dynamic SQL</li>
</ul>
<p><strong>DBT Execution Plan (dbt run):</strong></p>
<ol>
<li><strong>Parsing:</strong> Read .sql files, parse Jinja templates, resolve ref() functions</li>
<li><strong>DAG Building:</strong> Create dependency graph of all models</li>
<li><strong>Dependency Resolution:</strong> Determine execution order</li>
<li><strong>Materialization:</strong> Decide how to build each model (view, table, incremental, ephemeral)</li>
<li><strong>SQL Generation &amp; Execution:</strong> Generate SQL, execute in warehouse in correct order</li>
<li><strong>Post-Run Actions:</strong> Run tests, generate documentation</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q25">
<div class="question-title">Why continuous data load used - Use cases</div>
<div class="answer">
<p>Continuous data loading moves data as soon as it becomes available, providing near real-time insights instead of batch windows.</p>
<p><strong>Why Use It:</strong></p>
<ul>
<li>Near real-time analytics and operational dashboards</li>
<li>Reduced data latency</li>
<li>Improved responsiveness for decision-making</li>
<li>Eliminates batch window constraints</li>
<li>Distributes compute usage evenly over time</li>
<li>Better data quality feedback loops</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li><strong>Operational Analytics:</strong> Real-time sales dashboards, metrics</li>
<li><strong>Fraud Detection:</strong> Analyze transactions in real-time</li>
<li><strong>IoT/Sensor Data:</strong> Monitor machine performance, predictive maintenance</li>
<li><strong>Clickstream Analytics:</strong> Track user behavior, personalize experiences</li>
<li><strong>Log Analysis:</strong> Security monitoring, error detection</li>
<li><strong>Supply Chain:</strong> Real-time tracking and optimization</li>
<li><strong>Customer 360:</strong> Build comprehensive customer views for personalization</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q26">
<div class="question-title">Explain end-to-end data pipeline - What are the logics you followed</div>
<div class="answer">
<p><strong>Typical Stages:</strong></p>
<ol>
<li><strong>Data Sources:</strong> Operational databases, SaaS apps, APIs, logs (identify what data, how to access, frequency)</li>
<li><strong>Ingestion/Extraction:</strong> Batch or streaming (full load vs incremental CDC)</li>
<li><strong>Landing/Staging:</strong> Raw data storage with schema-on-read approach</li>
<li><strong>Transformation:</strong> Clean, enrich, conform, aggregate (using DBT, SQL, or external engines)</li>
<li><strong>Serving/Presentation:</strong> Optimized views/tables for consumption</li>
<li><strong>Consumption:</strong> BI tools, dashboards, data science, applications</li>
</ol>
<p><strong>Cross-cutting Concerns:</strong></p>
<ul>
<li><strong>Orchestration:</strong> Schedule and manage pipeline stages (Airflow, Tasks, Step Functions)</li>
<li><strong>Monitoring &amp; Alerting:</strong> Track pipeline health, errors, data quality</li>
<li><strong>Data Lineage:</strong> Understand data flow, debug issues</li>
<li><strong>Cost Management:</strong> Monitor and optimize cloud resource usage</li>
<li><strong>Version Control:</strong> Git for all code (SQL, Python, DBT)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q27">
<div class="question-title">Why implement CDC instead of other approaches - Use case</div>
<div class="answer">
<p><strong>Why CDC instead of full loads:</strong></p>
<ul>
<li><strong>Efficiency:</strong> Only changed data transferred/processed, reduces bandwidth and compute</li>
<li><strong>Near Real-time:</strong> Data availability within minutes/seconds</li>
<li><strong>Reduced Impact:</strong> Minimal strain on source systems (especially log-based CDC)</li>
<li><strong>Captures Deletes:</strong> Unlike timestamp queries, robust CDC captures DELETE operations</li>
<li><strong>Simplified Logic:</strong> Streams abstract away complexity of determining changes</li>
</ul>
<p><strong>Real-world Use Case: E-commerce Inventory Management</strong></p>
<p>Problem without CDC: Nightly batch updates mean inventory is 12-24 hours old â†’ customers order out-of-stock items, poor warehouse operations</p>
<p>Solution with CDC: Stream changes from inventory DB into Snowflake using CDC â†’ near real-time dashboards â†’ website always knows actual stock levels â†’ automated reordering â†’ efficient logistics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q28">
<div class="question-title">Snowflake Tasks - Troubleshooting failed queries and performance improvement</div>
<div class="answer">
<p><strong>Snowflake Tasks:</strong> Execute SQL statements or call stored procedures on a recurring schedule or when conditions are met</p>
<p><strong>Key Features:</strong> Scheduling (cron-like), DAG dependencies, conditional execution, error handling</p>
<p><strong>Troubleshooting Failed/Slow Queries:</strong></p>
<ol>
<li><strong>Identify Problem:</strong> Query History in UI or ACCOUNT_USAGE.QUERY_HISTORY</li>
<li><strong>Analyze Query Profile:</strong> Check execution phases, identify bottlenecks (scanning, joining, spilling)</li>
<li><strong>Common Issues &amp; Solutions:</strong>
<ul>
<li>Spilling to disk â†’ Increase warehouse size</li>
<li>Poor pruning â†’ Add clustering keys or improve WHERE clauses</li>
<li>Inefficient joins â†’ Rewrite query, change join order</li>
<li>Expensive operations â†’ Avoid SELECT *, DISTINCT, ORDER BY unless necessary</li>
</ul>
</li>
<li><strong>Task-Specific:</strong> Check TASK_HISTORY for STATE and ERROR_MESSAGE, verify warehouse is available</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q29">
<div class="question-title">What is Zero Copy clone - Uses</div>
<div class="answer">
<p>Zero-Copy Cloning creates an instant copy of a database, schema, or table without physically duplicating data. Snowflake creates metadata pointers to the same micro-partitions. Changes use copy-on-write.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li>Instantaneous - created almost instantly</li>
<li>No storage cost at creation - pay only for incremental changes</li>
<li>No data movement</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>CREATE DATABASE prod_db_clone CLONE prod_db;
CREATE SCHEMA temp_schema CLONE prod_schema;
CREATE TABLE test_table CLONE prod_table;</pre>
<p><strong>Uses:</strong></p>
<ul>
<li><strong>Development/Testing:</strong> Clone production DB for developers to test without affecting prod</li>
<li><strong>Disaster Recovery:</strong> Create point-in-time recovery environment quickly</li>
<li><strong>Analytical Workloads:</strong> Isolate complex queries from main system</li>
<li><strong>Version Control:</strong> Snapshot data at milestones for rollback capability</li>
<li><strong>Historical Analysis:</strong> Create monthly snapshots with minimal cost</li>
<li><strong>What-If Scenarios:</strong> Run simulations without impacting production</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q30">
<div class="question-title">Convert timestamp from one timezone to another</div>
<div class="answer">
<p>Snowflake has three TIMESTAMP types: TIMESTAMP_LTZ (Local Time Zone), TIMESTAMP_NTZ (No Time Zone), TIMESTAMP_TZ (Time Zone).</p>
<p><strong>Using CONVERT_TIMEZONE (Recommended):</strong></p>
<pre>SELECT CONVERT_TIMEZONE('UTC', 'America/New_York', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 06:00:00.000'

SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'Asia/Kolkata', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 22:30:00.000'

-- With a column
SELECT event_id, event_timestamp,
       CONVERT_TIMEZONE('UTC', 'America/New_York', event_timestamp) AS event_timestamp_est
FROM events;</pre>
<p><strong>Using AT TIME ZONE:</strong></p>
<pre>SELECT '2025-07-04 10:00:00'::TIMESTAMP_NTZ AT TIME ZONE 'America/New_York';

-- Set session timezone first
ALTER SESSION SET TIMEZONE = 'UTC';
SELECT event_timestamp AT TIME ZONE 'Asia/Kolkata'
FROM events;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q31">
<div class="question-title">Clustering Keys in Snowflake</div>
<div class="answer">
<p>Clustering Keys optimize the physical organization of data within micro-partitions to improve query performance.</p>
<p><strong>How They Work:</strong> Snowflake's Automatic Clustering service re-clusters data if clustering becomes stale, physically reorganizing micro-partitions to group similar values together.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Improved query performance for filtered/joined queries</li>
<li>Reduced data scanning â†’ fewer credits consumed</li>
<li>Automatic and transparent</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>-- During creation
CREATE TABLE sales (
    event_date DATE,
    category VARCHAR,
    value DECIMAL
) CLUSTER BY (event_date, category);

-- After creation
ALTER TABLE sales CLUSTER BY (event_date, category);

-- Remove clustering
ALTER TABLE sales DROP CLUSTERING KEY;</pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Very large tables (hundreds of GB to TB)</li>
<li>Frequent filtering/joining on specific columns</li>
<li>High cardinality columns with skewed data</li>
<li>Data ingestion patterns cause poor clustering over time</li>
</ul>
<p><strong>Monitor with:</strong> SYSTEM$CLUSTERING_INFORMATION() function</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q32">
<div class="question-title">How to monitor Snowflake performance</div>
<div class="answer">
<p><strong>Snowflake Web Interface:</strong></p>
<ul>
<li>Query History - view all queries, duration, credits, status</li>
<li>Query Profile - visualize execution plan, identify bottlenecks</li>
<li>Warehouses - monitor status, usage, credit consumption</li>
</ul>
<p><strong>Account Usage Views (SNOWFLAKE.ACCOUNT_USAGE):</strong></p>
<ul>
<li>QUERY_HISTORY - comprehensive query data</li>
<li>WAREHOUSE_METERING_HISTORY - daily credit consumption per warehouse</li>
<li>AUTOMATIC_CLUSTERING_HISTORY - credits consumed by clustering</li>
<li>PIPE_USAGE_HISTORY - credits consumed by Snowpipe</li>
<li>COPY_HISTORY - COPY INTO command status</li>
<li>TASK_HISTORY - task execution history</li>
</ul>
<p><strong>External Tools:</strong></p>
<ul>
<li>Cloud monitoring (CloudWatch, Azure Monitor)</li>
<li>Third-party APM tools (Datadog, Splunk)</li>
<li>BI tools for custom dashboards</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q33">
<div class="question-title">Attacama and Collibra - Uses and Differences</div>
<div class="answer">
<p><strong>Collibra:</strong> Business-centric data governance platform</p>
<ul>
<li>Strong emphasis on stewardship and collaboration</li>
<li>Excellent Business Glossary, workflow automation, policy management</li>
<li>Best for: Building data governance programs, empowering business users, compliance</li>
</ul>
<p><strong>Ataccama ONE:</strong> Integrated data management platform</p>
<ul>
<li>Combines data quality, MDM, catalog, and governance</li>
<li>Strong data quality engine, Master Data Management, AI/ML automation</li>
<li>Best for: Data quality frameworks, MDM solutions, operational data management</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Collibra</th>
<th>Ataccama ONE</th>
</tr>
<tr>
<td>Core Focus</td>
<td>Business Governance, Stewardship</td>
<td>Data Quality, MDM, Governance</td>
</tr>
<tr>
<td>Strength</td>
<td>Business Glossary, Workflows</td>
<td>Data Quality, MDM, AI/ML</td>
</tr>
<tr>
<td>Target Users</td>
<td>Data Stewards, Governance Teams</td>
<td>Data Engineers, Quality Teams</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q34">
<div class="question-title">How to optimize long running queries - How to reduce time</div>
<div class="answer">
<p><strong>Step 1: Analyze Query Profile (Mandatory)</strong></p>
<ul>
<li>Identify bottleneck operators (scanning, joining, aggregating, sorting)</li>
<li>Check spilling to disk - indicates memory constraints</li>
<li>Evaluate pruning efficiency</li>
</ul>
<p><strong>Step 2: Right-Size Virtual Warehouse</strong></p>
<pre>-- Increase warehouse size if spilling or high queue time
ALTER WAREHOUSE MY_WH SET WAREHOUSE_SIZE = 'LARGE';</pre>
<p><strong>Step 3: Optimize SQL Logic</strong></p>
<ul>
<li>SELECT only necessary columns (avoid SELECT *)</li>
<li>Filter early and effectively with WHERE clauses</li>
<li>Optimize joins - proper join order, use INNER JOIN when possible</li>
<li>Avoid correlated subqueries - use JOINs instead</li>
<li>Eliminate unnecessary DISTINCT or ORDER BY</li>
</ul>
<p><strong>Step 4: Leverage Data Organization</strong></p>
<ul>
<li>Define clustering keys for very large tables</li>
<li>Create materialized views for complex, frequently-run queries</li>
</ul>
<p><strong>Step 5: Leverage Caching</strong></p>
<ul>
<li>Encourage reuse of queries for result cache</li>
<li>Keep warehouses running for warehouse cache benefits</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q35">
<div class="question-title">What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</div>
<div class="answer">
<p>Snowpipe is Snowflake's continuous data ingestion service that loads data as soon as files appear in cloud storage.</p>
<p><strong>AWS Integration Flow:</strong> S3 Event â†’ SNS â†’ SQS â†’ Snowpipe â†’ COPY INTO</p>
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>AWS Setup:</strong> Create S3 bucket, SNS topic, SQS queue, IAM role with permissions</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_pipe_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="3">
<li><strong>Create Stage:</strong></li>
</ol>
<pre>CREATE STAGE raw_stage
  STORAGE_INTEGRATION = s3_pipe_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<ol start="4">
<li><strong>Create Target Table:</strong></li>
</ol>
<pre>CREATE TABLE raw_data (col1 INT, col2 VARCHAR, load_ts TIMESTAMP);</pre>
<ol start="5">
<li><strong>Create Snowpipe:</strong></li>
</ol>
<pre>CREATE PIPE raw_data_pipe
  AUTO_INGEST = TRUE
  AWS_SQS_QUEUE_ARN = 'arn:aws:sqs:region:account:queue'
  AS
  COPY INTO raw_data FROM @raw_stage;</pre>
<ol start="6">
<li><strong>Post-Load Processing (Optional):</strong></li>
</ol>
<pre>-- Create stream on target table
CREATE STREAM raw_data_stream ON TABLE raw_data;

-- Create stored proc for transformations
CREATE PROCEDURE process_new_data()
  AS $$ 
    MERGE INTO fact_table AS t
    USING raw_data_stream AS s ON t.id = s.id
    WHEN NOT MATCHED THEN INSERT ...;
  $$;

-- Create task to run after data arrival
CREATE TASK transform_task
  WHEN SYSTEM$STREAM_HAS_DATA('raw_data_stream')
  AS CALL process_new_data();</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q36">
<div class="question-title">Max Salary for each department and 4th max salary for each department</div>
<div class="answer">
<p><strong>Max Salary for Each Department:</strong></p>
<pre>SELECT department, MAX(salary) AS max_salary_in_department
FROM employees
GROUP BY department
ORDER BY department;</pre>
<p><strong>4th Max Salary for Each Department:</strong></p>
<pre>SELECT employee_id, department, salary
FROM employees
QUALIFY DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 4
ORDER BY department, salary DESC;</pre>
<p><strong>Using CTE (More Portable SQL):</strong></p>
<pre>WITH RankedSalaries AS (
    SELECT employee_id, department, salary,
           DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank
    FROM employees
)
SELECT employee_id, department, salary
FROM RankedSalaries
WHERE salary_rank = 4
ORDER BY department;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q37">
<div class="question-title">Swap the gender value for a table - Male to Female, Female to Male</div>
<div class="answer">
<pre>UPDATE users
SET gender = CASE
    WHEN gender = 'Male' THEN 'Female'
    WHEN gender = 'Female' THEN 'Male'
    ELSE gender -- Keep other values (Prefer not to say, NULL, etc.)
END
WHERE gender IN ('Male', 'Female');</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li>CASE statement evaluates current value of gender column</li>
<li>If 'Male', changes to 'Female' and vice versa</li>
<li>ELSE gender preserves other values unchanged</li>
<li>WHERE clause optimizes by processing only relevant rows</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q38">
<div class="question-title">DBT Project Architecture</div>
<div class="answer">
<p><strong>Typical DBT Project Structure:</strong></p>
<pre>my_dbt_project/
â”œâ”€â”€ dbt_project.yml         # Core configuration
â”œâ”€â”€ models/                 # Data transformation models
â”‚   â”œâ”€â”€ staging/           # Raw, light transformations
â”‚   â”œâ”€â”€ intermediate/      # Complex joins, business logic
â”‚   â””â”€â”€ marts/             # Final, user-facing models
â”œâ”€â”€ analysis/              # Ad-hoc SQL queries
â”œâ”€â”€ macros/                # Reusable SQL snippets
â”œâ”€â”€ seeds/                 # Static CSV lookup tables
â”œâ”€â”€ snapshots/             # SCD Type 2 configurations
â”œâ”€â”€ tests/                 # Custom data quality tests
â”œâ”€â”€ logs/                  # DBT execution logs
â””â”€â”€ target/                # Compiled SQL, manifests</pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>dbt_project.yml:</strong> Project configuration, default materializations</li>
<li><strong>Models:</strong> SQL SELECT statements - heart of DBT project</li>
<li><strong>Tests:</strong> Data quality validations (not_null, unique, accepted_values, custom)</li>
<li><strong>Macros:</strong> Reusable Jinja templates for dynamic SQL</li>
<li><strong>Seeds:</strong> Static CSV files loaded as tables</li>
<li><strong>Snapshots:</strong> Capture historical changes (SCD Type 2)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q39">
<div class="question-title">Difference between UPSERT and MERGE</div>
<div class="answer">
<p><strong>UPSERT:</strong> A conceptual operation - UPDATE if exists, INSERT if not</p>
<p><strong>MERGE:</strong> Standard SQL statement (SQL:2003) that implements UPSERT</p>
<p><strong>MERGE in Snowflake:</strong></p>
<pre>MERGE INTO target_table AS T
USING source_data AS S
ON T.id = S.id
WHEN MATCHED AND T.last_updated &lt; S.last_updated THEN
    UPDATE SET T.name = S.name, T.email = S.email
WHEN NOT MATCHED THEN
    INSERT (id, name, email) VALUES (S.id, S.name, S.email);</pre>
<p><strong>Example:</strong> Update customer 1's email, insert new customer 3</p>
<pre>-- Before MERGE
customers: (1, 'Alice', 'alice@old.com'), (2, 'Bob', 'bob@example.com')
staging: (1, 'Alice', 'alice@new.com'), (3, 'Charlie', 'charlie@example.com')

-- After MERGE
customers: (1, 'Alice', 'alice@new.com'), (2, 'Bob', 'bob@example.com'), (3, 'Charlie', 'charlie@example.com')</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q40">
<div class="question-title">Data Masking and Masking Policy</div>
<div class="answer">
<p><strong>Data Masking:</strong> Hiding sensitive data with fictional data while maintaining usefulness for development/testing/analytics.</p>
<p><strong>Snowflake Masking Policy:</strong> Dynamic masking at query time based on user role. Underlying data is never modified.</p>
<p><strong>Example:</strong></p>
<pre>-- Create masking policy
CREATE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR -&gt;
    CASE
        WHEN CURRENT_ROLE() IN ('ANALYST_ROLE') THEN '****'
        WHEN CURRENT_ROLE() = 'DATA_STEWARD_ROLE' THEN val
        ELSE 'No access'
    END;

-- Apply to column
ALTER TABLE users ALTER COLUMN email SET MASKING POLICY email_mask;</pre>
<p><strong>Benefits:</strong> Dynamic, centralized, granular control, secure, no data duplication</p>
<p><strong>Common Techniques:</strong> Substitution, shuffling, redaction, tokenization, hashing, encryption</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q41">
<div class="question-title">Difference between Hashing and Encryption</div>
<div class="answer">
<p><strong>Hashing:</strong> One-way function creating fixed-size unique string</p>
<ul>
<li>Purpose: Integrity verification, password storage</li>
<li>One-way: Cannot reverse to get original</li>
<li>Deterministic: Same input always produces same hash</li>
<li>Examples: MD5 (32 chars), SHA256 (64 chars)</li>
</ul>
<p><strong>Encryption:</strong> Two-way function requiring key to encrypt/decrypt</p>
<ul>
<li>Purpose: Confidentiality, protecting sensitive data</li>
<li>Reversible: Decrypt with correct key to get original</li>
<li>Key-dependent: Requires encryption/decryption key</li>
<li>Examples: AES, RSA</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Hashing</th>
<th>Encryption</th>
</tr>
<tr>
<td>Reversibility</td>
<td>One-way (irreversible)</td>
<td>Two-way (reversible)</td>
</tr>
<tr>
<td>Purpose</td>
<td>Integrity, passwords</td>
<td>Confidentiality</td>
</tr>
<tr>
<td>Key Usage</td>
<td>No key needed</td>
<td>Requires key</td>
</tr>
<tr>
<td>Output Size</td>
<td>Fixed</td>
<td>Variable</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q42">
<div class="question-title">Snowflake storage integration with AWS S3</div>
<div class="answer">
<p><strong>Purpose:</strong> Secure, credential-less access to S3 from Snowflake</p>
<p><strong>Step-by-Step Setup:</strong></p>
<ol>
<li><strong>AWS IAM Policy:</strong> Grant Snowflake permissions (s3:GetObject, s3:PutObject, s3:ListBucket)</li>
<li><strong>AWS IAM Role:</strong> Create role, attach policy</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="4">
<li><strong>Get Snowflake's External ID &amp; IAM User:</strong>
<pre>DESCRIBE INTEGRATION s3_integration;</pre>
</li>
<li><strong>Update AWS IAM Role Trust Relationship:</strong> Add Snowflake's IAM user ARN and external ID</li>
<li><strong>Create External Stage:</strong></li>
</ol>
<pre>CREATE STAGE my_s3_stage
  STORAGE_INTEGRATION = s3_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<p><strong>Benefits:</strong> Enhanced security, centralized control, no exposed AWS keys</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q43">
<div class="question-title">What is DBT and what problem does it solve in modern data stack</div>
<div class="answer">
<p>DBT (data build tool) is a command-line framework for transforming data in your warehouse with modular SQL and software engineering best practices.</p>
<p><strong>Problems it Solves:</strong></p>
<ul>
<li><strong>Code Proliferation:</strong> Centralizes transformation logic instead of scattered ETL scripts/BI tools</li>
<li><strong>Lack of Best Practices:</strong> Brings version control, modularity, testing, documentation to data work</li>
<li><strong>Dependency Management:</strong> Automatically determines correct model execution order</li>
<li><strong>Testing &amp; Quality:</strong> Provides framework for data quality tests and auto-documentation</li>
<li><strong>Transparency:</strong> Makes entire transformation process transparent and auditable</li>
</ul>
<p><strong>Core Philosophy:</strong> Leverage the power of the data warehouse itself (pushing computations) rather than using separate processing engines</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q44">
<div class="question-title">Core components of a DBT project: Models, Tests, Seeds</div>
<div class="answer">
<p><strong>Models:</strong> SQL SELECT statements organized into transformation logic</p>
<ul>
<li>Each .sql file = one model</li>
<li>Output: view or table (depends on materialization)</li>
<li>Example: Model calculating monthly sales from raw transactions</li>
</ul>
<p><strong>Tests:</strong> Data quality checks ensuring data integrity</p>
<ul>
<li><strong>Singular Tests:</strong> Custom SQL returning failing rows if condition not met</li>
<li><strong>Generic Tests:</strong> Pre-defined tests applied via YAML (not_null, unique, accepted_values)</li>
<li>Purpose: Prevent downstream errors, ensure data reliability</li>
</ul>
<p><strong>Seeds:</strong> Static CSV files loaded as reference/lookup tables</p>
<ul>
<li>Small, infrequently-changing data (country codes, mapping tables)</li>
<li>Loaded via dbt seed command</li>
</ul>
<p><strong>Macros (Bonus):</strong> Reusable Jinja+SQL code snippets (like functions)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q45">
<div class="question-title">Materializations in DBT - Four default types</div>
<div class="answer">
<p><strong>View (Default):</strong></p>
<ul>
<li>Mechanism: Creates SQL view</li>
<li>Pros: Always latest data, fast compile</li>
<li>Cons: Slow queries (logic runs every time)</li>
<li>Best for: Simple transformations, rarely-queried models</li>
</ul>
<p><strong>Table:</strong></p>
<ul>
<li>Mechanism: Creates persistent table with CREATE TABLE AS</li>
<li>Pros: Fast queries (pre-calculated)</li>
<li>Cons: Slow to build, consumes storage</li>
<li>Best for: Complex/frequently-queried models, large datasets</li>
</ul>
<p><strong>Incremental:</strong></p>
<ul>
<li>Mechanism: INSERT/MERGE only new/changed records</li>
<li>Pros: Very fast after initial build</li>
<li>Cons: Complex design, prone to errors</li>
<li>Best for: Large fact tables with incremental changes</li>
</ul>
<p><strong>Ephemeral:</strong></p>
<ul>
<li>Mechanism: No physical object, compiles to CTE</li>
<li>Pros: Excellent modularity</li>
<li>Cons: Not queryable directly</li>
<li>Best for: Simple intermediate cleaning steps</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q46">
<div class="question-title">Managing dependencies and referencing models in DBT</div>
<div class="answer">
<p><strong>The ref() Function:</strong> Reference models without hardcoding schema names</p>
<pre>-- Instead of FROM SCHEMA.TABLE
SELECT * FROM {{ ref('stg_orders') }}
WHERE is_valid = TRUE</pre>
<p><strong>How it Works:</strong></p>
<ul>
<li>{{ ref('stg_orders') }} replaced with correct schema/table name during compilation</li>
<li>DBT automatically knows stg_orders must be built before this model</li>
<li>Creates dependency edge in DAG</li>
</ul>
<p><strong>Automatic DAG Generation:</strong></p>
<ul>
<li>DBT scans all models and their ref() calls</li>
<li>Builds DAG showing ALL dependencies</li>
<li>dbt run executes models in correct order, ensuring source tables exist before use</li>
</ul>
<p><strong>Benefits:</strong> No manual ordering, automatic parallelization, clear dependencies</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q47">
<div class="question-title">What is Jinja in DBT - Simple example</div>
<div class="answer">
<p>Jinja is a templating language that adds programming logic to SQL. Processed before SQL reaches the warehouse.</p>
<p><strong>Capabilities:</strong> if/else statements, variables, loops, macro reuse, dynamic SQL</p>
<p><strong>Simple Example:</strong></p>
<pre>{% set limit_date = '2023-01-01' %}

SELECT * FROM {{ ref('raw_data') }}
WHERE created_at &gt;= '{{ limit_date }}'

{% if target.name == 'prod' %}
  AND is_active = TRUE
{% endif %}</pre>
<p><strong>Compiled SQL (Development):</strong></p>
<pre>SELECT * FROM my_dev_schema.raw_data
WHERE created_at &gt;= '2023-01-01'</pre>
<p><strong>Compiled SQL (Production):</strong></p>
<pre>SELECT * FROM my_prod_schema.raw_data
WHERE created_at &gt;= '2023-01-01'
  AND is_active = TRUE</pre>
<p><strong>Benefits:</strong> DRY (Don't Repeat Yourself), environment-specific logic, dynamic SQL generation</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q48">
<div class="question-title">Data Governance best practices</div>
<div class="answer">
<p><strong>Key Best Practices:</strong></p>
<ol>
<li><strong>Define Clear Ownership:</strong> Assign data owners and stewards for each domain/dataset</li>
<li><strong>Establish Data Dictionary:</strong> Document all data elements, definitions, relationships</li>
<li><strong>Implement Metadata Management:</strong> Track technical metadata (schema, lineage) and business metadata</li>
<li><strong>Quality Standards:</strong> Implement data quality rules, monitoring, and remediation processes</li>
<li><strong>Access Control:</strong> RBAC, row-level security, column-level masking</li>
<li><strong>Data Lineage:</strong> Understand data flow from source to consumption</li>
<li><strong>Compliance &amp; Auditing:</strong> Track data usage, audit logs, regulatory compliance (GDPR, HIPAA)</li>
<li><strong>Version Control:</strong> Version all code (SQL, DBT, Python) in Git</li>
<li><strong>Documentation:</strong> Auto-generate and maintain docs (DBT, Collibra, Ataccama)</li>
<li><strong>Collaboration:</strong> Foster data culture, communication between teams</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q49">
<div class="question-title">Data quality testing strategies</div>
<div class="answer">
<p><strong>Common Testing Approaches:</strong></p>
<ol>
<li><strong>Schema Tests:</strong> Validate data types, constraints
                        <ul>
<li><strong>not_null:</strong> Column should not have NULL values</li>
<li><strong>unique:</strong> Column values must be distinct</li>
<li><strong>relationships:</strong> Foreign key relationships</li>
<li><strong>accepted_values:</strong> Column values from allowed set</li>
</ul>
</li>
<li><strong>Statistical Tests:</strong> Validate data distributions, outliers
                        <ul>
<li>Range checks</li>
<li>Distribution analysis</li>
<li>Outlier detection</li>
</ul>
</li>
<li><strong>Business Logic Tests:</strong> Validate business rules
                        <ul>
<li>Cross-table consistency</li>
<li>Aggregation accuracy</li>
<li>Recency checks</li>
</ul>
</li>
<li><strong>Freshness Tests:</strong> Ensure data is current
                        <ul>
<li>Last update timestamp</li>
<li>Row count expectations</li>
</ul>
</li>
</ol>
<p><strong>Tools:</strong> DBT tests, Great Expectations, Ataccama, custom SQL checks</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q50">
<div class="question-title">Building a modern data stack - Technology selection</div>
<div class="answer">
<p><strong>Typical Modern Data Stack Layers:</strong></p>
<ul>
<li><strong>Source Systems:</strong> Operational databases, SaaS applications, APIs</li>
<li><strong>Ingestion:</strong> Fivetran, Stitch, Airbyte, AWS DMS, Qlik Replicate</li>
<li><strong>Cloud Data Warehouse:</strong> Snowflake, BigQuery, Redshift</li>
<li><strong>Transformation:</strong> DBT, Spark, Python</li>
<li><strong>Data Governance:</strong> Collibra, Ataccama, DataHub</li>
<li><strong>Analytics/BI:</strong> Tableau, Power BI, Looker</li>
<li><strong>Orchestration:</strong> Airflow, Prefect, Dagster</li>
<li><strong>Reverse ETL:</strong> Hightouch, Census (sync data back to source systems)</li>
</ul>
<p><strong>Selection Criteria:</strong></p>
<ul>
<li>Scalability and performance</li>
<li>Cost efficiency</li>
<li>Ease of use / learning curve</li>
<li>Integration with existing tools</li>
<li>Community and support</li>
<li>Security and compliance requirements</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q51">
<div class="question-title">Future trends in data engineering</div>
<div class="answer">
<p><strong>Emerging Trends:</strong></p>
<ul>
<li><strong>DataOps:</strong> Applying DevOps principles to data pipelines (CI/CD, monitoring, automation)</li>
<li><strong>Real-time Analytics:</strong> Move beyond batch to streaming/real-time data pipelines</li>
<li><strong>AI/ML Integration:</strong> ML models integrated into data pipelines, automated feature engineering</li>
<li><strong>Data Mesh:</strong> Decentralized data ownership model, domain-oriented data architecture</li>
<li><strong>Lakehouse Architecture:</strong> Combining data lake flexibility with warehouse performance (Delta Lake, Iceberg, Hudi)</li>
<li><strong>Cloud-Native:</strong> Serverless data platforms, compute separation from storage</li>
<li><strong>Data Quality as First-Class:</strong> Increased focus on data quality, governance, and observability</li>
<li><strong>Self-Service Analytics:</strong> Tools enabling business users to perform analysis without technical expertise</li>
<li><strong>Reverse ETL:</strong> Syncing aggregated/enriched data back to operational systems</li>
<li><strong>Privacy-Preserving Technologies:</strong> Differential privacy, federated learning for sensitive data</li>
</ul>
<p><strong>Skills for Future:</strong> Cloud platforms (AWS/Azure/GCP), DBT, SQL, Python, Spark, containers, Kubernetes, ML basics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
</div>
<footer>Â© 2026 Â· Vemula Suryateja Â· vsurya@duck.com</footer></body>
</html>
<!DOCTYPE html>

<html lang="en">
<div class="question" id="q58">
<div class="question-title">dbt Materializations â€” Syntax &amp; When to Use</div>
<div class="answer">
<p><strong>Materialization defines how dbt builds a model in the warehouse.</strong></p>
<p><strong>1. view â€” Syntax</strong></p>
<pre>{{ config(materialized='view') }}

select *
from {{ ref('stg_orders') }}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Creates a Snowflake VIEW (no data stored)</li>
<li>Always reflects latest source data</li>
</ul>
<p><strong>When to use</strong></p>
<ul>
<li>Staging models, light transformations, fast iteration</li>
</ul>
<p><strong>2. table â€” Syntax</strong></p>
<pre>{{ config(materialized='table') }}

select *
from {{ ref('int_sales') }}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Creates a physical table rebuilt completely on every run</li>
</ul>
<p><strong>3. incremental â€” Syntax (MOST IMPORTANT)</strong></p>
<pre>{{ config(
        materialized='incremental',
        unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}

{% if is_incremental() %}
    where updated_at &gt; (select max(updated_at) from {{ this }})
{% endif %}</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>Loads only new or changed data â€” saves cost and time</li>
</ul>
<p><strong>4. ephemeral â€” Syntax</strong></p>
<pre>{{ config(materialized='ephemeral') }}

select
    order_id,
    sum(amount) as total_amount
from {{ ref('stg_payments') }}
group by order_id</pre>
<p><strong>Purpose</strong></p>
<ul>
<li>No table/view â€” SQL is inlined into downstream models</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q59">
<div class="question-title">dbt Snapshots â€” Syntax &amp; Purpose</div>
<div class="answer">
<p><strong>Snapshots are used for SCD Type 2.</strong></p>
<p><strong>Timestamp strategy</strong></p>
<pre>{% snapshot dim_customer_snapshot %}
{{
    config(
        target_schema='snapshots',
        unique_key='customer_id',
        strategy='timestamp',
        updated_at='updated_at',
        invalidate_hard_deletes=true
    )
}}

select *
from {{ ref('stg_customers') }}

{% endsnapshot %}</pre>
<p><strong>What dbt adds</strong></p>
<ul>
<li><code>dbt_valid_from</code>, <code>dbt_valid_to</code> and history tracking</li>
</ul>
<p><strong>Check strategy (track specific cols)</strong></p>
<pre>{% snapshot dim_product_snapshot %}
{{
    config(
        target_schema='snapshots',
        unique_key='product_id',
        strategy='check',
        check_cols=['price', 'status']
    )
}}

select *
from {{ ref('stg_products') }}

{% endsnapshot %}</pre>
<p><strong>Use when</strong></p>
<ul>
<li>Timestamp: source has reliable <code>updated_at</code></li>
<li>Check: no reliable timestamp â€” track changes on specific columns</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q60">
<div class="question-title">dbt YAML Files â€” Syntax, Indentation &amp; Purpose</div>
<div class="answer">
<p><strong>YAML defines metadata, not SQL logic.</strong></p>
<p><strong>schema.yml (example)</strong></p>
<pre>version: 2
                                <pre>version: 2

models:
    - name: stg_orders
        description: "Cleaned orders data"
        columns:
            - name: order_id
                description: "Primary key"
                tests:
                    - not_null
                    - unique</pre>

                                <p><strong>Indentation rules</strong></p>
                                <p>YAML breaks if indentation is wrong â€” use two spaces only.</p>

                                <p><strong>Source YAML (example)</strong></p>
                                <pre>version: 2

sources:
    - name: sales
        database: raw
        schema: public
        tables:
            - name: orders
                description: "Raw orders table"</pre>

                                <p><strong>Snapshot &amp; Seeds YAML</strong></p>
                                <pre>version: 2

snapshots:
    - name: dim_customer_snapshot
        description: "Customer SCD Type 2 history"</pre>

                                <pre>version: 2

seeds:
    - name: country_codes
        description: "ISO country codes"
        columns:
            - name: country_code
                tests:
                    - not_null</pre>
                        <a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</pre></div>
</div>
<div class="question" id="q61">
<div class="question-title">Tests in YAML â€” Built-in &amp; Relationships</div>
<div class="answer">
<p><strong>Built-in tests</strong></p>
<pre>tests:
    - not_null
    - unique
    - accepted_values:
            values: ['A', 'I']</pre>
<p><strong>Relationships test</strong></p>
<pre>tests:
    - relationships:
            to: ref('dim_customer')
            field: customer_id</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q62">
<div class="question-title">Materialization Configuration via dbt_project.yml</div>
<div class="answer">
<p><strong>Example</strong></p>
<pre>models:
    my_project:
        staging:
            +materialized: view
        marts:
            +materialized: incremental</pre>
<p><strong>Why this matters</strong></p>
<ul>
<li>Enforces standards and avoids per-model config</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q63">
<div class="question-title">Quick Interview Cheat Table &amp; Power Lines</div>
<div class="answer">
<p><strong>Cheat table</strong></p>
<table>
<tr><th>Feature</th><th>Used For</th></tr>
<tr><td>view</td><td>Staging</td></tr>
<tr><td>table</td><td>Small dimensions</td></tr>
<tr><td>incremental</td><td>Large facts</td></tr>
<tr><td>ephemeral</td><td>Reusable logic</td></tr>
<tr><td>snapshot</td><td>SCD Type 2</td></tr>
<tr><td>schema.yml</td><td>Tests &amp; docs</td></tr>
<tr><td>sources.yml</td><td>Source metadata</td></tr>
<tr><td>seeds</td><td>Static reference data</td></tr>
</table>
<p><strong>Interview Power Lines</strong></p>
<ul>
<li>â€œdbt handles transformations, Airflow handles orchestration.â€</li>
<li>â€œIncremental models are critical for cost control in Snowflake.â€</li>
<li>â€œSnapshots are the cleanest way to implement SCD Type 2.â€</li>
<li>â€œYAML defines what should be true, SQL defines how data is built.â€</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>


</html>
            line-height: 1.6;
            color: #333;
            background-color: #f9f9f9;
        }
        
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        header {
            border-bottom: 3px solid #007acc;
            padding-bottom: 20px;
            margin-bottom: 40px;
        }
        
        h1 {
            color: #007acc;
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        
        .subtitle {
            color: #666;
            font-size: 1.1em;
        }
        
        .question {
            margin-bottom: 40px;
            border-left: 4px solid #007acc;
            padding-left: 20px;
        }
        
        .question-number {
            font-size: 0.9em;
            color: #007acc;
            font-weight: bold;
        }
        
        .question-title {
            font-size: 1.4em;
            color: #0066cc;
            margin: 10px 0;
            font-weight: 600;
        }
        
        .answer {
            margin-top: 15px;
            color: #333;
            line-height: 1.8;
        }
        
        .answer p {
            margin-bottom: 12px;
        }
        
        pre {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.4;
        }
        
        code {
            background-color: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        strong {
            color: #0066cc;
            font-weight: 600;
        }
        
        ul, ol {
            margin: 15px 0 15px 30px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .highlight {
            background-color: #e7f3ff;
            padding: 15px;
            border-left: 3px solid #0066cc;
            margin: 15px 0;
            border-radius: 3px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: #f9f9f9;
        }
        
        table th {
            background-color: #007acc;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        
        table td {
            padding: 12px;
            border-bottom: 1px solid #ddd;
        }
        
        table tr:hover {
            background-color: #f0f0f0;
        }
        
        @media print {
            body {
                background-color: white;
            }
            .container {
                max-width: 100%;
                box-shadow: none;
                padding: 20px;
            }
            h1 {
                font-size: 1.8em;
                page-break-after: avoid;
            }
            .question {
                page-break-inside: avoid;
            }
            pre {
                font-size: 8pt;
                padding: 10px;
            }
            code {
                font-size: 8pt;
            }
        }
    
html { scroll-behavior: smooth; }
.question-title { color: #8e44ad !important; font-weight: 700; }

/* ---------------- Responsive Enhancements ---------------- */
.container {
  max-width: 1100px;
  padding: 20px;
}

pre {
  white-space: pre-wrap;
  word-wrap: break-word;
  overflow-x: auto;
}

code {
  word-break: break-word;
}



@media (min-width: 768px) {
  
}

@media (min-width: 1024px) {
  
}

@media (max-width: 767px) {
  body {
    font-size: 15px;
  }

  h1 {
    font-size: 1.6em;
  }

  .question-title {
    font-size: 1.15em;
  }

  .question {
    padding-left: 15px;
  }

  .back-to-toc {
    font-size: 0.8em;
    padding: 6px 10px;
  }
}

/* -------- TOC Layout Control -------- */
#toc ul {
  columns: 1;
}

@media (min-width: 1024px) {
  #toc ul {
    columns: 2;
  }
}


<body>
<div class="container">
<header>
<h1>Complete Snowflake &amp; Data Engineering Interview Guide</h1>
<p class="subtitle">Comprehensive Questions with Detailed Answers</p>
</header>
<div class="toc-container" id="toc">
<h2>ðŸ“˜ Contents</h2>
<ul>
<li><a href="#q1">Q1: Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</a></li>
<li><a href="#q2">Q2: Snowflake Architecture</a></li>
<li><a href="#q3">Q3: Time-Travel with scenarios like offset, query_id and timestamp</a></li>
<li><a href="#q4">Q4: Optimize the query performance and how you do it</a></li>
<li><a href="#q5">Q5: What is partition and what is micro-partitions</a></li>
<li><a href="#q6">Q6: SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</a></li>
<li><a href="#q7">Q7: How you ingested JSON data into Snowflake, what steps you perform</a></li>
<li><a href="#q8">Q8: If the file size is 50GB, how you will ingest the data</a></li>
<li><a href="#q9">Q9: Internal and External stages in Snowflake - Uses for each</a></li>
<li><a href="#q10">Q10: How you schedule a data pipeline in Snowflake</a></li>
<li><a href="#q11">Q11: Stored Procedure</a></li>
<li><a href="#q12">Q12: User Defined Functions (UDF)</a></li>
<li><a href="#q13">Q13: What is Git</a></li>
<li><a href="#q14">Q14: What is QUALIFY in Snowflake - Why it's used</a></li>
<li><a href="#q15">Q15: Benefits of Snowflake</a></li>
<li><a href="#q16">Q16: What is CDC (Change Data Capture) - Is it tool or term</a></li>
<li><a href="#q17">Q17: Python questions - List and Tuple</a></li>
<li><a href="#q18">Q18: AVRO, Parquet, ORC file formats - Uses</a></li>
<li><a href="#q19">Q19: Difference between CTE and Temp table - Where to use</a></li>
<li><a href="#q20">Q20: Difference between Transient and Dynamic and Dynamic Transient tables</a></li>
<li><a href="#q21">Q21: How you will handle schema changes in the downstream</a></li>
<li><a href="#q22">Q22: Ephemeral vs Permanent tables</a></li>
<li><a href="#q23">Q23: How to implement CDC without ETL tools in Snowflake</a></li>
<li><a href="#q24">Q24: What is DBT and uses - Execution plan</a></li>
<li><a href="#q25">Q25: Why continuous data load used - Use cases</a></li>
<li><a href="#q26">Q26: Explain end-to-end data pipeline - What are the logics you followed</a></li>
<li><a href="#q27">Q27: Why implement CDC instead of other approaches - Use case</a></li>
<li><a href="#q28">Q28: Snowflake Tasks - Troubleshooting failed queries and performance improvement</a></li>
<li><a href="#q29">Q29: What is Zero Copy clone - Uses</a></li>
<li><a href="#q30">Q30: Convert timestamp from one timezone to another</a></li>
<li><a href="#q31">Q31: Clustering Keys in Snowflake</a></li>
<li><a href="#q32">Q32: How to monitor Snowflake performance</a></li>
<li><a href="#q33">Q33: Attacama and Collibra - Uses and Differences</a></li>
<li><a href="#q34">Q34: How to optimize long running queries - How to reduce time</a></li>
<li><a href="#q35">Q35: What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</a></li>
<li><a href="#q36">Q36: Max Salary for each department and 4th max salary for each department</a></li>
<li><a href="#q37">Q37: Swap the gender value for a table - Male to Female, Female to Male</a></li>
<li><a href="#q38">Q38: DBT Project Architecture</a></li>
<li><a href="#q39">Q39: Difference between UPSERT and MERGE</a></li>
<li><a href="#q40">Q40: Data Masking and Masking Policy</a></li>
<li><a href="#q41">Q41: Difference between Hashing and Encryption</a></li>
<li><a href="#q42">Q42: Snowflake storage integration with AWS S3</a></li>
<li><a href="#q43">Q43: What is DBT and what problem does it solve in modern data stack</a></li>
<li><a href="#q44">Q44: Core components of a DBT project: Models, Tests, Seeds</a></li>
<li><a href="#q45">Q45: Materializations in DBT - Four default types</a></li>
<li><a href="#q46">Q46: Managing dependencies and referencing models in DBT</a></li>
<li><a href="#q47">Q47: What is Jinja in DBT - Simple example</a></li>
<li><a href="#q48">Q48: Data Governance best practices</a></li>
<li><a href="#q49">Q49: Data quality testing strategies</a></li>
<li><a href="#q50">Q50: Building a modern data stack - Technology selection</a></li>
<li><a href="#q51">Q51: Future trends in data engineering</a></li>
<li><a href="#q52">Q52: Explain your end-to-end data pipeline (tools + flow)</a></li>
<li><a href="#q53">Q53: How do you use Python inside Airflow for orchestration and failure alerts?</a></li>
<li><a href="#q54">Q54: How do you implement SCD Type 2 in dbt?</a></li>
<li><a href="#q55">Q55: How does dbt snapshot handle deletes?</a></li>
<li><a href="#q56">Q56: When and why do you use dbt seeds?</a></li>
<li><a href="#q57">Q57: SQL Challenge â€“ Orders and Payments</a></li>
<li><a href="#q58">Q58: dbt Materializations â€” Syntax &amp; When to Use</a></li>
<li><a href="#q59">Q59: dbt Snapshots â€” Syntax &amp; Purpose</a></li>
<li><a href="#q60">Q60: dbt YAML Files â€” Syntax, Indentation &amp; Purpose</a></li>
<li><a href="#q61">Q61: Tests in YAML â€” Built-in &amp; Relationships</a></li>
<li><a href="#q62">Q62: Materialization Configuration via dbt_project.yml</a></li>
<li><a href="#q63">Q63: Quick Interview Cheat Table &amp; Power Lines</a></li>
<li><a href="#q64">Q64: Advanced Incremental Strategies â€“ MERGE vs APPEND</a></li>
<li><a href="#q65">Q65: Custom dbt Tests â€“ Beyond Built-ins</a></li>
<li><a href="#q66">Q66: Enterprise-Grade dbt Folder Structure</a></li>
<li><a href="#q67">Q67: Production dbt Failures &amp; Root Causes (What Seniors Face)</a></li>
<li><a href="#q68">Q68: Senior-Level dbt Cheat Sheet &amp; Interview Power Lines</a></li>
<li><a href="#q69">Q69: Advanced dbt Macros &amp; Jinja Templating Patterns</a></li>
<li><a href="#q70">Q70: dbt Exposures &amp; Metrics â€“ Connecting to BI &amp; Analytics</a></li>
<li><a href="#q71">Q71: dbt on Snowflake: Performance Optimization &amp; Run Config</a></li>
<li><a href="#q72">Q72: Data Contracts &amp; Dynamic Testing in dbt</a></li>
<li><a href="#q73">Q73: CI/CD Workflows in dbt Cloud &amp; Git-Driven Development</a></li>
<li><a href="#q74">Q74: dbt Pre-Hooks &amp; Post-Hooks â€“ Advanced Execution Control</a></li>
</ul>
</div>
<div class="question" id="q1">
<div class="question-title">Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</div>
<div class="answer">
<p>This requires interpreting the start and end events to calculate the duration. Assuming Date_timestamp is a DATETIME or TIMESTAMP column.</p>
<pre>WITH EventSequence AS (
    SELECT
        Date_timestamp,
        progress,
        LAG(Date_timestamp) OVER (ORDER BY Date_timestamp) AS Previous_Timestamp,
        LAG(progress) OVER (ORDER BY Date_timestamp) AS Previous_Progress
    FROM your_table_name
),
ComputeDurations AS (
    SELECT
        Date_timestamp,
        progress,
        Previous_Timestamp,
        Previous_Progress,
        CASE
            WHEN progress = 'end' AND Previous_Progress = 'start'
            THEN DATEDIFF(SECOND, Previous_Timestamp, Date_timestamp)
            ELSE 0
        END AS Duration_Seconds
    FROM EventSequence
    WHERE progress = 'end' AND Previous_Progress = 'start'
)
SELECT
    SUM(Duration_Seconds) / 3600.0 AS Total_Compute_Hours_Overall,
    TO_DATE(Date_timestamp) AS Compute_Date,
    SUM(Duration_Seconds) / 3600.0 AS Daily_Compute_Hours
FROM ComputeDurations
GROUP BY TO_DATE(Date_timestamp)
ORDER BY Compute_Date;</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>EventSequence:</strong> This CTE assigns the Previous_Timestamp and Previous_Progress to each row</li>
<li><strong>ComputeDurations:</strong> Calculates the Duration_Seconds only for valid end events preceded by a start event</li>
<li><strong>Final SELECT:</strong> Sums durations to get Total and groups by date for Daily_Compute_Hours</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q2">
<div class="question-title">Snowflake Architecture</div>
<div class="answer">
<p>Snowflake's architecture is a unique multi-cluster shared data architecture. It separates compute and storage, allowing them to scale independently, and includes a cloud services layer for management.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Database Storage:</strong> Data is reorganized into a columnar, compressed format, encrypted, and stored in micro-partitions. This storage layer is elastic and can scale dynamically.</li>
<li><strong>Query Processing (Compute Layer):</strong> Consists of virtual warehouses - independent compute clusters that execute queries without sharing compute resources.</li>
<li><strong>Cloud Services Layer:</strong> The brain of Snowflake, coordinating all activities including authentication, metadata management, query optimization, infrastructure management, and transaction management.</li>
</ul>
<p><strong>Analogy:</strong> Imagine a library where storage is the books on shelves, compute is the reading rooms, and cloud services is the librarian system managing everything.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q3">
<div class="question-title">Time-Travel with scenarios like offset, query_id and timestamp</div>
<div class="answer">
<p>Snowflake's Time Travel feature allows you to query historical data that has been changed or deleted within a specified retention period (default 1 day, up to 90 days for Enterprise Edition).</p>
<p><strong>Using AT (OFFSET):</strong> Query data from a specific point in time relative to current time (negative offset for past).</p>
<pre>-- Current data
SELECT * FROM my_table;

-- Data from 5 minutes (300 seconds) ago
SELECT * FROM my_table AT (OFFSET =&gt; -300);</pre>
<p><strong>Using AT (TIMESTAMP):</strong> Query data as it existed at an exact past timestamp.</p>
<pre>SELECT * FROM my_table AT (TIMESTAMP =&gt; '2025-06-25 10:30:00'::TIMESTAMP_LTZ);</pre>
<p><strong>Using BEFORE (STATEMENT):</strong> Query data immediately before a specific DML statement executed.</p>
<pre>CREATE TABLE my_table_recovery AS
SELECT * FROM my_table BEFORE (STATEMENT =&gt; '123abc456def');</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q4">
<div class="question-title">Optimize the query performance and how you do it</div>
<div class="answer">
<p><strong>Key Optimization Strategies:</strong></p>
<ol>
<li><strong>Choose the Right Virtual Warehouse Size:</strong> Larger warehouses provide more compute power and memory. Use larger warehouses for complex queries on large datasets.</li>
<li><strong>Cluster Your Tables:</strong> Define clustering keys on columns frequently used in WHERE clauses, JOIN conditions, or GROUP BY clauses.</li>
<li><strong>Use Materialized Views:</strong> Pre-compute and store results of complex queries, automatically updating when base tables change.</li>
<li><strong>Query Pruning:</strong> Ensure your WHERE clauses are effective. Filter on clustered columns or columns with high cardinality.</li>
<li><strong>Effective Caching:</strong> Leverage Snowflake's Result Cache and Warehouse Cache by running the same queries multiple times.</li>
<li><strong>Avoid Anti-Patterns:</strong> Don't use SELECT *, avoid correlated subqueries, eliminate unnecessary ORDER BY or DISTINCT.</li>
<li><strong>Monitor Query Profile:</strong> Use Snowflake's Query Profile to identify bottlenecks and optimize accordingly.</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q5">
<div class="question-title">What is partition and what is micro-partitions</div>
<div class="answer">
<p><strong>Partition (General Database Concept):</strong> A strategy to divide a large table into smaller, manageable pieces based on a specified column (e.g., date, region). Usually managed explicitly by the database administrator.</p>
<p><strong>Micro-partitions (Snowflake Specific):</strong> Snowflake automatically organizes all data into immutable, compressed, columnar units typically ranging from 50 MB to 500 MB. This is automatic and transparent.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>Automatic and Transparent - handled by Snowflake</li>
<li>Rich metadata stored about each micro-partition (value ranges, distinct values, null counts)</li>
<li>Query Pruning - metadata allows Snowflake to skip irrelevant micro-partitions</li>
<li>Clustering - you can define clustering keys to optimize physical co-location of data</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q6">
<div class="question-title">SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</div>
<div class="answer">
<p><strong>SCD Type 0: Retain Original</strong> - No changes tracked. Dimension attribute value never changes.</p>
<p><strong>SCD Type 1: Overwrite</strong> - New data overwrites old data. History is not preserved.</p>
<p><strong>SCD Type 2: Add New Row</strong> - New row added with effective date ranges. History is fully preserved.</p>
<pre>Initial: EmployeeID: 201, Department: Sales, StartDate: 2020-01-01, EndDate: 9999-12-31, IsCurrent: TRUE
After Change: Department changes to Marketing
Result: 
- Old row: EndDate: 2025-05-31, IsCurrent: FALSE
- New row: Department: Marketing, StartDate: 2025-06-01, IsCurrent: TRUE</pre>
<p><strong>SCD Type 3: Add New Attribute</strong> - New column added for previous value. Partial history.</p>
<p><strong>SCD Type 4: Use History Table</strong> - Main table holds current (Type 1), separate history table stores all past versions (Type 2).</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q7">
<div class="question-title">How you ingested JSON data into Snowflake, what steps you perform</div>
<div class="answer">
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>Prepare JSON Data:</strong> Ensure well-formed JSON files (individual objects or newline-delimited)</li>
<li><strong>Stage the Files:</strong> Use Internal Stage (PUT command) or External Stage (S3, Azure, GCP)</li>
<li><strong>Create File Format:</strong> Define how to interpret JSON files
                        <pre>CREATE FILE FORMAT json_file_format
    TYPE = 'JSON'
    STRIP_OUTER_ARRAY = TRUE
    NULL_IF = ('', 'NULL');</pre>
</li>
<li><strong>Create Target Table:</strong> Either single VARIANT column or structured columns
                        <pre>CREATE TABLE raw_json_data (
    id INT,
    json_payload VARIANT,
    load_timestamp TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);</pre>
</li>
<li><strong>Copy Data:</strong> Use COPY INTO to load data from stage
                        <pre>COPY INTO raw_json_data
FROM @my_external_json_stage/
FILE_FORMAT = json_file_format
ON_ERROR = 'CONTINUE';</pre>
</li>
<li><strong>Query &amp; Transform:</strong> Use VARIANT functions to extract and transform data</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q8">
<div class="question-title">If the file size is 50GB, how you will ingest the data</div>
<div class="answer">
<p><strong>For 50GB files, the approach is:</strong></p>
<ol>
<li><strong>Stage the File:</strong> Use External Stage (S3, Azure, GCP) - faster and more reliable than internal stage</li>
<li><strong>Use Large Virtual Warehouse:</strong> Use at least X-Large or larger (2X-Large, 3X-Large) for parallel processing and sufficient memory</li>
<li><strong>Execute COPY INTO:</strong>
<pre>USE WAREHOUSE MY_LARGE_WAREHOUSE;

COPY INTO your_target_table
FROM @my_external_stage/your_50gb_file.csv
FILE_FORMAT = (FORMAT_NAME = my_csv_file_format)
ON_ERROR = 'CONTINUE'
PURGE = TRUE;</pre>
</li>
<li><strong>Key Considerations:</strong>
<ul>
<li>Use columnar formats (Parquet, ORC) if possible</li>
<li>Ensure good network bandwidth</li>
<li>Monitor COPY_HISTORY for status</li>
<li>COPY INTO is inherently parallel - larger warehouse leverages this</li>
</ul>
</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q9">
<div class="question-title">Internal and External stages in Snowflake - Uses for each</div>
<div class="answer">
<p><strong>Internal Stages (Snowflake-managed Storage):</strong></p>
<ul>
<li>User stage: @~/</li>
<li>Table stage: @%table_name</li>
<li>Named stage: CREATE STAGE my_internal_stage</li>
<li>Uses: Quick data loading, secure data transfer, temporary files</li>
</ul>
<p><strong>External Stages (User-managed Cloud Storage):</strong></p>
<ul>
<li>Points to AWS S3, Azure Blob, Google Cloud Storage</li>
<li>Uses: Large-scale automated loads, data lake integration, Snowpipe, continuous loading</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Internal Stage</th>
<th>External Stage</th>
</tr>
<tr>
<td>Storage</td>
<td>Snowflake managed</td>
<td>Your cloud storage</td>
</tr>
<tr>
<td>File Access</td>
<td>PUT / GET commands</td>
<td>COPY INTO</td>
</tr>
<tr>
<td>Cost</td>
<td>Included in Snowflake</td>
<td>Cloud provider costs</td>
</tr>
<tr>
<td>Ideal Use</td>
<td>Ad-hoc, smaller loads</td>
<td>Large-scale automated</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q10">
<div class="question-title">How you schedule a data pipeline in Snowflake</div>
<div class="answer">
<p><strong>Snowflake Tasks (Native):</strong></p>
<pre>-- Create warehouse for tasks
CREATE WAREHOUSE ETL_WH WAREHOUSE_SIZE = 'XSMALL';

-- 1. Root Task (scheduled)
CREATE TASK load_raw_data
  WAREHOUSE = ETL_WH
  SCHEDULE = 'USING CRON 0 9 * * * Asia/Kolkata'
  AS
  COPY INTO RAW_TABLE FROM @my_external_stage/raw_files/;

-- 2. Child Task (depends on load_raw_data)
CREATE TASK transform_data
  WAREHOUSE = ETL_WH
  AFTER load_raw_data
  AS
  INSERT INTO STAGING_TABLE SELECT ... FROM RAW_TABLE;

-- Enable tasks
ALTER TASK load_raw_data RESUME;
ALTER TASK transform_data RESUME;</pre>
<p><strong>External Orchestration Tools:</strong> Apache Airflow, AWS Step Functions, Azure Data Factory, Control-M, Autosys - for complex cross-platform pipelines.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q11">
<div class="question-title">Stored Procedure</div>
<div class="answer">
<p>A Stored Procedure is a set of SQL statements and procedural logic compiled and stored in the database. It can be executed by calling its name with input parameters.</p>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Encapsulation of complex logic</li>
<li>Reusability - write once, run many times</li>
<li>Parameterization - accepts input parameters</li>
<li>Better performance - pre-compiled</li>
<li>Enhanced security - grant permissions on procedure, not underlying tables</li>
</ul>
<pre>CREATE PROCEDURE calculate_daily_sales(sales_date DATE)
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE
    total_sales DECIMAL(18, 2);
BEGIN
    DELETE FROM DAILY_SALES_SUMMARY WHERE summary_date = :sales_date;
    SELECT SUM(amount) INTO total_sales FROM RAW_SALES_DATA WHERE sale_date = :sales_date;
    INSERT INTO DAILY_SALES_SUMMARY VALUES (:sales_date, :total_sales);
    IF (total_sales IS NULL) THEN
        RETURN 'No sales data found for ' || :sales_date;
    ELSE
        RETURN 'Successfully summarized for ' || :sales_date;
    END IF;
END;
$$;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q12">
<div class="question-title">User Defined Functions (UDF)</div>
<div class="answer">
<p>A User Defined Function is a custom function that performs specific operations, similar to built-in functions. UDFs encapsulate logic that can be reused within SQL queries.</p>
<p><strong>Types:</strong></p>
<ul>
<li>Scalar UDFs - return single value per input row</li>
<li>Table UDFs (UDTFs) - return set of rows per input row</li>
</ul>
<p><strong>Scalar UDF Example:</strong></p>
<pre>CREATE FUNCTION calculate_net_sales(sales_amount DECIMAL(10,2), return_amount DECIMAL(10,2))
RETURNS DECIMAL(10,2)
AS
$$
    sales_amount - COALESCE(return_amount, 0)
$$;

SELECT order_id, calculate_net_sales(sales_amount, return_amount) AS net_sales
FROM daily_transactions;</pre>
<p><strong>Languages Supported:</strong> SQL, JavaScript, Python, Java, Scala (via Snowpark)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q13">
<div class="question-title">What is Git</div>
<div class="answer">
<p>Git is a free, open-source distributed version control system (DVCS) designed to handle projects of any size. It tracks changes in source code and other files during software development, enabling collaborative work and complete change history.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Distributed:</strong> Every developer has a complete copy of the entire repository with full history. No single point of failure</li>
<li><strong>Version Control:</strong> Tracks every change with author, timestamp, and message. Complete history allows reverting to any previous state</li>
<li><strong>Branches:</strong> Parallel lines of development (main, develop, feature branches) allowing teams to work independently</li>
<li><strong>Commits:</strong> Snapshots of the entire repository at specific points with unique SHA-1 hash identifiers</li>
<li><strong>Repository:</strong> Complete collection of files, branches, and entire change history (can be local or on remote servers like GitHub)</li>
<li><strong>Staging Area (Index):</strong> Intermediate area where you select which changes to include in the next commit</li>
</ul>
<p><strong>Typical Workflow:</strong></p>
<pre>git clone https://github.com/user/repo.git  # Clone remote repo
git branch feature/new-analysis              # Create feature branch
git checkout feature/new-analysis             # Switch to feature branch
git add analysis_script.sql                   # Stage changes
git commit -m "Add new sales analysis"        # Commit with message
git push origin feature/new-analysis          # Push to remote
# Create pull request on GitHub for review
git checkout main                             # Switch back to main
git pull origin main                          # Get latest main
git merge feature/new-analysis                # Merge feature branch</pre>
<p><strong>For Data Teams:</strong> Git is essential for version controlling DBT projects, SQL scripts, Python ETL code, and documentation. Enables code review, audit trails, and rollback capabilities</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q14">
<div class="question-title">What is QUALIFY in Snowflake - Why it's used</div>
<div class="answer">
<p>QUALIFY is a clause unique to Snowflake that filters results of window functions without needing to wrap queries in subqueries or CTEs.</p>
<p><strong>Example: Find the 2nd highest paid employee in each department</strong></p>
<p><strong>With QUALIFY (Simpler):</strong></p>
<pre>SELECT employee_id, employee_name, department, salary
FROM employees
QUALIFY RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 2;</pre>
<p><strong>Without QUALIFY (requires CTE):</strong></p>
<pre>WITH EmployeeRank AS (
    SELECT ..., RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rnk
    FROM employees
)
SELECT * FROM EmployeeRank WHERE rnk = 2;</pre>
<p><strong>Benefits:</strong> Simplicity, readability, potential performance improvements, direct filtering on window function results</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q15">
<div class="question-title">Benefits of Snowflake</div>
<div class="answer">
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Separation of Compute and Storage:</strong> Scale independently - add storage without compute cost. Run small XS warehouse for lightweight queries, large 3X warehouse for complex jobs. Only pay for compute when warehouse is running</li>
<li><strong>Elasticity and Scalability:</strong> Instantly resize warehouses (XS to 3X in seconds) or auto-scale with multi-cluster warehouses for concurrency</li>
<li><strong>Near-Zero Management:</strong> Fully managed - Snowflake handles patching, updates, hardware provisioning, index maintenance. No DBAs needed for infrastructure</li>
<li><strong>Support for Semi-Structured Data:</strong> Native VARIANT type handles JSON/Avro/Parquet natively. Query nested data with dot notation without flattening</li>
<li><strong>Concurrency:</strong> Multiple independent virtual warehouses access same data simultaneously without locking or contention. BI team on WH1, ETL team on WH2, no impact</li>
<li><strong>Data Sharing:</strong> Secure, instantaneous live data sharing with other Snowflake accounts (same or different regions). Share without copying data</li>
<li><strong>Time Travel &amp; Fail-safe:</strong> Query data as it existed 1-90 days ago. Accidentally dropped table? UNDROP within retention period. 7-day fail-safe for disaster recovery</li>
<li><strong>Performance Optimization:</strong> Automatic micro-partitioning with intelligent pruning, multi-layer caching (result + warehouse cache), optional clustering keys</li>
<li><strong>Security &amp; Compliance:</strong> AES-256 encryption at rest/transit, multi-factor MFA, RBAC + object-level privileges, row/column masking, SOC2/PCI/HIPAA/GDPR certified</li>
<li><strong>Ecosystem Integration:</strong> Native connectors for Tableau, Power BI, Looker. DBT integration, Spark through connectors, Python/Pandas via snowpark_python</li>
</ul>
<p><strong>Cost Efficiency Example:</strong> Traditional data warehouse requires expensive hardware, undergoes periods of low utilization (pay for unused capacity). Snowflake: suspend warehouse when not used, pay only for storage. Scale up for batch jobs, down immediately after</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q16">
<div class="question-title">What is CDC (Change Data Capture) - Is it tool or term</div>
<div class="answer">
<p>CDC is primarily a <strong>term and technique</strong> (not a single tool). It identifies and captures changes (INSERT, UPDATE, DELETE) made to data in a source database and delivers those changes to a target system efficiently instead of reprocessing entire datasets.</p>
<p><strong>Common Mechanisms:</strong></p>
<ul>
<li><strong>Timestamp-Based (Polling):</strong> Periodic queries find rows where last_updated_timestamp &gt; previous_check_time. Simple but can miss deletes, slow for large tables, unreliable if timestamps aren't consistent</li>
<li><strong>Log-Based (Most Robust):</strong> Read database transaction logs (MySQL binlog, Oracle redo logs, SQL Server transaction log). Captures ALL changes in real-time, no impact on source DB, handles deletes</li>
<li><strong>Trigger-Based:</strong> Database triggers fire on INSERT/UPDATE/DELETE, write change details to change table. Real-time but adds overhead to source system performance</li>
<li><strong>Hash-Based:</strong> Calculate hash of row, compare snapshots to find differences. Works without source modifications but resource-intensive for large datasets</li>
</ul>
<p><strong>Why CDC Matters:</strong></p>
<pre>-- Without CDC (Full Load Every Time) - INEFFICIENT
COPY INTO warehouse_sales FROM source_db  -- 100GB table
-- Takes 2 hours, processes entire table even if only 1GB changed
-- Heavy impact on source system

-- With CDC (Only Changes) - EFFICIENT
COPY INTO warehouse_sales FROM change_stream  -- Only 1GB changed
-- Takes 5 minutes, minimal source system impact
-- Near real-time data availability</pre>
<p><strong>Popular CDC Tools:</strong> Qlik Replicate (log-based), Fivetran (managed CDC), Debezium (Kafka-based open source), Oracle GoldenGate (enterprise), AWS DMS, Apache Hudi/Iceberg (data lake CDC)</p>
<p><strong>Real Use Cases:</strong> Real-time analytics dashboards, microservices data synchronization, database migration with zero downtime, maintaining dimensional data in data warehouse</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q17">
<div class="question-title">Python questions - List and Tuple</div>
<div class="answer">
<p><strong>List []: Mutable (changeable)</strong> - ordered collection that can be modified after creation</p>
<pre>my_list = [10, "apple", 3.14, True, [1, 2, 3]]  # Can contain mixed types, even nested lists
print(my_list[1])              # Output: apple (zero-indexed)
my_list[0] = 20                # Modify element
my_list.append("banana")       # Add element
my_list.extend([4, 5, 6])      # Add multiple elements
my_list.insert(1, "orange")    # Insert at specific position
my_list.remove("apple")        # Remove by value
popped = my_list.pop()         # Remove and return last element
my_list.sort()                 # Sort in-place
my_list.clear()                # Remove all elements

# Common operations
for item in my_list:
    print(item)  # Iterate

sliced = my_list[1:4]  # Slicing creates new list
reversed_list = my_list[::-1]  # Reverse</pre>
<p><strong>Tuple (): Immutable (unchangeable)</strong> - ordered collection that cannot be modified after creation</p>
<pre>my_tuple = (10, "apple", 3.14, True, (1, 2, 3))  # Can contain mixed types
print(my_tuple[1])           # Output: apple
# my_tuple[0] = 20           # TypeError! Tuples are immutable

# Single element tuple REQUIRES trailing comma
single_tuple = (5,)          # Correct
wrong_single = (5)           # This is just an int, not a tuple!

# Tuple packing and unpacking
coordinates = 10, 20, 30     # Automatic packing into tuple
x, y, z = coordinates        # Unpacking tuple to variables

# Tuples can be used as dictionary keys (lists cannot)
my_dict = {(0, 0): "origin", (1, 1): "diagonal"}

# Operations (read-only)
print(my_tuple.count(10))     # Count occurrences
print(my_tuple.index("apple"))  # Find index
print(len(my_tuple))          # Length</pre>
<table>
<tr>
<th>Feature</th>
<th>List</th>
<th>Tuple</th>
</tr>
<tr>
<td>Mutability</td>
<td>Mutable (changeable)</td>
<td>Immutable (fixed)</td>
</tr>
<tr>
<td>Performance</td>
<td>Slower (tracking changes)</td>
<td>Faster (fixed size)</td>
</tr>
<tr>
<td>Memory</td>
<td>More memory overhead</td>
<td>Less memory usage</td>
</tr>
<tr>
<td>As Dict Key</td>
<td>No - not hashable</td>
<td>Yes - if elements are hashable</td>
</tr>
<tr>
<td>Best Use Case</td>
<td>Dynamic data, frequent changes</td>
<td>Fixed data, constant values</td>
</tr>
<tr>
<td>Return from Function</td>
<td>N/A</td>
<td>Yes - functions return tuples</td>
</tr>
</table>
<p><strong>In Data Engineering:</strong> Tuples used for immutable data records, fixed field orders. Lists used for accumulating/processing data. DBT source definitions, data lineage metadata often use tuples</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q18">
<div class="question-title">AVRO, Parquet, ORC file formats - Uses</div>
<div class="answer">
<p><strong>AVRO (Apache Avro):</strong> Row-oriented data serialization format with self-describing schema</p>
<ul>
<li><strong>Schema Definition:</strong> Schema stored in JSON within each file (or managed externally via schema registry)</li>
<li><strong>Excellent Schema Evolution:</strong> Can add/remove/modify fields with clear rules for backward/forward compatibility</li>
<li><strong>Size:</strong> Compact binary format after serialization</li>
<li><strong>Uses:</strong>
<ul>
<li>Real-time streaming with Apache Kafka (Confluent)</li>
<li>Inter-process communication between services</li>
<li>Data archival where schema changes occur over time</li>
<li>CDC platforms like Debezium</li>
</ul>
</li>
<li><strong>Limitations:</strong> Row-based format, less efficient for OLAP queries (must read many rows to get one column)</li>
</ul>
<p><strong>Parquet (Apache Parquet):</strong> Columnar storage format optimized for analytics</p>
<ul>
<li><strong>Column Storage:</strong> Data organized by column, not row. Reading one column doesn't require reading others</li>
<li><strong>Compression:</strong> Column-level compression (RLE, dictionary encoding). Achieves 10x+ compression on repetitive columns</li>
<li><strong>Query Performance:</strong> Queries scanning specific columns are 10-100x faster than row-oriented formats</li>
<li><strong>Uses:</strong>
<ul>
<li>Data lakes (AWS S3, Azure ADLS) - primary format for analytics</li>
<li>Spark DataFrames and Pandas - native support</li>
<li>Snowflake, BigQuery, Redshift - optimized for Parquet</li>
<li>Machine Learning datasets - efficient feature access</li>
</ul>
</li>
<li><strong>Overhead:</strong> Largest metadata overhead, slower for row-by-row access but excellent for batch analytics</li>
</ul>
<p><strong>ORC (Optimized Row Columnar):</strong> Columnar format developed by Hortonworks for Hadoop ecosystem</p>
<ul>
<li><strong>Predicate Pushdown:</strong> Filtering applied during read from disk, not after. Scans only relevant stripes</li>
<li><strong>Type Awareness:</strong> Understanding of data types enables better compression and optimizations</li>
<li><strong>Uses:</strong>
<ul>
<li>Apache Hive (OLAP queries on Hadoop)</li>
<li>Data warehousing on Hadoop clusters</li>
<li>Spark SQL workloads in Hadoop ecosystems</li>
</ul>
</li>
<li><strong>Comparison:</strong> ORC typically smaller than Parquet (better compression) but less ecosystem support outside Hadoop</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>AVRO</th>
<th>Parquet</th>
<th>ORC</th>
</tr>
<tr>
<td>Storage Type</td>
<td>Row-oriented</td>
<td>Columnar</td>
<td>Columnar</td>
</tr>
<tr>
<td>Schema Evolution</td>
<td>Excellent (reader/writer schemas)</td>
<td>Good (backward compatible)</td>
<td>Good</td>
</tr>
<tr>
<td>Compression Ratio</td>
<td>Moderate</td>
<td>Very Good</td>
<td>Best</td>
</tr>
<tr>
<td>Query Performance</td>
<td>Good for full row reads</td>
<td>Excellent for column subset</td>
<td>Excellent for analytic queries</td>
</tr>
<tr>
<td>Ecosystem Support</td>
<td>Kafka, Pulsar, Streaming</td>
<td>Universal (Cloud DWs, Spark)</td>
<td>Hadoop/Hive focused</td>
</tr>
<tr>
<td>Best For</td>
<td>Event streaming, CDC</td>
<td>Data lakes, analytics</td>
<td>Hadoop, Hive queries</td>
</tr>
</table>
<p><strong>Recommendation for Modern Data Stack:</strong> Use Parquet for data lakes (S3/ADLS); Use Avro for streaming pipelines and CDC; ORC if using Hadoop ecosystem</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q19">
<div class="question-title">Difference between CTE and Temp table - Where to use</div>
<div class="answer">
<p><strong>CTE (Common Table Expression) - WITH Clause:</strong> Temporary named result set defined within a single query. Often called "WITH clause" or subquery alternative</p>
<pre>-- Multiple CTEs in one query
WITH 
-- CTE 1: Get top customers
TopCustomers AS (
    SELECT customer_id, SUM(order_amount) AS total_amount
    FROM orders 
    GROUP BY customer_id 
    ORDER BY total_amount DESC 
    LIMIT 100
),
-- CTE 2: Get their recent orders
RecentOrders AS (
    SELECT tc.customer_id, o.order_id, o.order_date, o.amount
    FROM TopCustomers tc
    JOIN orders o ON tc.customer_id = o.customer_id
    WHERE o.order_date &gt;= CURRENT_DATE - 30
)
-- Main query uses CTEs
SELECT customer_id, COUNT(*) as order_count, AVG(amount) as avg_amount
FROM RecentOrders
GROUP BY customer_id;</pre>
<p><strong>Temp Table (TEMPORARY Table):</strong> Session-scoped table that persists in database until session ends. Can be used across multiple queries in same session</p>
<pre>-- Create temp table that exists for this session
CREATE TEMPORARY TABLE top_customers AS
SELECT customer_id, SUM(order_amount) AS total_amount
FROM orders 
GROUP BY customer_id 
ORDER BY total_amount DESC 
LIMIT 100;

-- Use temp table in Query 1
SELECT * FROM top_customers WHERE total_amount &gt; 10000;

-- Use same temp table in Query 2
SELECT tc.customer_id, COUNT(*) 
FROM top_customers tc
JOIN orders o ON tc.customer_id = o.customer_id
GROUP BY tc.customer_id;

-- Query 3 still has access to temp table
INSERT INTO analytics_summary
SELECT * FROM top_customers;

-- Eventually session ends, temp table auto-drops</pre>
<table>
<tr>
<th>Feature</th>
<th>CTE (WITH)</th>
<th>Temporary Table</th>
</tr>
<tr>
<td>Scope</td>
<td>Single query only</td>
<td>Entire session</td>
</tr>
<tr>
<td>Persistence</td>
<td>Logical, not stored</td>
<td>Physical table in database</td>
</tr>
<tr>
<td>Reusability</td>
<td>Within same query</td>
<td>Across multiple queries</td>
</tr>
<tr>
<td>Materialization</td>
<td>Optimizer decides (often inlined)</td>
<td>Always materialized</td>
</tr>
<tr>
<td>Storage Cost</td>
<td>None</td>
<td>Counts toward table storage</td>
</tr>
<tr>
<td>Performance</td>
<td>Can be inlined (fast) or materialized</td>
<td>Predictable, always fast for reads</td>
</tr>
<tr>
<td>When to Use</td>
<td>Breaking down complex queries, single-use logic</td>
<td>Expensive operation reused many times in session</td>
</tr>
</table>
<p><strong>When to Use CTEs:</strong></p>
<ul>
<li>Breaking down complex queries into readable parts</li>
<li>Recursive queries (hierarchical data like org charts)</li>
<li>Logical organization without needing storage</li>
</ul>
<p><strong>When to Use Temp Tables:</strong></p>
<ul>
<li>Expensive calculation reused multiple times in session</li>
<li>Multi-step ETL processes in stored procedures</li>
<li>Needing to add indexes or optimize for specific access patterns</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q20">
<div class="question-title">Difference between Transient and Dynamic and Dynamic Transient tables</div>
<div class="answer">
<p><strong>TRANSIENT Table:</strong> Designed specifically for temporary data that doesn't require long-term protection</p>
<ul>
<li><strong>Time Travel:</strong> Default 0 days (configurable up to 1 day max) - cannot query historical versions</li>
<li><strong>Fail-safe:</strong> None (no 7-day recovery window)</li>
<li><strong>Storage Cost:</strong> Approximately 50% lower than permanent tables</li>
<li><strong>Creation:</strong> CREATE TRANSIENT TABLE my_table (...);</li>
<li><strong>Uses:</strong> Staging tables for daily loads, intermediate ETL results, temporary aggregations</li>
</ul>
<p><strong>Example Use Case:</strong></p>
<pre>-- Daily staging table for raw data
CREATE OR REPLACE TRANSIENT TABLE raw_daily_sales AS
COPY INTO FROM @s3_stage/daily_sales/
FILE_FORMAT = (TYPE = CSV);

-- Transform and move to permanent table
INSERT INTO fact_sales
SELECT * FROM raw_daily_sales
WHERE processing_complete = TRUE;

-- raw_daily_sales can be safely dropped - data is in fact_sales</pre>
<p><strong>Note:</strong> "Dynamic" and "Dynamic Transient" are not standard Snowflake table types. They may refer to:</p>
<ul>
<li><strong>Dynamic Data Loading:</strong> Frequently updated data with continuous CDC</li>
<li><strong>Project-specific Naming:</strong> Custom conventions for different use cases</li>
</ul>
<p>In production, most tables are either <strong>Permanent</strong> (for analytics) or <strong>Transient</strong> (for staging). See Question 22 for Ephemeral/Temporary tables comparison.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q21">
<div class="question-title">How you will handle schema changes in the downstream</div>
<div class="answer">
<p><strong>Strategies for Handling Schema Changes:</strong></p>
<ol>
<li><strong>Communication &amp; Collaboration:</strong> Establish data governance processes, cross-functional meetings, maintain documentation</li>
<li><strong>Flexible Data Types:</strong> Use VARIANT columns for semi-structured data resilience to schema changes</li>
<li><strong>Schema-on-Read:</strong> Apply schema at query time, not load time (data lakes with Parquet/ORC)</li>
<li><strong>Additive Schema Changes (Preferred):</strong> Add new columns rather than renaming/deleting/changing types</li>
<li><strong>Soft Deletes/Deprecation:</strong> Mark columns as deprecated before removal</li>
<li><strong>Versioning Tables/Views:</strong> Create views over base tables to maintain consistent interface
                        <pre>-- Original view
CREATE VIEW v_customer AS SELECT customer_id, email FROM raw_customer;

-- Source table changes: 'email' becomes 'primary_contact_email'
-- Update view to maintain compatibility
CREATE VIEW v_customer AS SELECT customer_id, primary_contact_email AS email FROM raw_customer;</pre>
</li>
<li><strong>Data Contracts/Schema Registries:</strong> Define formal contracts using Avro/Protobuf</li>
<li><strong>Impact Analysis &amp; Testing:</strong> Understand dependencies, test in dev/test before prod</li>
<li><strong>Graceful Degradation:</strong> Implement logic that handles missing columns gracefully</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q22">
<div class="question-title">Ephemeral vs Permanent tables</div>
<div class="answer">
<p><strong>Permanent Table (Default):</strong> Standard table type with full data durability</p>
<ul>
<li>Time Travel: 1-90 days</li>
<li>Fail-safe: 7 days</li>
<li>Persistence: Until explicitly dropped</li>
<li>Cost: Highest storage cost</li>
<li>Uses: Core data, historical records</li>
</ul>
<p><strong>Ephemeral (TEMPORARY Table):</strong> Session-scoped table</p>
<ul>
<li>Persistence: Auto-dropped at session end</li>
<li>Time Travel: Session-bound (1 day max)</li>
<li>Fail-safe: None</li>
<li>Cost: Lowest storage cost</li>
<li>Uses: Ad-hoc analysis, session-specific work</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Permanent</th>
<th>TEMPORARY (Ephemeral)</th>
</tr>
<tr>
<td>Persistence</td>
<td>Until dropped</td>
<td>Session-scoped</td>
</tr>
<tr>
<td>Time Travel</td>
<td>1-90 days</td>
<td>Session-bound</td>
</tr>
<tr>
<td>Fail-safe</td>
<td>7 days</td>
<td>None</td>
</tr>
<tr>
<td>Cost</td>
<td>Highest</td>
<td>Lowest</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q23">
<div class="question-title">How to implement CDC without ETL tools in Snowflake</div>
<div class="answer">
<p><strong>Strategy 1: Timestamp-Based CDC</strong></p>
<pre>CREATE TABLE staging_data (
    id INT, data VARCHAR, last_modified TIMESTAMP
);

MERGE INTO target_table AS t
USING (SELECT * FROM staging_data WHERE last_modified &gt; :last_load_time) AS s
ON t.id = s.id
WHEN MATCHED THEN UPDATE SET t.data = s.data
WHEN NOT MATCHED THEN INSERT VALUES (s.id, s.data);</pre>
<p><strong>Strategy 2: Snowflake Streams (Recommended):</strong></p>
<pre>CREATE STREAM my_stream ON TABLE source_table;

-- Query stream - tracks inserts, updates, deletes
SELECT * FROM my_stream;
-- METADATA$ACTION shows 'INSERT' or 'DELETE'
-- METADATA$ISUPDATE shows if row is part of UPDATE

-- Consume changes atomically
MERGE INTO target_table AS t
USING my_stream AS s
ON t.id = s.id
WHEN MATCHED AND s.METADATA$ACTION='DELETE' THEN DELETE
WHEN NOT MATCHED AND s.METADATA$ACTION='INSERT' THEN INSERT ...;</pre>
<p><strong>Benefits of Streams:</strong> Exactly-once processing, captures all DML, low performance impact</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q24">
<div class="question-title">What is DBT and uses - Execution plan</div>
<div class="answer">
<p>DBT (Data Build Tool) is an open-source framework that enables data analysts to transform data in their warehouse using SQL and software engineering best practices. It focuses on the T in ELT.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>SQL-centric transformations</li>
<li>Modularity and reusability</li>
<li>Version control with Git</li>
<li>Built-in data quality tests</li>
<li>Auto-generated documentation</li>
<li>Dependency graph (DAG)</li>
<li>Jinja templating for dynamic SQL</li>
</ul>
<p><strong>DBT Execution Plan (dbt run):</strong></p>
<ol>
<li><strong>Parsing:</strong> Read .sql files, parse Jinja templates, resolve ref() functions</li>
<li><strong>DAG Building:</strong> Create dependency graph of all models</li>
<li><strong>Dependency Resolution:</strong> Determine execution order</li>
<li><strong>Materialization:</strong> Decide how to build each model (view, table, incremental, ephemeral)</li>
<li><strong>SQL Generation &amp; Execution:</strong> Generate SQL, execute in warehouse in correct order</li>
<li><strong>Post-Run Actions:</strong> Run tests, generate documentation</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q25">
<div class="question-title">Why continuous data load used - Use cases</div>
<div class="answer">
<p>Continuous data loading moves data as soon as it becomes available, providing near real-time insights instead of batch windows.</p>
<p><strong>Why Use It:</strong></p>
<ul>
<li>Near real-time analytics and operational dashboards</li>
<li>Reduced data latency</li>
<li>Improved responsiveness for decision-making</li>
<li>Eliminates batch window constraints</li>
<li>Distributes compute usage evenly over time</li>
<li>Better data quality feedback loops</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li><strong>Operational Analytics:</strong> Real-time sales dashboards, metrics</li>
<li><strong>Fraud Detection:</strong> Analyze transactions in real-time</li>
<li><strong>IoT/Sensor Data:</strong> Monitor machine performance, predictive maintenance</li>
<li><strong>Clickstream Analytics:</strong> Track user behavior, personalize experiences</li>
<li><strong>Log Analysis:</strong> Security monitoring, error detection</li>
<li><strong>Supply Chain:</strong> Real-time tracking and optimization</li>
<li><strong>Customer 360:</strong> Build comprehensive customer views for personalization</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q26">
<div class="question-title">Explain end-to-end data pipeline - What are the logics you followed</div>
<div class="answer">
<p><strong>Typical Stages:</strong></p>
<ol>
<li><strong>Data Sources:</strong> Operational databases, SaaS apps, APIs, logs (identify what data, how to access, frequency)</li>
<li><strong>Ingestion/Extraction:</strong> Batch or streaming (full load vs incremental CDC)</li>
<li><strong>Landing/Staging:</strong> Raw data storage with schema-on-read approach</li>
<li><strong>Transformation:</strong> Clean, enrich, conform, aggregate (using DBT, SQL, or external engines)</li>
<li><strong>Serving/Presentation:</strong> Optimized views/tables for consumption</li>
<li><strong>Consumption:</strong> BI tools, dashboards, data science, applications</li>
</ol>
<p><strong>Cross-cutting Concerns:</strong></p>
<ul>
<li><strong>Orchestration:</strong> Schedule and manage pipeline stages (Airflow, Tasks, Step Functions)</li>
<li><strong>Monitoring &amp; Alerting:</strong> Track pipeline health, errors, data quality</li>
<li><strong>Data Lineage:</strong> Understand data flow, debug issues</li>
<li><strong>Cost Management:</strong> Monitor and optimize cloud resource usage</li>
<li><strong>Version Control:</strong> Git for all code (SQL, Python, DBT)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q27">
<div class="question-title">Why implement CDC instead of other approaches - Use case</div>
<div class="answer">
<p><strong>Why CDC instead of full loads:</strong></p>
<ul>
<li><strong>Efficiency:</strong> Only changed data transferred/processed, reduces bandwidth and compute</li>
<li><strong>Near Real-time:</strong> Data availability within minutes/seconds</li>
<li><strong>Reduced Impact:</strong> Minimal strain on source systems (especially log-based CDC)</li>
<li><strong>Captures Deletes:</strong> Unlike timestamp queries, robust CDC captures DELETE operations</li>
<li><strong>Simplified Logic:</strong> Streams abstract away complexity of determining changes</li>
</ul>
<p><strong>Real-world Use Case: E-commerce Inventory Management</strong></p>
<p>Problem without CDC: Nightly batch updates mean inventory is 12-24 hours old â†’ customers order out-of-stock items, poor warehouse operations</p>
<p>Solution with CDC: Stream changes from inventory DB into Snowflake using CDC â†’ near real-time dashboards â†’ website always knows actual stock levels â†’ automated reordering â†’ efficient logistics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q28">
<div class="question-title">Snowflake Tasks - Troubleshooting failed queries and performance improvement</div>
<div class="answer">
<p><strong>Snowflake Tasks:</strong> Execute SQL statements or call stored procedures on a recurring schedule or when conditions are met</p>
<p><strong>Key Features:</strong> Scheduling (cron-like), DAG dependencies, conditional execution, error handling</p>
<p><strong>Troubleshooting Failed/Slow Queries:</strong></p>
<ol>
<li><strong>Identify Problem:</strong> Query History in UI or ACCOUNT_USAGE.QUERY_HISTORY</li>
<li><strong>Analyze Query Profile:</strong> Check execution phases, identify bottlenecks (scanning, joining, spilling)</li>
<li><strong>Common Issues &amp; Solutions:</strong>
<ul>
<li>Spilling to disk â†’ Increase warehouse size</li>
<li>Poor pruning â†’ Add clustering keys or improve WHERE clauses</li>
<li>Inefficient joins â†’ Rewrite query, change join order</li>
<li>Expensive operations â†’ Avoid SELECT *, DISTINCT, ORDER BY unless necessary</li>
</ul>
</li>
<li><strong>Task-Specific:</strong> Check TASK_HISTORY for STATE and ERROR_MESSAGE, verify warehouse is available</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q29">
<div class="question-title">What is Zero Copy clone - Uses</div>
<div class="answer">
<p>Zero-Copy Cloning creates an instant copy of a database, schema, or table without physically duplicating data. Snowflake creates metadata pointers to the same micro-partitions. Changes use copy-on-write.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li>Instantaneous - created almost instantly</li>
<li>No storage cost at creation - pay only for incremental changes</li>
<li>No data movement</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>CREATE DATABASE prod_db_clone CLONE prod_db;
CREATE SCHEMA temp_schema CLONE prod_schema;
CREATE TABLE test_table CLONE prod_table;</pre>
<p><strong>Uses:</strong></p>
<ul>
<li><strong>Development/Testing:</strong> Clone production DB for developers to test without affecting prod</li>
<li><strong>Disaster Recovery:</strong> Create point-in-time recovery environment quickly</li>
<li><strong>Analytical Workloads:</strong> Isolate complex queries from main system</li>
<li><strong>Version Control:</strong> Snapshot data at milestones for rollback capability</li>
<li><strong>Historical Analysis:</strong> Create monthly snapshots with minimal cost</li>
<li><strong>What-If Scenarios:</strong> Run simulations without impacting production</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q30">
<div class="question-title">Convert timestamp from one timezone to another</div>
<div class="answer">
<p>Snowflake has three TIMESTAMP types: TIMESTAMP_LTZ (Local Time Zone), TIMESTAMP_NTZ (No Time Zone), TIMESTAMP_TZ (Time Zone).</p>
<p><strong>Using CONVERT_TIMEZONE (Recommended):</strong></p>
<pre>SELECT CONVERT_TIMEZONE('UTC', 'America/New_York', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 06:00:00.000'

SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'Asia/Kolkata', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 22:30:00.000'

-- With a column
SELECT event_id, event_timestamp,
       CONVERT_TIMEZONE('UTC', 'America/New_York', event_timestamp) AS event_timestamp_est
FROM events;</pre>
<p><strong>Using AT TIME ZONE:</strong></p>
<pre>SELECT '2025-07-04 10:00:00'::TIMESTAMP_NTZ AT TIME ZONE 'America/New_York';

-- Set session timezone first
ALTER SESSION SET TIMEZONE = 'UTC';
SELECT event_timestamp AT TIME ZONE 'Asia/Kolkata'
FROM events;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q31">
<div class="question-title">Clustering Keys in Snowflake</div>
<div class="answer">
<p>Clustering Keys optimize the physical organization of data within micro-partitions to improve query performance.</p>
<p><strong>How They Work:</strong> Snowflake's Automatic Clustering service re-clusters data if clustering becomes stale, physically reorganizing micro-partitions to group similar values together.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Improved query performance for filtered/joined queries</li>
<li>Reduced data scanning â†’ fewer credits consumed</li>
<li>Automatic and transparent</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>-- During creation
CREATE TABLE sales (
    event_date DATE,
    category VARCHAR,
    value DECIMAL
) CLUSTER BY (event_date, category);

-- After creation
ALTER TABLE sales CLUSTER BY (event_date, category);

-- Remove clustering
ALTER TABLE sales DROP CLUSTERING KEY;</pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Very large tables (hundreds of GB to TB)</li>
<li>Frequent filtering/joining on specific columns</li>
<li>High cardinality columns with skewed data</li>
<li>Data ingestion patterns cause poor clustering over time</li>
</ul>
<p><strong>Monitor with:</strong> SYSTEM$CLUSTERING_INFORMATION() function</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q32">
<div class="question-title">How to monitor Snowflake performance</div>
<div class="answer">
<p><strong>Snowflake Web Interface:</strong></p>
<ul>
<li>Query History - view all queries, duration, credits, status</li>
<li>Query Profile - visualize execution plan, identify bottlenecks</li>
<li>Warehouses - monitor status, usage, credit consumption</li>
</ul>
<p><strong>Account Usage Views (SNOWFLAKE.ACCOUNT_USAGE):</strong></p>
<ul>
<li>QUERY_HISTORY - comprehensive query data</li>
<li>WAREHOUSE_METERING_HISTORY - daily credit consumption per warehouse</li>
<li>AUTOMATIC_CLUSTERING_HISTORY - credits consumed by clustering</li>
<li>PIPE_USAGE_HISTORY - credits consumed by Snowpipe</li>
<li>COPY_HISTORY - COPY INTO command status</li>
<li>TASK_HISTORY - task execution history</li>
</ul>
<p><strong>External Tools:</strong></p>
<ul>
<li>Cloud monitoring (CloudWatch, Azure Monitor)</li>
<li>Third-party APM tools (Datadog, Splunk)</li>
<li>BI tools for custom dashboards</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q33">
<div class="question-title">Attacama and Collibra - Uses and Differences</div>
<div class="answer">
<p><strong>Collibra:</strong> Business-centric data governance platform</p>
<ul>
<li>Strong emphasis on stewardship and collaboration</li>
<li>Excellent Business Glossary, workflow automation, policy management</li>
<li>Best for: Building data governance programs, empowering business users, compliance</li>
</ul>
<p><strong>Ataccama ONE:</strong> Integrated data management platform</p>
<ul>
<li>Combines data quality, MDM, catalog, and governance</li>
<li>Strong data quality engine, Master Data Management, AI/ML automation</li>
<li>Best for: Data quality frameworks, MDM solutions, operational data management</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Collibra</th>
<th>Ataccama ONE</th>
</tr>
<tr>
<td>Core Focus</td>
<td>Business Governance, Stewardship</td>
<td>Data Quality, MDM, Governance</td>
</tr>
<tr>
<td>Strength</td>
<td>Business Glossary, Workflows</td>
<td>Data Quality, MDM, AI/ML</td>
</tr>
<tr>
<td>Target Users</td>
<td>Data Stewards, Governance Teams</td>
<td>Data Engineers, Quality Teams</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q34">
<div class="question-title">How to optimize long running queries - How to reduce time</div>
<div class="answer">
<p><strong>Step 1: Analyze Query Profile (Mandatory)</strong></p>
<ul>
<li>Identify bottleneck operators (scanning, joining, aggregating, sorting)</li>
<li>Check spilling to disk - indicates memory constraints</li>
<li>Evaluate pruning efficiency</li>
</ul>
<p><strong>Step 2: Right-Size Virtual Warehouse</strong></p>
<pre>-- Increase warehouse size if spilling or high queue time
ALTER WAREHOUSE MY_WH SET WAREHOUSE_SIZE = 'LARGE';</pre>
<p><strong>Step 3: Optimize SQL Logic</strong></p>
<ul>
<li>SELECT only necessary columns (avoid SELECT *)</li>
<li>Filter early and effectively with WHERE clauses</li>
<li>Optimize joins - proper join order, use INNER JOIN when possible</li>
<li>Avoid correlated subqueries - use JOINs instead</li>
<li>Eliminate unnecessary DISTINCT or ORDER BY</li>
</ul>
<p><strong>Step 4: Leverage Data Organization</strong></p>
<ul>
<li>Define clustering keys for very large tables</li>
<li>Create materialized views for complex, frequently-run queries</li>
</ul>
<p><strong>Step 5: Leverage Caching</strong></p>
<ul>
<li>Encourage reuse of queries for result cache</li>
<li>Keep warehouses running for warehouse cache benefits</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q35">
<div class="question-title">What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</div>
<div class="answer">
<p>Snowpipe is Snowflake's continuous data ingestion service that loads data as soon as files appear in cloud storage.</p>
<p><strong>AWS Integration Flow:</strong> S3 Event â†’ SNS â†’ SQS â†’ Snowpipe â†’ COPY INTO</p>
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>AWS Setup:</strong> Create S3 bucket, SNS topic, SQS queue, IAM role with permissions</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_pipe_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="3">
<li><strong>Create Stage:</strong></li>
</ol>
<pre>CREATE STAGE raw_stage
  STORAGE_INTEGRATION = s3_pipe_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<ol start="4">
<li><strong>Create Target Table:</strong></li>
</ol>
<pre>CREATE TABLE raw_data (col1 INT, col2 VARCHAR, load_ts TIMESTAMP);</pre>
<ol start="5">
<li><strong>Create Snowpipe:</strong></li>
</ol>
<pre>CREATE PIPE raw_data_pipe
  AUTO_INGEST = TRUE
  AWS_SQS_QUEUE_ARN = 'arn:aws:sqs:region:account:queue'
  AS
  COPY INTO raw_data FROM @raw_stage;</pre>
<ol start="6">
<li><strong>Post-Load Processing (Optional):</strong></li>
</ol>
<pre>-- Create stream on target table
CREATE STREAM raw_data_stream ON TABLE raw_data;

-- Create stored proc for transformations
CREATE PROCEDURE process_new_data()
  AS $$ 
    MERGE INTO fact_table AS t
    USING raw_data_stream AS s ON t.id = s.id
    WHEN NOT MATCHED THEN INSERT ...;
  $$;

-- Create task to run after data arrival
CREATE TASK transform_task
  WHEN SYSTEM$STREAM_HAS_DATA('raw_data_stream')
  AS CALL process_new_data();</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q36">
<div class="question-title">Max Salary for each department and 4th max salary for each department</div>
<div class="answer">
<p><strong>Max Salary for Each Department:</strong></p>
<pre>SELECT department, MAX(salary) AS max_salary_in_department
FROM employees
GROUP BY department
ORDER BY department;</pre>
<p><strong>4th Max Salary for Each Department:</strong></p>
<pre>SELECT employee_id, department, salary
FROM employees
QUALIFY DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 4
ORDER BY department, salary DESC;</pre>
<p><strong>Using CTE (More Portable SQL):</strong></p>
<pre>WITH RankedSalaries AS (
    SELECT employee_id, department, salary,
           DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank
    FROM employees
)
SELECT employee_id, department, salary
FROM RankedSalaries
WHERE salary_rank = 4
ORDER BY department;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q37">
<div class="question-title">Swap the gender value for a table - Male to Female, Female to Male</div>
<div class="answer">
<pre>UPDATE users
SET gender = CASE
    WHEN gender = 'Male' THEN 'Female'
    WHEN gender = 'Female' THEN 'Male'
    ELSE gender -- Keep other values (Prefer not to say, NULL, etc.)
END
WHERE gender IN ('Male', 'Female');</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li>CASE statement evaluates current value of gender column</li>
<li>If 'Male', changes to 'Female' and vice versa</li>
<li>ELSE gender preserves other values unchanged</li>
<li>WHERE clause optimizes by processing only relevant rows</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q38">
<div class="question-title">DBT Project Architecture</div>
<div class="answer">
<p><strong>Typical DBT Project Structure:</strong></p>
<pre>my_dbt_project/
â”œâ”€â”€ dbt_project.yml         # Core configuration
â”œâ”€â”€ models/                 # Data transformation models
â”‚   â”œâ”€â”€ staging/           # Raw, light transformations
â”‚   â”œâ”€â”€ intermediate/      # Complex joins, business logic
â”‚   â””â”€â”€ marts/             # Final, user-facing models
â”œâ”€â”€ analysis/              # Ad-hoc SQL queries
â”œâ”€â”€ macros/                # Reusable SQL snippets
â”œâ”€â”€ seeds/                 # Static CSV lookup tables
â”œâ”€â”€ snapshots/             # SCD Type 2 configurations
â”œâ”€â”€ tests/                 # Custom data quality tests
â”œâ”€â”€ logs/                  # DBT execution logs
â””â”€â”€ target/                # Compiled SQL, manifests</pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>dbt_project.yml:</strong> Project configuration, default materializations</li>
<li><strong>Models:</strong> SQL SELECT statements - heart of DBT project</li>
<li><strong>Tests:</strong> Data quality validations (not_null, unique, accepted_values, custom)</li>
<li><strong>Macros:</strong> Reusable Jinja templates for dynamic SQL</li>
<li><strong>Seeds:</strong> Static CSV files loaded as tables</li>
<li><strong>Snapshots:</strong> Capture historical changes (SCD Type 2)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q39">
<div class="question-title">Difference between UPSERT and MERGE</div>
<div class="answer">
<p><strong>UPSERT:</strong> A conceptual operation - UPDATE if exists, INSERT if not</p>
<p><strong>MERGE:</strong> Standard SQL statement (SQL:2003) that implements UPSERT</p>
<p><strong>MERGE in Snowflake:</strong></p>
<pre>MERGE INTO target_table AS T
USING source_data AS S
ON T.id = S.id
WHEN MATCHED AND T.last_updated &lt; S.last_updated THEN
    UPDATE SET T.name = S.name, T.email = S.email
WHEN NOT MATCHED THEN
    INSERT (id, name, email) VALUES (S.id, S.name, S.email);</pre>
<p><strong>Example:</strong> Update customer 1's email, insert new customer 3</p>
<pre>-- Before MERGE
customers: (1, 'Alice', 'alice@old.com'), (2, 'Bob', 'bob@example.com')
staging: (1, 'Alice', 'alice@new.com'), (3, 'Charlie', 'charlie@example.com')

-- After MERGE
customers: (1, 'Alice', 'alice@new.com'), (2, 'Bob', 'bob@example.com'), (3, 'Charlie', 'charlie@example.com')</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q40">
<div class="question-title">Data Masking and Masking Policy</div>
<div class="answer">
<p><strong>Data Masking:</strong> Hiding sensitive data with fictional data while maintaining usefulness for development/testing/analytics.</p>
<p><strong>Snowflake Masking Policy:</strong> Dynamic masking at query time based on user role. Underlying data is never modified.</p>
<p><strong>Example:</strong></p>
<pre>-- Create masking policy
CREATE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR -&gt;
    CASE
        WHEN CURRENT_ROLE() IN ('ANALYST_ROLE') THEN '****'
        WHEN CURRENT_ROLE() = 'DATA_STEWARD_ROLE' THEN val
        ELSE 'No access'
    END;

-- Apply to column
ALTER TABLE users ALTER COLUMN email SET MASKING POLICY email_mask;</pre>
<p><strong>Benefits:</strong> Dynamic, centralized, granular control, secure, no data duplication</p>
<p><strong>Common Techniques:</strong> Substitution, shuffling, redaction, tokenization, hashing, encryption</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q41">
<div class="question-title">Difference between Hashing and Encryption</div>
<div class="answer">
<p><strong>Hashing:</strong> One-way function creating fixed-size unique string</p>
<ul>
<li>Purpose: Integrity verification, password storage</li>
<li>One-way: Cannot reverse to get original</li>
<li>Deterministic: Same input always produces same hash</li>
<li>Examples: MD5 (32 chars), SHA256 (64 chars)</li>
</ul>
<p><strong>Encryption:</strong> Two-way function requiring key to encrypt/decrypt</p>
<ul>
<li>Purpose: Confidentiality, protecting sensitive data</li>
<li>Reversible: Decrypt with correct key to get original</li>
<li>Key-dependent: Requires encryption/decryption key</li>
<li>Examples: AES, RSA</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Hashing</th>
<th>Encryption</th>
</tr>
<tr>
<td>Reversibility</td>
<td>One-way (irreversible)</td>
<td>Two-way (reversible)</td>
</tr>
<tr>
<td>Purpose</td>
<td>Integrity, passwords</td>
<td>Confidentiality</td>
</tr>
<tr>
<td>Key Usage</td>
<td>No key needed</td>
<td>Requires key</td>
</tr>
<tr>
<td>Output Size</td>
<td>Fixed</td>
<td>Variable</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q42">
<div class="question-title">Snowflake storage integration with AWS S3</div>
<div class="answer">
<p><strong>Purpose:</strong> Secure, credential-less access to S3 from Snowflake</p>
<p><strong>Step-by-Step Setup:</strong></p>
<ol>
<li><strong>AWS IAM Policy:</strong> Grant Snowflake permissions (s3:GetObject, s3:PutObject, s3:ListBucket)</li>
<li><strong>AWS IAM Role:</strong> Create role, attach policy</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="4">
<li><strong>Get Snowflake's External ID &amp; IAM User:</strong>
<pre>DESCRIBE INTEGRATION s3_integration;</pre>
</li>
<li><strong>Update AWS IAM Role Trust Relationship:</strong> Add Snowflake's IAM user ARN and external ID</li>
<li><strong>Create External Stage:</strong></li>
</ol>
<pre>CREATE STAGE my_s3_stage
  STORAGE_INTEGRATION = s3_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<p><strong>Benefits:</strong> Enhanced security, centralized control, no exposed AWS keys</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q43">
<div class="question-title">What is DBT and what problem does it solve in modern data stack</div>
<div class="answer">
<p>DBT (data build tool) is a command-line framework for transforming data in your warehouse with modular SQL and software engineering best practices.</p>
<p><strong>Problems it Solves:</strong></p>
<ul>
<li><strong>Code Proliferation:</strong> Centralizes transformation logic instead of scattered ETL scripts/BI tools</li>
<li><strong>Lack of Best Practices:</strong> Brings version control, modularity, testing, documentation to data work</li>
<li><strong>Dependency Management:</strong> Automatically determines correct model execution order</li>
<li><strong>Testing &amp; Quality:</strong> Provides framework for data quality tests and auto-documentation</li>
<li><strong>Transparency:</strong> Makes entire transformation process transparent and auditable</li>
</ul>
<p><strong>Core Philosophy:</strong> Leverage the power of the data warehouse itself (pushing computations) rather than using separate processing engines</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q44">
<div class="question-title">Core components of a DBT project: Models, Tests, Seeds</div>
<div class="answer">
<p><strong>Models:</strong> SQL SELECT statements organized into transformation logic</p>
<ul>
<li>Each .sql file = one model</li>
<li>Output: view or table (depends on materialization)</li>
<li>Example: Model calculating monthly sales from raw transactions</li>
</ul>
<p><strong>Tests:</strong> Data quality checks ensuring data integrity</p>
<ul>
<li><strong>Singular Tests:</strong> Custom SQL returning failing rows if condition not met</li>
<li><strong>Generic Tests:</strong> Pre-defined tests applied via YAML (not_null, unique, accepted_values)</li>
<li>Purpose: Prevent downstream errors, ensure data reliability</li>
</ul>
<p><strong>Seeds:</strong> Static CSV files loaded as reference/lookup tables</p>
<ul>
<li>Small, infrequently-changing data (country codes, mapping tables)</li>
<li>Loaded via dbt seed command</li>
</ul>
<p><strong>Macros (Bonus):</strong> Reusable Jinja+SQL code snippets (like functions)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q45">
<div class="question-title">Materializations in DBT - Four default types</div>
<div class="answer">
<p><strong>View (Default):</strong></p>
<ul>
<li>Mechanism: Creates SQL view</li>
<li>Pros: Always latest data, fast compile</li>
<li>Cons: Slow queries (logic runs every time)</li>
<li>Best for: Simple transformations, rarely-queried models</li>
</ul>
<p><strong>Table:</strong></p>
<ul>
<li>Mechanism: Creates persistent table with CREATE TABLE AS</li>
<li>Pros: Fast queries (pre-calculated)</li>
<li>Cons: Slow to build, consumes storage</li>
<li>Best for: Complex/frequently-queried models, large datasets</li>
</ul>
<p><strong>Incremental:</strong></p>
<ul>
<li>Mechanism: INSERT/MERGE only new/changed records</li>
<li>Pros: Very fast after initial build</li>
<li>Cons: Complex design, prone to errors</li>
<li>Best for: Large fact tables with incremental changes</li>
</ul>
<p><strong>Ephemeral:</strong></p>
<ul>
<li>Mechanism: No physical object, compiles to CTE</li>
<li>Pros: Excellent modularity</li>
<li>Cons: Not queryable directly</li>
<li>Best for: Simple intermediate cleaning steps</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q46">
<div class="question-title">Managing dependencies and referencing models in DBT</div>
<div class="answer">
<p><strong>The ref() Function:</strong> Reference models without hardcoding schema names</p>
<pre>-- Instead of FROM SCHEMA.TABLE
SELECT * FROM {{ ref('stg_orders') }}
WHERE is_valid = TRUE</pre>
<p><strong>How it Works:</strong></p>
<ul>
<li>{{ ref('stg_orders') }} replaced with correct schema/table name during compilation</li>
<li>DBT automatically knows stg_orders must be built before this model</li>
<li>Creates dependency edge in DAG</li>
</ul>
<p><strong>Automatic DAG Generation:</strong></p>
<ul>
<li>DBT scans all models and their ref() calls</li>
<li>Builds DAG showing ALL dependencies</li>
<li>dbt run executes models in correct order, ensuring source tables exist before use</li>
</ul>
<p><strong>Benefits:</strong> No manual ordering, automatic parallelization, clear dependencies</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q47">
<div class="question-title">What is Jinja in DBT - Simple example</div>
<div class="answer">
<p>Jinja is a templating language that adds programming logic to SQL. Processed before SQL reaches the warehouse.</p>
<p><strong>Capabilities:</strong> if/else statements, variables, loops, macro reuse, dynamic SQL</p>
<p><strong>Simple Example:</strong></p>
<pre>{% set limit_date = '2023-01-01' %}

SELECT * FROM {{ ref('raw_data') }}
WHERE created_at &gt;= '{{ limit_date }}'

{% if target.name == 'prod' %}
  AND is_active = TRUE
{% endif %}</pre>
<p><strong>Compiled SQL (Development):</strong></p>
<pre>SELECT * FROM my_dev_schema.raw_data
WHERE created_at &gt;= '2023-01-01'</pre>
<p><strong>Compiled SQL (Production):</strong></p>
<pre>SELECT * FROM my_prod_schema.raw_data
WHERE created_at &gt;= '2023-01-01'
  AND is_active = TRUE</pre>
<p><strong>Benefits:</strong> DRY (Don't Repeat Yourself), environment-specific logic, dynamic SQL generation</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q48">
<div class="question-title">Data Governance best practices</div>
<div class="answer">
<p><strong>Key Best Practices:</strong></p>
<ol>
<li><strong>Define Clear Ownership:</strong> Assign data owners and stewards for each domain/dataset</li>
<li><strong>Establish Data Dictionary:</strong> Document all data elements, definitions, relationships</li>
<li><strong>Implement Metadata Management:</strong> Track technical metadata (schema, lineage) and business metadata</li>
<li><strong>Quality Standards:</strong> Implement data quality rules, monitoring, and remediation processes</li>
<li><strong>Access Control:</strong> RBAC, row-level security, column-level masking</li>
<li><strong>Data Lineage:</strong> Understand data flow from source to consumption</li>
<li><strong>Compliance &amp; Auditing:</strong> Track data usage, audit logs, regulatory compliance (GDPR, HIPAA)</li>
<li><strong>Version Control:</strong> Version all code (SQL, DBT, Python) in Git</li>
<li><strong>Documentation:</strong> Auto-generate and maintain docs (DBT, Collibra, Ataccama)</li>
<li><strong>Collaboration:</strong> Foster data culture, communication between teams</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q49">
<div class="question-title">Data quality testing strategies</div>
<div class="answer">
<p><strong>Common Testing Approaches:</strong></p>
<ol>
<li><strong>Schema Tests:</strong> Validate data types, constraints
                        <ul>
<li><strong>not_null:</strong> Column should not have NULL values</li>
<li><strong>unique:</strong> Column values must be distinct</li>
<li><strong>relationships:</strong> Foreign key relationships</li>
<li><strong>accepted_values:</strong> Column values from allowed set</li>
</ul>
</li>
<li><strong>Statistical Tests:</strong> Validate data distributions, outliers
                        <ul>
<li>Range checks</li>
<li>Distribution analysis</li>
<li>Outlier detection</li>
</ul>
</li>
<li><strong>Business Logic Tests:</strong> Validate business rules
                        <ul>
<li>Cross-table consistency</li>
<li>Aggregation accuracy</li>
<li>Recency checks</li>
</ul>
</li>
<li><strong>Freshness Tests:</strong> Ensure data is current
                        <ul>
<li>Last update timestamp</li>
<li>Row count expectations</li>
</ul>
</li>
</ol>
<p><strong>Tools:</strong> DBT tests, Great Expectations, Ataccama, custom SQL checks</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q50">
<div class="question-title">Building a modern data stack - Technology selection</div>
<div class="answer">
<p><strong>Typical Modern Data Stack Layers:</strong></p>
<ul>
<li><strong>Source Systems:</strong> Operational databases, SaaS applications, APIs</li>
<li><strong>Ingestion:</strong> Fivetran, Stitch, Airbyte, AWS DMS, Qlik Replicate</li>
<li><strong>Cloud Data Warehouse:</strong> Snowflake, BigQuery, Redshift</li>
<li><strong>Transformation:</strong> DBT, Spark, Python</li>
<li><strong>Data Governance:</strong> Collibra, Ataccama, DataHub</li>
<li><strong>Analytics/BI:</strong> Tableau, Power BI, Looker</li>
<li><strong>Orchestration:</strong> Airflow, Prefect, Dagster</li>
<li><strong>Reverse ETL:</strong> Hightouch, Census (sync data back to source systems)</li>
</ul>
<p><strong>Selection Criteria:</strong></p>
<ul>
<li>Scalability and performance</li>
<li>Cost efficiency</li>
<li>Ease of use / learning curve</li>
<li>Integration with existing tools</li>
<li>Community and support</li>
<li>Security and compliance requirements</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q51">
<div class="question-title">Future trends in data engineering</div>
<div class="answer">
<p><strong>Emerging Trends:</strong></p>
<ul>
<li><strong>DataOps:</strong> Applying DevOps principles to data pipelines (CI/CD, monitoring, automation)</li>
<li><strong>Real-time Analytics:</strong> Move beyond batch to streaming/real-time data pipelines</li>
<li><strong>AI/ML Integration:</strong> ML models integrated into data pipelines, automated feature engineering</li>
<li><strong>Data Mesh:</strong> Decentralized data ownership model, domain-oriented data architecture</li>
<li><strong>Lakehouse Architecture:</strong> Combining data lake flexibility with warehouse performance (Delta Lake, Iceberg, Hudi)</li>
<li><strong>Cloud-Native:</strong> Serverless data platforms, compute separation from storage</li>
<li><strong>Data Quality as First-Class:</strong> Increased focus on data quality, governance, and observability</li>
<li><strong>Self-Service Analytics:</strong> Tools enabling business users to perform analysis without technical expertise</li>
<li><strong>Reverse ETL:</strong> Syncing aggregated/enriched data back to operational systems</li>
<li><strong>Privacy-Preserving Technologies:</strong> Differential privacy, federated learning for sensitive data</li>
</ul>
<p><strong>Skills for Future:</strong> Cloud platforms (AWS/Azure/GCP), DBT, SQL, Python, Spark, containers, Kubernetes, ML basics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<!-- Appended Snowflake + dbt Interview Q&A -->
<div class="question" id="q52">
<div class="question-title">Explain your end-to-end data pipeline (tools + flow)</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>In a real-world setup, my end-to-end data pipeline looks like this:</p>
<p>&gt; <strong>Source systems â†’ Ingestion â†’ Cloud storage â†’ Data warehouse â†’ Transformations â†’ Analytics</strong></p>
<p><strong>Typical stack Iâ€™ve worked with:</strong></p>
<ul>
<li><strong>Sources:</strong> Oracle, PostgreSQL, Salesforce, REST APIs</li>
<li><strong>Ingestion:</strong> Qlik Replicate / Fivetran / Airbyte</li>
<li><strong>Landing zone:</strong> AWS S3 (raw, immutable data)</li>
<li><strong>Warehouse:</strong> Snowflake</li>
<li><strong>Transformations:</strong> dbt (staging â†’ intermediate â†’ marts)</li>
<li><strong>Orchestration:</strong> Airflow</li>
<li><strong>Monitoring:</strong> Airflow alerts + dbt tests</li>
<li><strong>Consumption:</strong> Power BI / Tableau</li>
</ul>
<p><strong>Key design principles:</strong></p>
<ul>
<li>Raw data is <strong>append-only</strong> and never modified</li>
<li>All transformations happen <strong>inside Snowflake</strong></li>
<li><strong>dbt</strong> owns business logic and data modeling</li>
<li><strong>Airflow</strong> manages scheduling, dependencies, and retries</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q53">
<div class="question-title">How do you use Python inside Airflow for orchestration and failure alerts?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>Airflow is used strictly for <strong>orchestration</strong>, not transformations.</p>
<p><strong>I use Python in Airflow</strong> mainly to:</p>
<ul>
<li>Define DAGs and task dependencies</li>
<li>Validate upstream conditions (file arrival, row counts)</li>
<li>Trigger dbt runs</li>
<li>Handle retries, SLAs, and alerts</li>
</ul>
<p><strong>Typical usage:</strong></p>
<ul>
<li><code>PythonOperator</code> for file availability checks and pre/post DQ checks</li>
<li><code>BashOperator</code> to trigger dbt commands</li>
<li><code>on_failure_callback</code> to send Slack/Email with DAG, task, exec date, and error</li>
</ul>
<p><strong>Failure handling approach:</strong></p>
<ul>
<li>Configured retries with exponential backoff</li>
<li>SLA monitoring for long-running tasks</li>
<li>Alerts triggered only after retries are exhausted</li>
</ul>
<p><em>Interview-ready line:</em></p>
<p><strong>â€œAirflow controls <em>when</em> and <em>in what order</em> things run, while dbt controls <em>how</em> the data is transformed.â€</strong></p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q54">
<div class="question-title">How do you implement SCD Type 2 in dbt?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>I implement SCD Type 2 in dbt using <strong>dbt snapshots</strong>, which is the recommended and scalable approach.</p>
<p><strong>Example snapshot:</strong></p>
<pre>{% raw %}{% snapshot dim_customer_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='customer_id',
    strategy='timestamp',
    updated_at='updated_at'
  )
}}

select * from {{ ref('stg_customers') }}

{% endsnapshot %}{% endraw %}</pre>
<p><strong>What dbt handles automatically:</strong></p>
<ul>
<li>Adds <code>dbt_valid_from</code> and <code>dbt_valid_to</code> columns</li>
<li>Inserts a new row when tracked columns change</li>
<li>Closes the old record by updating <code>dbt_valid_to</code></li>
</ul>
<p><strong>Common use cases:</strong> Customer attribute history, product price changes, account/subscription status tracking</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q55">
<div class="question-title">How does dbt snapshot handle deletes?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>By default, <strong>dbt snapshots do not detect deletes</strong>.</p>
<p><strong>Default behavior:</strong></p>
<ul>
<li>If a record disappears from the source table, the snapshot keeps the last version open (<code>dbt_valid_to</code> remains NULL)</li>
</ul>
<p><strong>To handle deletes properly:</strong></p>
<pre>invalidate_hard_deletes=true</pre>
<p><strong>Result:</strong></p>
<ul>
<li>When a source row is deleted, <code>dbt_valid_to</code> is populated and the record is marked as inactive</li>
</ul>
<p><em>Interview-ready line:</em></p>
<p><strong>â€œdbt snapshots donâ€™t track deletes by default; hard deletes must be explicitly enabled.â€</strong></p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q56">
<div class="question-title">When and why do you use dbt seeds?</div>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>I use <strong>dbt seeds</strong> for <strong>small, static reference data</strong> that rarely changes and should live alongside code.</p>
<p><strong>Typical examples:</strong></p>
<ul>
<li>Country or currency codes</li>
<li>Status mappings</li>
<li>Business rule lookup tables</li>
<li>SLA thresholds</li>
</ul>
<p><strong>Why seeds are useful:</strong></p>
<ul>
<li>Version-controlled using Git</li>
<li>Easy to deploy with dbt</li>
<li>No dependency on upstream systems</li>
</ul>
<p><strong>When I avoid seeds:</strong></p>
<ul>
<li>Large datasets</li>
<li>Frequently changing or transactional data</li>
</ul>
<p><em>One-liner for interviews:</em></p>
<p><strong>â€œdbt seeds are ideal for small, stable reference data that belongs with transformation logic.â€</strong></p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q57">
<div class="question-title">SQL Challenge â€“ Orders and Payments</div>
<div class="answer">
<p><strong>Problem:</strong></p>
<ul>
<li>Orders table</li>
<li>Payments table</li>
<li>Partial payments allowed</li>
<li>Identify orders <strong>not fully paid within 3 days of order date</strong></li>
</ul>
<p><strong>Solution (Snowflake SQL):</strong></p>
<pre>WITH payments_3_days AS (
    SELECT
        o.order_id,
        o.order_amount,
        SUM(p.payment_amount) AS paid_amount
    FROM orders o
    LEFT JOIN payments p
        ON o.order_id = p.order_id
       AND p.payment_date &lt;= o.order_date + INTERVAL '3 DAY'
    GROUP BY o.order_id, o.order_amount
)
SELECT *
FROM payments_3_days
WHERE COALESCE(paid_amount, 0) &lt; order_amount;</pre>
<p><strong>Why this solution works:</strong></p>
<ul>
<li>Handles partial payments correctly</li>
<li>Handles orders with no payments</li>
<li>Encodes business logic, not just joins</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q58">
<div class="question-title">dbt Materializations â€“ Syntax &amp; When to Use</div>
<div class="answer">
<p>Materialization defines <strong>how dbt builds a model in the warehouse</strong>.</p>
<p><strong>1. View (Default)</strong></p>
<pre>{{ config(materialized='view') }}

select *
from {{ ref('stg_orders') }}</pre>
<ul>
<li><strong>Purpose:</strong> Creates a Snowflake VIEW. No data stored, always reflects latest source data.</li>
<li><strong>When to use:</strong> Staging models, light transformations, fast iteration</li>
<li><strong>Interview line:</strong> "Views are best for staging where we want zero storage and always-fresh data."</li>
</ul>
<p><strong>2. Table</strong></p>
<pre>{{ config(materialized='table') }}

select *
from {{ ref('int_sales') }}</pre>
<ul>
<li><strong>Purpose:</strong> Creates a physical table. Rebuilt completely on every run.</li>
<li><strong>When to use:</strong> Small dimension tables, final marts with low data volume</li>
<li><strong>Trade-off:</strong> Expensive for large datasets</li>
</ul>
<p><strong>3. Incremental (MOST IMPORTANT)</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}

{% if is_incremental() %}
  where updated_at &gt; (select max(updated_at) from {{ this }})
{% endif %}</pre>
<ul>
<li><strong>Purpose:</strong> Loads only new or changed data. Saves cost and time.</li>
<li><strong>When to use:</strong> Fact tables, large datasets, streaming/CDC data</li>
<li><strong>Key points:</strong> <code>is_incremental()</code> runs only after first load; <code>unique_key</code> enables merge</li>
<li><strong>Interview line:</strong> "Incremental models are mandatory for large fact tables in production."</li>
</ul>
<p><strong>4. Ephemeral</strong></p>
<pre>{{ config(materialized='ephemeral') }}

select
    order_id,
    sum(amount) as total_amount
from {{ ref('stg_payments') }}
group by order_id</pre>
<ul>
<li><strong>Purpose:</strong> No table, no view. SQL is inlined into downstream models.</li>
<li><strong>When to use:</strong> Reusable logic, intermediate calculations</li>
<li><strong>Interview line:</strong> "Ephemeral models are like SQL macros with structure."</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q59">
<div class="question-title">dbt Snapshots â€“ Full Syntax &amp; Purpose (SCD Type 2)</div>
<div class="answer">
<p>Snapshots are used for <strong>SCD Type 2</strong> â€“ tracking changes over time.</p>
<p><strong>Timestamp Strategy (When source has reliable updated_at)</strong></p>
<pre>{% snapshot dim_customer_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='customer_id',
    strategy='timestamp',
    updated_at='updated_at',
    invalidate_hard_deletes=true
  )
}}

select *
from {{ ref('stg_customers') }}

{% endsnapshot %}</pre>
<p><strong>What dbt adds automatically:</strong></p>
<ul>
<li><code>dbt_valid_from</code> â€“ When the record became active</li>
<li><code>dbt_valid_to</code> â€“ When the record expired (NULL if current)</li>
<li>History tracking with row versions</li>
</ul>
<p><strong>Check Strategy (When no reliable timestamp)</strong></p>
<pre>{% snapshot dim_product_snapshot %}
{{
  config(
    target_schema='snapshots',
    unique_key='product_id',
    strategy='check',
    check_cols=['price', 'status']
  )
}}

select *
from {{ ref('stg_products') }}

{% endsnapshot %}</pre>
<p><strong>Use when:</strong> No reliable timestamp; track changes in specific columns</p>
<p><strong>Key Configuration Options:</strong></p>
<ul>
<li><code>invalidate_hard_deletes=true</code> â€“ Closes records when source row is deleted</li>
<li><code>target_schema</code> â€“ Where snapshots are stored</li>
<li><code>unique_key</code> â€“ Column(s) identifying a record</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q60">
<div class="question-title">dbt YAML Files â€“ Syntax, Indentation &amp; Purpose</div>
<div class="answer">
<p>YAML defines <strong>metadata and tests</strong>, not SQL logic.</p>
<p><strong>schema.yml (Most Important YAML File)</strong></p>
<pre>version: 2

models:
  - name: stg_orders
    description: "Cleaned orders data"
    columns:
      - name: order_id
        description: "Primary key"
        tests:
          - not_null
          - unique
      - name: status
        tests:
          - accepted_values:
              values: ['pending', 'completed', 'cancelled']</pre>
<p><strong>Indentation Rules (CRITICAL â€“ YAML breaks with wrong spacing)</strong></p>
<table>
<tr>
<th>Level</th>
<th>Indentation</th>
<th>Meaning</th>
</tr>
<tr>
<td>models:</td>
<td>0 spaces</td>
<td>Top-level key</td>
</tr>
<tr>
<td>- name:</td>
<td>2 spaces</td>
<td>Model item</td>
</tr>
<tr>
<td>columns:</td>
<td>4 spaces</td>
<td>Column metadata</td>
</tr>
<tr>
<td>tests:</td>
<td>6 spaces</td>
<td>Data quality tests</td>
</tr>
</table>
<p><strong>Interview Tip:</strong> "YAML breaks if indentation is wrong. Two spaces only â€“ no tabs."</p>
<p><strong>Relationships Test (Foreign Key Validation)</strong></p>
<pre>- name: order_id
    description: "Foreign key to customer"
    tests:
      - relationships:
          to: ref('dim_customer')
          field: customer_id</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q61">
<div class="question-title">Source YAML, Snapshot YAML, and Seeds YAML</div>
<div class="answer">
<p><strong>Source YAML (sources.yml)</strong></p>
<pre>version: 2

sources:
  - name: sales
    database: raw
    schema: public
    tables:
      - name: orders
        description: "Raw orders table"
        columns:
          - name: order_id
            tests:
              - not_null</pre>
<ul>
<li><strong>Purpose:</strong> Define raw data sources, source freshness, source-level testing, lineage tracking</li>
<li><strong>Usage in models:</strong> <code>{{ source('sales', 'orders') }}</code></li>
</ul>
<p><strong>Snapshot YAML (snapshots.yml)</strong></p>
<pre>version: 2

snapshots:
  - name: dim_customer_snapshot
    description: "Customer SCD Type 2 history"
    columns:
      - name: customer_id
        tests:
          - not_null</pre>
<p><strong>Seeds YAML (seeds.yml)</strong></p>
<pre>version: 2

seeds:
  - name: country_codes
    description: "ISO country codes lookup table"
    columns:
      - name: country_code
        description: "2-letter ISO code"
        tests:
          - not_null
          - unique</pre>
<ul>
<li><strong>Purpose:</strong> Version-controlled static reference data (CSV files)</li>
<li><strong>To load:</strong> <code>dbt seed</code></li>
<li><strong>Usage in models:</strong> <code>{{ ref('country_codes') }}</code></li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q62">
<div class="question-title">Tests in YAML â€“ Built-in &amp; Custom Tests</div>
<div class="answer">
<p><strong>Built-in Generic Tests</strong></p>
<pre>tests:
  - not_null  # Column must not be NULL
  - unique    # Column values must be distinct
  - accepted_values:
      values: ['A', 'I', 'U']  # Only these values allowed</pre>
<p><strong>Relationships Test (Foreign Key Validation)</strong></p>
<pre>- name: customer_id
    description: "Foreign key to dim_customer"
    tests:
      - relationships:
          to: ref('dim_customer')
          field: customer_id</pre>
<ul>
<li><strong>Purpose:</strong> Ensures referential integrity across tables</li>
<li><strong>When to use:</strong> Foreign key validation, cross-table consistency</li>
</ul>
<p><strong>Custom Singular Tests (SQL-based)</strong></p>
<p>Create a file: <code>tests/assert_no_negative_amounts.sql</code></p>
<pre>-- Should return zero rows for test to pass
select * 
from {{ ref('fact_orders') }}
where amount &lt; 0</pre>
<p><strong>Why custom tests:</strong> Business logic checks, complex validations (e.g., "order count should not drop &gt; 10% daily")</p>
<p><strong>Run tests:</strong> <code>dbt test</code></p>
<ul>
<li>Executes all YAML tests and singular tests</li>
<li>Fails build if any test fails (default behavior)</li>
<li>With flag: <code>dbt test --select stg_orders</code> (test specific model)</li>
</ul>
<p><strong>Interview Line:</strong> "Tests are the backbone of data quality in dbt â€“ they run after every build and catch issues early."</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q63">
<div class="question-title">dbt_project.yml Configuration &amp; Interview Power Lines</div>
<div class="answer">
<p><strong>dbt_project.yml â€“ Enforce Materialization Standards</strong></p>
<pre>name: 'my_analytics_project'
version: '1.0'
config-version: 2

models:
  my_analytics_project:
    staging:
      +materialized: view        # All staging models are views
    intermediate:
      +materialized: view
    marts:
      +materialized: table       # All mart models are tables
      
    # Fact tables override to incremental
    marts_facts:
      +materialized: incremental
      +unique_key: order_id</pre>
<p><strong>Why This Matters:</strong></p>
<ul>
<li>Enforces organizational standards without per-model config</li>
<li>New developers don't need to decide materialization</li>
<li>Prevents expensive mistakes (e.g., 500GB table as view)</li>
</ul>
<p><strong>Quick Interview Cheat Table</strong></p>
<table>
<tr>
<th>Feature</th>
<th>Used For</th>
<th>Output</th>
</tr>
<tr>
<td>view</td>
<td>Staging</td>
<td>SQL SELECT</td>
</tr>
<tr>
<td>table</td>
<td>Small dimensions</td>
<td>Physical table</td>
</tr>
<tr>
<td>incremental</td>
<td>Large facts</td>
<td>Fast updates</td>
</tr>
<tr>
<td>ephemeral</td>
<td>Reusable logic</td>
<td>Inlined CTE</td>
</tr>
<tr>
<td>snapshot</td>
<td>SCD Type 2</td>
<td>History table</td>
</tr>
<tr>
<td>schema.yml</td>
<td>Tests &amp; docs</td>
<td>Metadata</td>
</tr>
<tr>
<td>sources.yml</td>
<td>Source metadata</td>
<td>Data lineage</td>
</tr>
<tr>
<td>seeds</td>
<td>Static reference data</td>
<td>CSV â†’ Table</td>
</tr>
</table>
<p><strong>Interview Power Lines (Use These Under Pressure)</strong></p>
<ul>
<li><strong>"dbt handles transformations, Airflow handles orchestration."</strong></li>
<li><strong>"Incremental models are critical for cost control in Snowflake."</strong></li>
<li><strong>"Snapshots are the cleanest way to implement SCD Type 2."</strong></li>
<li><strong>"YAML defines <em>what should be true</em>, SQL defines <em>how data is built</em>."</strong></li>
<li><strong>"Views for staging keep iterative work fast; tables for marts ensure performance."</strong></li>
<li><strong>"Tests catch data quality issues before they reach BI tools or dashboards."</strong></li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q64">
<div class="question-title">Advanced Incremental Strategies â€“ MERGE vs APPEND</div>
<div class="answer">
<p>Incremental â‰  one thing. There are <strong>multiple patterns</strong>, and choosing the wrong one causes <strong>duplicates, data loss, or high cost</strong>.</p>
<p><strong>Strategy 1: MERGE (Upsert) â€“ Most Common</strong></p>
<p><strong>When to use:</strong> CDC data, updates are possible, late-arriving data, deduplication required</p>
<p><strong>How it works:</strong> Uses MERGE INTO, updates existing rows, inserts new rows</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}

select *
from {{ ref('stg_orders') }}</pre>
<p><strong>What dbt generates:</strong></p>
<pre>MERGE INTO target t
USING source s
ON t.order_id = s.order_id
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT</pre>
<p><strong>Interview explanation:</strong> "MERGE is used when data can change after ingestion. It guarantees idempotency and correctness."</p>
<p><strong>Strategy 2: APPEND-ONLY Incremental</strong></p>
<p><strong>When to use:</strong> Event data, immutable logs, streaming data, no updates ever</p>
<pre>{{ config(
    materialized='incremental'
) }}

select *
from {{ ref('stg_events') }}

{% if is_incremental() %}
  where event_date &gt; (select max(event_date) from {{ this }})
{% endif %}</pre>
<p><strong>Key difference:</strong> No <code>unique_key</code>, dbt does INSERT ONLY</p>
<p><strong>Interview explanation:</strong> "Append strategy is faster and cheaper but only safe when data is immutable."</p>
<p><strong>Strategy 3: Micro-Batch Incremental (Advanced)</strong></p>
<p><strong>Problem it solves:</strong> Late-arriving data, backfills, partial refresh without full rebuild</p>
<pre>{% if is_incremental() %}
  where updated_at &gt;= dateadd(day, -3, current_date)
{% endif %}</pre>
<p><strong>Use case:</strong> Refresh last N days every run</p>
<p><strong>Interview line:</strong> "Micro-batching balances correctness and cost by reprocessing a sliding window."</p>
<p><strong>Strategy 4: Delete + Insert (Hard Reset per Key)</strong></p>
<p><strong>When to use:</strong> Source sends full replacement per key, no reliable update timestamp</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='customer_id',
    incremental_strategy='delete+insert'
) }}</pre>
<p><strong>Interview explanation:</strong> "delete+insert is safer when updates are complex and partial merges aren't reliable."</p>
<p><strong>Incremental Strategy Decision Table</strong></p>
<table>
<tr>
<th>Scenario</th>
<th>Strategy</th>
</tr>
<tr>
<td>CDC / updates</td>
<td>MERGE</td>
</tr>
<tr>
<td>Logs / events</td>
<td>APPEND</td>
</tr>
<tr>
<td>Late data</td>
<td>Micro-batch</td>
</tr>
<tr>
<td>Full row replace</td>
<td>delete+insert</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q65">
<div class="question-title">Custom dbt Tests â€“ Beyond Built-ins</div>
<div class="answer">
<p>Built-in tests are <strong>not enough</strong> in real projects. Custom tests validate business logic.</p>
<p><strong>Generic Test (Reusable)</strong></p>
<p><strong>Example:</strong> Amount must be positive</p>
<p><strong>File:</strong> <code>tests/assert_positive_amount.sql</code></p>
<pre>select *
from {{ model }}
where {{ column_name }} &lt;= 0</pre>
<p><strong>YAML usage:</strong></p>
<pre>columns:
  - name: amount
    tests:
      - assert_positive_amount</pre>
<p><strong>Interview explanation:</strong> "Generic tests are reusable across models and columns."</p>
<p><strong>Singular Test (Business Rule Test)</strong></p>
<p><strong>Example:</strong> Orders should have payments</p>
<p><strong>File:</strong> <code>tests/orders_without_payments.sql</code></p>
<pre>select o.order_id
from {{ ref('fct_orders') }} o
left join {{ ref('fct_payments') }} p
  on o.order_id = p.order_id
where p.order_id is null</pre>
<p><strong>Interview explanation:</strong> "Singular tests validate complex business assumptions, not just column constraints."</p>
<p><strong>Severity Levels (VERY IMPORTANT)</strong></p>
<pre>tests:
  - not_null:
      severity: warn</pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li><code>error</code> â†’ pipeline fails</li>
<li><code>warn</code> â†’ alert but continue</li>
</ul>
<p><strong>Interview line:</strong> "Not all data issues should break pipelines; severity controls blast radius."</p>
<p><strong>Snapshot-Specific Tests</strong></p>
<pre>tests:
  - dbt_utils.expression_is_true:
      expression: "dbt_valid_to &gt; dbt_valid_from"</pre>
<p><strong>Key Takeaway:</strong> Custom tests catch production bugs early. Integration tests are the last line of defense.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q66">
<div class="question-title">Enterprise-Grade dbt Folder Structure</div>
<div class="answer">
<p>Bad structure = unmaintainable project. This is <strong>conversion-critical</strong>.</p>
<p><strong>Recommended Enterprise Structure</strong></p>
<pre>dbt_project/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ sales/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_payments.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ int_order_payments.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ finance/
â”‚   â”‚   â”‚   â”œâ”€â”€ fct_revenue.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customer.sql
â”‚   â”‚   â”‚   â””â”€â”€ schema.yml
â”‚
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ dim_customer_snapshot.sql
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ assert_positive_amount.sql
â”‚
â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ country_codes.csv
â”‚
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ generate_surrogate_key.sql
â”‚
â”œâ”€â”€ dbt_project.yml</pre>
<p><strong>Layer Responsibilities (Interview Gold)</strong></p>
<p><strong>Staging (stg_)</strong></p>
<ul>
<li>1-to-1 with source</li>
<li>Rename columns</li>
<li>Type casting</li>
<li>No joins</li>
</ul>
<p>"Staging models are clean mirrors of source data."</p>
<p><strong>Intermediate (int_)</strong></p>
<ul>
<li>Joins</li>
<li>Deduplication</li>
<li>Business prep logic</li>
</ul>
<p>"Intermediate models simplify downstream marts."</p>
<p><strong>Marts (fct_, dim_)</strong></p>
<ul>
<li>Business-ready</li>
<li>KPI definitions</li>
<li>Analytics layer</li>
</ul>
<p>"Marts are the contract with BI and consumers."</p>
<p><strong>Enforcing Standards via dbt_project.yml</strong></p>
<pre>models:
  my_project:
    staging:
      +materialized: view
    intermediate:
      +materialized: ephemeral
    marts:
      +materialized: incremental</pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Prevents mistakes</li>
<li>Enforces architecture</li>
<li>Scales across teams</li>
</ul>
<p><strong>Final Interview Power Statements</strong></p>
<ul>
<li>"Incremental strategy selection depends on data mutability."</li>
<li>"MERGE ensures idempotency in CDC pipelines."</li>
<li>"Custom tests validate business truth, not just schema."</li>
<li>"Folder structure enforces responsibility boundaries."</li>
<li>"dbt scales through convention, not configuration."</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q67">
<div class="question-title">Production dbt Failures &amp; Root Causes (What Seniors Face)</div>
<div class="answer">
<p><strong>These scenarios separate junior users from senior owners. Interviewers love failure stories.</strong></p>
<p><strong>Failure 1: Incremental Model Creating Duplicates</strong></p>
<p><strong>Root Cause:</strong> Missing or incorrect <code>unique_key</code>; append strategy used where updates exist</p>
<p><strong>Symptom:</strong> Row counts grow unexpectedly, KPI inflation, duplicate primary keys</p>
<p><strong>Bad Pattern:</strong></p>
<pre>{{ config(materialized='incremental') }}
select * from source</pre>
<p><strong>Fix:</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id'
) }}</pre>
<p><strong>Interview Explanation:</strong> "If updates are possible, append-only incrementals are dangerous. MERGE is mandatory."</p>
<p><strong>Failure 2: Late-Arriving Data Missing in Incremental Loads</strong></p>
<p><strong>Root Cause:</strong> Filter based only on <code>max(updated_at)</code>; source sends delayed records</p>
<p><strong>Symptom:</strong> Historical gaps, inconsistent reports</p>
<p><strong>Bad Pattern:</strong></p>
<pre>where updated_at &gt; (select max(updated_at) from {{ this }})</pre>
<p><strong>Fix - Micro-Batching:</strong></p>
<pre>{% if is_incremental() %}
  where updated_at &gt;= dateadd(day, -3, current_date)
{% endif %}</pre>
<p><strong>Interview Line:</strong> "We reprocess a rolling window to handle late-arriving data safely."</p>
<p><strong>Failure 3: dbt Snapshot Growing Infinitely</strong></p>
<p><strong>Root Cause:</strong> Using <code>check</code> strategy on volatile columns (timestamps, operational noise)</p>
<p><strong>Symptom:</strong> Snapshot table explodes, performance degradation</p>
<p><strong>Fix:</strong> Track <strong>only business-relevant columns</strong></p>
<pre>check_cols=['status', 'tier']</pre>
<p><strong>Interview Line:</strong> "Snapshots should track business change, not operational noise."</p>
<p><strong>Failure 4: Pipelines Failing Due to Non-Critical Data Issues</strong></p>
<p><strong>Root Cause:</strong> All tests set to <code>severity: error</code></p>
<p><strong>Symptom:</strong> Frequent production failures, alert fatigue</p>
<p><strong>Fix:</strong></p>
<pre>tests:
  - not_null:
      severity: warn</pre>
<p><strong>Interview Line:</strong> "Not all data quality issues deserve to break pipelines."</p>
<p><strong>Failure 5: dbt Models Rebuilt Accidentally in Production</strong></p>
<p><strong>Root Cause:</strong> Using <code>table</code> materialization for large datasets</p>
<p><strong>Symptom:</strong> Snowflake cost spike, long downtimes</p>
<p><strong>Fix:</strong> Use <code>incremental</code> and enforce via <code>dbt_project.yml</code></p>
<pre>marts:
  +materialized: incremental</pre>
<p><strong>Failure 6: Broken Downstream Models Due to Schema Changes</strong></p>
<p><strong>Root Cause:</strong> Source column renamed/dropped; no contract enforcement</p>
<p><strong>Fix:</strong> Source tests + freshness checks; staging layer isolation</p>
<p><strong>Interview Line:</strong> "Staging models act as shock absorbers for upstream changes."</p>
<p><strong>Key Takeaway:</strong> Most production issues stem from wrong materialization choice or missing constraints. Prevention is cheaper than firefighting.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q68">
<div class="question-title">Senior-Level dbt Cheat Sheet &amp; Interview Power Lines</div>
<div class="answer">
<p><strong>Memorize this table before every interview.</strong></p>
<p><strong>Materialization Selection Grid</strong></p>
<table>
<tr>
<th>Type</th>
<th>Use Case</th>
<th>Risk</th>
</tr>
<tr>
<td>view</td>
<td>Staging</td>
<td>None (zero storage)</td>
</tr>
<tr>
<td>table</td>
<td>Small dimensions</td>
<td>Full rebuild cost</td>
</tr>
<tr>
<td>incremental</td>
<td>Large facts</td>
<td>Wrong strategy = data loss</td>
</tr>
<tr>
<td>ephemeral</td>
<td>Reusable logic</td>
<td>Harder debugging</td>
</tr>
<tr>
<td>snapshot</td>
<td>SCD Type 2</td>
<td>Size explosion if misused</td>
</tr>
</table>
<p><strong>Incremental Strategy Selection</strong></p>
<table>
<tr>
<th>Data Pattern</th>
<th>Strategy</th>
</tr>
<tr>
<td>Immutable events</td>
<td>Append</td>
</tr>
<tr>
<td>Updates possible</td>
<td>Merge</td>
</tr>
<tr>
<td>Late data</td>
<td>Micro-batch</td>
</tr>
<tr>
<td>Full row replacement</td>
<td>delete+insert</td>
</tr>
</table>
<p><strong>Snapshot Strategy Decision</strong></p>
<table>
<tr>
<th>Situation</th>
<th>Strategy</th>
</tr>
<tr>
<td>Reliable updated_at</td>
<td>timestamp</td>
</tr>
<tr>
<td>No timestamp</td>
<td>check</td>
</tr>
<tr>
<td>Need delete tracking</td>
<td>invalidate_hard_deletes</td>
</tr>
</table>
<p><strong>Testing Strategy Framework</strong></p>
<table>
<tr>
<th>Test Type</th>
<th>Purpose</th>
</tr>
<tr>
<td>not_null</td>
<td>Mandatory keys</td>
</tr>
<tr>
<td>unique</td>
<td>Primary keys</td>
</tr>
<tr>
<td>relationships</td>
<td>Referential integrity</td>
</tr>
<tr>
<td>singular tests</td>
<td>Business rules</td>
</tr>
<tr>
<td>severity: warn</td>
<td>Non-blocking checks</td>
</tr>
</table>
<p><strong>Folder Responsibility Contract</strong></p>
<table>
<tr>
<th>Layer</th>
<th>Responsibility</th>
</tr>
<tr>
<td>staging</td>
<td>Clean + type cast (1-to-1 with source)</td>
</tr>
<tr>
<td>intermediate</td>
<td>Join + deduplicate (business prep)</td>
</tr>
<tr>
<td>marts</td>
<td>Business metrics (analytics layer)</td>
</tr>
<tr>
<td>snapshots</td>
<td>History tracking (SCD Type 2)</td>
</tr>
<tr>
<td>seeds</td>
<td>Static reference data</td>
</tr>
</table>
<p><strong>dbt Project Guardrails (dbt_project.yml)</strong></p>
<pre>staging:
  +materialized: view
intermediate:
  +materialized: ephemeral
marts:
  +materialized: incremental</pre>
<p><strong>YAML Golden Rules</strong></p>
<ul>
<li>2-space indentation only (never tabs)</li>
<li>YAML = expectations (what should be true)</li>
<li>SQL = logic (how data is built)</li>
<li>Tests are trust guarantees</li>
</ul>
<p><strong>Senior Interview Power Lines (Memorize These)</strong></p>
<ul>
<li><strong>"Incremental strategy depends on data mutability â€“ know your source."</strong></li>
<li><strong>"Snapshots track business change, not operational noise."</strong></li>
<li><strong>"dbt scales through convention, not ad-hoc configurations."</strong></li>
<li><strong>"Tests define trust; they catch issues before BI and dashboards do."</strong></li>
<li><strong>"Staging layers isolate upstream chaos; marts define business truth."</strong></li>
<li><strong>"MERGE ensures idempotency in production pipelines."</strong></li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q69">
<div class="question-title">Advanced dbt Macros &amp; Jinja Templating Patterns</div>
<div class="answer">
<p><strong>Macros are reusable SQL/Jinja snippets. They're the difference between repetitive code and elegant architecture.</strong></p>
<p><strong>Pattern 1: Surrogate Key Generation (Enterprise Standard)</strong></p>
<pre>{% macro surrogate_key(field_list) %}
    {% set fields = field_list|join(", ") %}
    md5(concat({{fields}}))
{% endmacro %}</pre>
<p><strong>Usage in model:</strong></p>
<pre>select
    {{ surrogate_key(['customer_id', 'order_date']) }} as sk_customer_order,
    *
from {{ ref('stg_orders') }}</pre>
<p><strong>Why this matters:</strong> Ensures consistent hashing across all models without writing MD5 repeatedly</p>
<p><strong>Pattern 2: Dynamic Column Generation</strong></p>
<pre>{% macro generate_staging_columns(source_table) %}
    {% set columns = run_query("SELECT column_name FROM information_schema.columns WHERE table_name = '" ~ source_table ~ "'") %}
    {% for col in columns %}
        cast({{ col.column_name }} as string) as {{ col.column_name|lower }}{{ "," if not loop.last }}
    {% endfor %}
{% endmacro %}</pre>
<p><strong>Interview power line:</strong> "Macros eliminate manual column enumeration and allow code to adapt to schema changes."</p>
<p><strong>Pattern 3: Conditional Logic Based on Target Environment</strong></p>
<pre>{% macro limit_data() %}
    {% if target.name == 'dev' %}
        where date_trunc('day', created_at) &gt;= current_date - 30
    {% endif %}
{% endmacro %}</pre>
<p><strong>Use case:</strong> Dev uses 30 days, prod uses all data. Single model config, multiple behaviors.</p>
<p><strong>Pattern 4: Testing Helper Macros</strong></p>
<pre>{% macro assert_column_not_null(model, column_name) %}
    select *
    from {{ model }}
    where {{ column_name }} is null
{% endmacro %}</pre>
<p><strong>Called in singular test (tests/check_order_id.sql):</strong></p>
<pre>{{ assert_column_not_null(ref('fct_orders'), 'order_id') }}</pre>
<p><strong>Advanced Jinja Patterns</strong></p>
<ul>
<li><code>run_query()</code> â€“ Executes SQL during parsing (only in execute block)</li>
<li><code>fromjson()</code> â€“ Parse JSON in Jinja</li>
<li><code>tojson()</code> â€“ Convert to JSON</li>
<li><code>safe_divide()</code> â€“ Built-in macro to avoid division by zero</li>
</ul>
<p><strong>Interview line:</strong> "Macros reduce duplication by centralizing transformation logic. They're the foundation of scalable dbt projects."</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q70">
<div class="question-title">dbt Exposures &amp; Metrics â€“ Connecting to BI &amp; Analytics</div>
<div class="answer">
<p><strong>Exposures define downstream BI artifacts (dashboards, reports) that depend on dbt models. They enable end-to-end lineage tracking.</strong></p>
<p><strong>Exposures YAML Structure (metrics.yml or exposures.yml)</strong></p>
<pre>version: 2

exposures:
  - name: executive_dashboard
    type: dashboard
    maturity: production
    owner:
      name: Analytics Lead
      email: analytics@company.com
    description: "Executive KPI dashboard"
    depends_on:
      - ref('fct_revenue')
      - ref('dim_customer')
    url: "https://looker.company.com/dashboards/executive"
    tags:
      - executive
      - critical</pre>
<p><strong>Benefits of Exposures:</strong></p>
<ul>
<li>dbt knows which dashboards depend on which models</li>
<li><code>dbt ls --select state:new</code> shows impacted downstream dashboards</li>
<li>Documentation becomes bi-directional</li>
<li>Stakeholder visibility in dbt DAG</li>
</ul>
<p><strong>dbt Metrics (Semantic Layer)</strong></p>
<p><strong>Purpose:</strong> Define business metrics once, use everywhere (BI tools, APIs, reports)</p>
<pre>metrics:
  - name: total_revenue
    description: "Sum of all orders"
    calculation_method: sum
    expression: order_amount
    timestamp: order_date
    time_grains: [day, month, year]
    dimensions:
      - customer_id
      - region
    meta:
      owner: "Finance"
      sla: "daily"</pre>
<p><strong>How it works:</strong></p>
<ul>
<li>BI tool queries dbt Semantic Layer API</li>
<li>dbt compiles metric to SQL</li>
<li>Single source of truth for business definitions</li>
</ul>
<p><strong>Interview power line:</strong> "Exposures create accountability by showing downstream blast radius. Metrics eliminate metric drift across teams."</p>
<p><strong>Advanced: Custom Properties for Governance</strong></p>
<pre>meta:
  owner: analytics_team
  pii: true
  retention_days: 90
  tags: [critical, gdpr-sensitive]</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q71">
<div class="question-title">dbt on Snowflake: Performance Optimization &amp; Run Config</div>
<div class="answer">
<p><strong>Snowflake-specific tuning that senior engineers use in production to reduce cost and runtime.</strong></p>
<p><strong>1. Warehouse Sizing per Model Type</strong></p>
<pre>models:
  my_project:
    staging:
      +snowflake_warehouse: xs_wh     # Light transformations
    intermediate:
      +snowflake_warehouse: sm_wh
    marts:
      +snowflake_warehouse: lg_wh     # Heavy aggregations</pre>
<p><strong>Why:</strong> Prevents expensive large warehouse for simple selects</p>
<p><strong>2. Query Tags for Cost Attribution</strong></p>
<pre>{{ config(
    tags=['finance', 'daily'],
    query_tag='dbt_incremental_fact_revenue'
) }}</pre>
<p><strong>Result:</strong> Snowflake's query history shows exact cost per model</p>
<p><strong>3. Pre- and Post-Hooks for Cleanup</strong></p>
<pre>{{ config(
    pre_hook="ALTER SESSION SET TIMEZONE = 'UTC'",
    post_hook="DELETE FROM {{ table }} WHERE dbt_valid_to is not null"
) }}</pre>
<p><strong>4. Clustering Keys for Incremental Models</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    cluster_by=['order_date', 'customer_id']
) }}</pre>
<p><strong>Result:</strong> Faster scans on large fact tables</p>
<p><strong>5. Transient Tables for Intermediate Results</strong></p>
<pre>{{ config(
    materialized='table',
    transient=true  # Auto-drops after 24 hours if not queried
) }}</pre>
<p><strong>Use case:</strong> Bulk loading temporary data that's refreshed daily</p>
<p><strong>6. Execute Batch Loading with Dynamic SQL</strong></p>
<pre>{% if execute %}
    {% set batch_size = 10000 %}
    {% set total_rows = run_query("SELECT COUNT(*) FROM source").columns[0][0] %}
    {% for offset in range(0, total_rows, batch_size) %}
        INSERT INTO target SELECT * FROM source LIMIT {{ batch_size }} OFFSET {{ offset }};
    {% endfor %}
{% endif %}</pre>
<p><strong>Advanced: dbt Config per Profile</strong></p>
<pre>profiles.yml:
dev:
  target: dev
  outputs:
    dev:
      type: snowflake
      warehouse: xs_wh
      threads: 1

prod:
  target: prod
  outputs:
    prod:
      type: snowflake
      warehouse: xl_wh
      threads: 8</pre>
<p><strong>Interview line:</strong> "Small warehouses for dev, large for prod, clustering for fact tables, transient for temps. This cuts costs by 40%."</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q72">
<div class="question-title">Data Contracts &amp; Dynamic Testing in dbt</div>
<div class="answer">
<p><strong>Contracts enforce schema guarantees between producers and consumers. They detect breaking changes before they reach dashboards.</strong></p>
<p><strong>Enforcing Contracts via schema.yml</strong></p>
<pre>models:
  - name: fct_revenue
    description: "Revenue facts â€“ daily granularity"
    config:
      contract:
        enforced: true
    columns:
      - name: order_id
        description: "Foreign key to orders"
        data_type: integer
        constraints:
          - type: not_null
          - type: unique
      - name: order_amount
        description: "Amount in USD"
        data_type: numeric(10, 2)
        constraints:
          - type: not_null
          - type: check
            expression: "order_amount &gt; 0"</pre>
<p><strong>What enforced contracts do:</strong></p>
<ul>
<li>dbt fails build if column order changes</li>
<li>dbt fails if data types don't match</li>
<li>dbt fails if NOT NULL dropped</li>
<li>Prevents accidental schema breaking changes</li>
</ul>
<p><strong>Dynamic Testing Pattern (Anomaly Detection)</strong></p>
<pre>{% macro anomaly_detection(model, column_name, threshold=2.5) %}
    with baseline_stats as (
        select
            avg({{ column_name }}) as avg_val,
            stddev_pop({{ column_name }}) as stddev_val
        from {{ model }}
        where date_trunc('day', created_at) &gt;= current_date - 30
    ),
    today_stats as (
        select
            avg({{ column_name }}) as avg_val
        from {{ model }}
        where date_trunc('day', created_at) = current_date
    )
    select *
    from today_stats, baseline_stats
    where abs((today_stats.avg_val - baseline_stats.avg_val) / baseline_stats.stddev_val) &gt; {{ threshold }}
{% endmacro %}</pre>
<p><strong>Use in test:</strong></p>
<pre>{{ anomaly_detection(ref('fct_revenue'), 'order_amount', 3.0) }}</pre>
<p><strong>Result:</strong> Detects unusual spikes/drops in amounts (e.g., bot traffic, data quality issues)</p>
<p><strong>Mutual Exclusivity Testing</strong></p>
<pre>with exclusive_check as (
    select *
    from {{ ref('fct_orders') }}
    where status IN ('completed', 'cancelled', 'pending')
      and status IS NULL
)
select * from exclusive_check</pre>
<p><strong>Interview line:</strong> "Contracts catch schema breaking changes at parse time. Dynamic tests catch data anomalies at execution time."</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q73">
<div class="question-title">CI/CD Workflows in dbt Cloud &amp; Git-Driven Development</div>
<div class="answer">
<p><strong>Modern dbt development = Git + dbt Cloud + Automated Testing. This is how seniors ship reliable changes.</strong></p>
<p><strong>Git Workflow in dbt Projects</strong></p>
<pre>Workflow:
1. Create feature branch: git checkout -b feature/new-metric
2. Develop locally: dbt run -s models/marts/metrics
3. Test locally: dbt test -s models/marts/metrics
4. Commit: git commit -m "Add revenue metric for Q1"
5. Push &amp; Open PR: git push origin feature/new-metric
6. dbt Cloud runs automated tests on PR
7. Merge to main after approval
8. dbt Cloud runs full CI/CD pipeline in prod</pre>
<p><strong>dbt Cloud PR Checks (CI/CD)</strong></p>
<pre>dbt_project.yml:
vars:
  dbt_environment: "{{ env_var('DBT_ENV', 'dev') }}"</pre>
<p><strong>What happens on PR:</strong></p>
<ul>
<li>dbt parses all models (catches syntax errors early)</li>
<li>Runs <code>dbt test</code> on changed models + downstream</li>
<li>Produces diff report (new rows, changed columns)</li>
<li>Shows estimated cost impact</li>
<li>Blocks merge if tests fail</li>
</ul>
<p><strong>State-Based Testing (Only Check What Changed)</strong></p>
<pre>dbt test --select state:modified+
# Tests only changed models + downstream dependencies
# Saves time vs full suite</pre>
<p><strong>Behind the scenes:</strong> dbt tracks manifest.json (DAG) in Git, compares branches</p>
<p><strong>Deployment Best Practice: Slim CI</strong></p>
<pre>dbt run --select state:modified+ --threads 8
dbt test --select state:modified+ --threads 8</pre>
<p><strong>Result:</strong> 30-second CI pipeline instead of 10-minute full run</p>
<p><strong>Production Deployment Strategy</strong></p>
<pre>Deployment Job in dbt Cloud:
1. dbt run --select state:modified+
2. dbt test
3. dbt snapshot (if needed)
4. Rollback if tests fail (automated)</pre>
<p><strong>Advanced: Webhook Triggers</strong></p>
<pre>Webhook â†’ dbt Cloud Job when:
- Upstream data loaded to Snowflake
- Schedule (daily 2 AM UTC)
- Manual trigger from dashboard</pre>
<p><strong>Team Guardrails</strong></p>
<ul>
<li>No one can edit prod without PR approval</li>
<li>dbt Cloud enforces PR checks</li>
<li>Secrets in environment variables (never in code)</li>
<li>Audit trail in Git history</li>
</ul>
<p><strong>Interview power line:</strong> "State-based CI testing catches breaking changes in seconds. dbt Cloud makes data pipelines as reliable as software deployments."</p>
<p><strong>Senior gotcha:</strong> "Developers think 'dbt run' works locally, then fail in PR because of missing upstream data. Always test against prod database in CI."</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q74">
<div class="question-title">dbt Pre-Hooks &amp; Post-Hooks â€“ Advanced Execution Control</div>
<div class="answer">
<p><strong>Pre-hooks and post-hooks execute arbitrary SQL before/after model runs. They're essential for setup, cleanup, and auditing in production pipelines.</strong></p>
<p><strong>What Are Pre-Hooks &amp; Post-Hooks?</strong></p>
<ul>
<li><strong>Pre-hook:</strong> SQL that runs BEFORE a model is built</li>
<li><strong>Post-hook:</strong> SQL that runs AFTER a model is built successfully</li>
</ul>
<p><strong>Hooks don't run if the model build fails (unless on-first-run is set).</strong></p>
<p><strong>Pattern 1: Timezone Configuration (Pre-Hook)</strong></p>
<p><strong>Problem:</strong> Different teammates have different session timezones, causing inconsistent date calculations</p>
<pre>{{ config(
    pre_hook="ALTER SESSION SET TIMEZONE = 'UTC'",
    post_hook="ALTER SESSION SET TIMEZONE = 'America/New_York'"
) }}

select
    current_timestamp as event_time,  -- Always UTC
    order_date,
    amount
from {{ ref('stg_orders') }}</pre>
<p><strong>Result:</strong> All date arithmetic runs in UTC regardless of user timezone</p>
<p><strong>Pattern 2: Grant Permissions (Post-Hook)</strong></p>
<p><strong>Problem:</strong> BI team can't query new marketing mart models</p>
<pre>{{ config(
    materialized='table',
    post_hook="GRANT SELECT ON {{ this }} TO ROLE analytics_viewer"
) }}

select
    customer_id,
    total_orders,
    ltv
from {{ ref('int_customer_metrics') }}</pre>
<p><strong>Result:</strong> Model automatically grants read access to analytics_viewer when built</p>
<p><strong>Pattern 3: Audit Logging (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Track when models are refreshed for compliance/SLA monitoring</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    post_hook="INSERT INTO audit_log (table_name, load_timestamp, row_count, status) 
               SELECT '{{ this.name }}', current_timestamp, COUNT(*), 'SUCCESS' FROM {{ this }}"
) }}

select * from {{ ref('stg_orders') }}</pre>
<p><strong>Audit log result:</strong></p>
<pre>table_name     load_timestamp              row_count  status
fct_orders     2025-02-08 10:15:30.123     1,245,678  SUCCESS
fct_orders     2025-02-08 09:15:05.456     1,244,932  SUCCESS</pre>
<p><strong>Pattern 4: Data Quality Check (Post-Hook with Conditional Fail)</strong></p>
<p><strong>Problem:</strong> Need to fail model if row count drops &gt; 10% from yesterday</p>
<pre>{{ config(
    materialized='table',
    post_hook=[
        "INSERT INTO dbt_audit_checks (model_name, check_type, status, details)
         SELECT 
            '{{ this.name }}',
            'row_count_validation',
            CASE 
                WHEN (SELECT COUNT(*) FROM {{ this }}) &lt; (SELECT COUNT(*) FROM {{ this }}_prev) * 0.9
                THEN 'FAILED'
                ELSE 'PASSED'
            END,
            'Expected ' || (SELECT COUNT(*) FROM {{ this }}_prev) || ' rows, got ' || (SELECT COUNT(*) FROM {{ this }})
        "
    ]
) }}

select * from {{ ref('fct_revenue') }}</pre>
<p><strong>Pattern 5: Snapshot &amp; Archive (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Automatically snapshot fact table before incremental refresh (backup safety)</p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    post_hook="CREATE TABLE {{ this }}_snapshot_{{ today() }} AS SELECT * FROM {{ this }}"
) }}</pre>
<p><strong>Result:</strong> Automatic daily backups (fct_orders_snapshot_2025-02-08, etc.)</p>
<p><strong>Pattern 6: Cluster Keys Management (Pre-Hook)</strong></p>
<p><strong>Problem:</strong> Need to reclustered table without rebuilding data</p>
<pre>{{ config(
    materialized='table',
    pre_hook="ALTER TABLE {{ this }} CLUSTER BY (order_date, customer_id)",
    post_hook="SELECT SYSTEM$CLUSTERING_INFORMATION('{{ this }}')"
) }}</pre>
<p><strong>Why:</strong> Clustering improves scan performance on large tables by 3-5x</p>
<p><strong>Pattern 7: Dynamic Statistics Update (Post-Hook)</strong></p>
<p><strong>Purpose:</strong> Update table statistics for query optimizer (Snowflake performance)</p>
<pre>{{ config(
    materialized='table',
    post_hook="ANALYZE TABLE {{ this }} COMPUTE STATS"
) }}</pre>
<p><strong>Why:</strong> Outdated stats lead to poor query plans; forces re-analysis</p>
<p><strong>Hook Execution Order</strong></p>
<pre>1. Pre-hook (before model build)
2. Model SQL executes
3. Post-hook (after successful build)
4. Cleanup (drop old table if full refresh)</pre>
<p><strong>Multiple Hooks (Array Syntax)</strong></p>
<p><strong>You can chain multiple post-hooks:</strong></p>
<pre>{{ config(
    post_hook=[
        "GRANT SELECT ON {{ this }} TO ROLE analytics",
        "INSERT INTO audit_log VALUES ('{{ this.name }}', current_timestamp)",
        "CALL refresh_downstream_views('{{ this.name }}')"
    ]
) }}</pre>
<p><strong>They execute in order; if any fails, subsequent hooks don't run.</strong></p>
<p><strong>Conditional Hooks (if-then Logic)</strong></p>
<p><strong>Run hooks only in production:</strong></p>
<pre>{% if target.name == 'prod' %}
    {{ config(
        post_hook="GRANT SELECT ON {{ this }} TO ROLE bi_team"
    ) }}
{% endif %}</pre>
<p><strong>Common pattern:</strong> Permissions+grants in prod only, skip in dev</p>
<p><strong>Hook Access to dbt Context</strong></p>
<p><strong>Inside hooks, you can access:</strong></p>
<ul>
<li><code>{{ this }}</code> â€“ Current table/view name</li>
<li><code>{{ this.name }}</code> â€“ Table name only</li>
<li><code>{{ target.name }}</code> â€“ Environment (dev/prod)</li>
<li><code>{{ env_var('VAR_NAME') }}</code> â€“ Environment variables</li>
<li><code>current_timestamp</code> â€“ Timestamp function</li>
</ul>
<p><strong>Anti-Patterns to Avoid</strong></p>
<ul>
<li>Don't grant permissions in pre-hook (table doesn't exist yet)</li>
<li>Don't query {{ this }} in pre-hook (old table still exists)</li>
<li>Don't fail tests in post-hooks (use dbt test instead)</li>
<li>Avoid expensive operations in post-hooks (slows down CI/CD)</li>
</ul>
<p><strong>Interview Power Lines</strong></p>
<ul>
<li><strong>"Pre/post-hooks automate operational setup without modifying SQL logic."</strong></li>
<li><strong>"Post-hook grants make permission management scale with new models."</strong></li>
<li><strong>"Audit logging via post-hooks creates compliance trails automatically."</strong></li>
<li><strong>"Hooks can cluster tables and update statistics without full rebuild."</strong></li>
</ul>
<p><strong>Real Production Example: Complete Flow</strong></p>
<pre>{{ config(
    materialized='incremental',
    unique_key='order_id',
    pre_hook=[
        "ALTER SESSION SET TIMEZONE = 'UTC'",
        "ALTER SESSION SET NLS_DATE_FORMAT = 'YYYY-MM-DD'"
    ],
    post_hook=[
        "GRANT SELECT ON {{ this }} TO ROLE analytics_viewer",
        "INSERT INTO dbt_audit_log (model, loaded_at, rows) SELECT '{{ this.name }}', current_timestamp, COUNT(*) FROM {{ this }}",
        "ALTER TABLE {{ this }} CLUSTER BY (order_date, customer_id)",
        "CALL refresh_downstream_materialized_views('{{ this.name }}')"
    ]
) }}

select
    order_id,
    order_date,
    customer_id,
    amount
from {{ ref('stg_orders') }}
where order_date &gt;= '2025-01-01'</pre>
<a class='\"back-to-toc\"' href='\"#table-of-contents\"'>â†‘ Back to Contents</a>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
</div>
</body>

