<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Complete Snowflake & Data Engineering Interview Guide</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>
<body>

<div class="container">

<div class="container">
<header>
<h1>Complete Snowflake &amp; Data Engineering Interview Guide</h1>
<p class="subtitle">Comprehensive Questions with Detailed Answers</p>
</header>
<div class="toc-container" id="toc">
<h2>ðŸ“˜ Contents</h2>
<ul>
<li><a href="#q1">Q1: Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</a></li>
<li><a href="#q2">Q2: Snowflake Architecture</a></li>
<li><a href="#q3">Q3: Time-Travel with scenarios like offset, query_id and timestamp</a></li>
<li><a href="#q4">Q4: Optimize the query performance and how you do it</a></li>
<li><a href="#q5">Q5: What is partition and what is micro-partitions</a></li>
<li><a href="#q6">Q6: SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</a></li>
<li><a href="#q7">Q7: How you ingested JSON data into Snowflake, what steps you perform</a></li>
<li><a href="#q8">Q8: If the file size is 50GB, how you will ingest the data</a></li>
<li><a href="#q9">Q9: Internal and External stages in Snowflake - Uses for each</a></li>
<li><a href="#q10">Q10: How you schedule a data pipeline in Snowflake</a></li>
<li><a href="#q11">Q11: Stored Procedure</a></li>
<li><a href="#q12">Q12: User Defined Functions (UDF)</a></li>
<li><a href="#q13">Q13: What is Git</a></li>
<li><a href="#q14">Q14: What is QUALIFY in Snowflake - Why it's used</a></li>
<li><a href="#q15">Q15: Benefits of Snowflake</a></li>
<li><a href="#q16">Q16: What is CDC (Change Data Capture) - Is it tool or term</a></li>
<li><a href="#q17">Q17: Python questions - List and Tuple</a></li>
<li><a href="#q18">Q18: AVRO, Parquet, ORC file formats - Uses</a></li>
<li><a href="#q19">Q19: Difference between CTE and Temp table - Where to use</a></li>
<li><a href="#q20">Q20: Difference between Transient and Dynamic and Dynamic Transient tables</a></li>
<li><a href="#q21">Q21: How you will handle schema changes in the downstream</a></li>
<li><a href="#q22">Q22: Ephemeral vs Permanent tables</a></li>
<li><a href="#q23">Q23: How to implement CDC without ETL tools in Snowflake</a></li>
<li><a href="#q24">Q24: What is DBT and uses - Execution plan</a></li>
<li><a href="#q25">Q25: Why continuous data load used - Use cases</a></li>
<li><a href="#q26">Q26: Explain end-to-end data pipeline - What are the logics you followed</a></li>
<li><a href="#q27">Q27: Why implement CDC instead of other approaches - Use case</a></li>
<li><a href="#q28">Q28: Snowflake Tasks - Troubleshooting failed queries and performance improvement</a></li>
<li><a href="#q29">Q29: What is Zero Copy clone - Uses</a></li>
<li><a href="#q30">Q30: Convert timestamp from one timezone to another</a></li>
<li><a href="#q31">Q31: Clustering Keys in Snowflake</a></li>
<li><a href="#q32">Q32: How to monitor Snowflake performance</a></li>
<li><a href="#q33">Q33: Attacama and Collibra - Uses and Differences</a></li>
<li><a href="#q34">Q34: How to optimize long running queries - How to reduce time</a></li>
<li><a href="#q35">Q35: What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</a></li>
<li><a href="#q36">Q36: Max Salary for each department and 4th max salary for each department</a></li>
<li><a href="#q37">Q37: Swap the gender value for a table - Male to Female, Female to Male</a></li>
<li><a href="#q38">Q38: DBT Project Architecture</a></li>
<li><a href="#q39">Q39: Difference between UPSERT and MERGE</a></li>
<li><a href="#q40">Q40: Data Masking and Masking Policy</a></li>
<li><a href="#q41">Q41: Difference between Hashing and Encryption</a></li>
<li><a href="#q42">Q42: Snowflake storage integration with AWS S3</a></li>
<li><a href="#q43">Q43: What is DBT and what problem does it solve in modern data stack</a></li>
<li><a href="#q44">Q44: Core components of a DBT project: Models, Tests, Seeds</a></li>
<li><a href="#q45">Q45: Materializations in DBT - Four default types</a></li>
<li><a href="#q46">Q46: Managing dependencies and referencing models in DBT</a></li>
<li><a href="#q47">Q47: What is Jinja in DBT - Simple example</a></li>
<li><a href="#q48">Q48: Data Governance best practices</a></li>
<li><a href="#q49">Q49: Data quality testing strategies</a></li>
<li><a href="#q50">Q50: Building a modern data stack - Technology selection</a></li>
<li><a href="#q51">Q51: Future trends in data engineering</a></li>
<li><a href="#q52">Q52: Explain your end-to-end data pipeline (tools + flow)</a></li>
<li><a href="#q53">Q53: How do you use Python inside Airflow for orchestration and failure alerts?</a></li>
<li><a href="#q54">Q54: How do you implement SCD Type 2 in dbt?</a></li>
<li><a href="#q55">Q55: How does dbt snapshot handle deletes?</a></li>
<li><a href="#q56">Q56: When and why do you use dbt seeds?</a></li>
<li><a href="#q57">Q57: SQL Challenge â€“ Orders and Payments</a></li>
<li><a href="#q58">Q58: dbt Materializations â€” Syntax &amp; When to Use</a></li>
<li><a href="#q59">Q59: dbt Snapshots â€” Syntax &amp; Purpose</a></li>
<li><a href="#q60">Q60: dbt YAML Files â€” Syntax, Indentation &amp; Purpose</a></li>
<li><a href="#q61">Q61: Tests in YAML â€” Built-in &amp; Relationships</a></li>
<li><a href="#q62">Q62: Materialization Configuration via dbt_project.yml</a></li>
<li><a href="#q63">Q63: Quick Interview Cheat Table &amp; Power Lines</a></li>
<li><a href="#q64">Q64: Advanced Incremental Strategies â€“ MERGE vs APPEND</a></li>
<li><a href="#q65">Q65: Custom dbt Tests â€“ Beyond Built-ins</a></li>
<li><a href="#q66">Q66: Enterprise-Grade dbt Folder Structure</a></li>
<li><a href="#q67">Q67: Production dbt Failures &amp; Root Causes (What Seniors Face)</a></li>
<li><a href="#q68">Q68: Senior-Level dbt Cheat Sheet &amp; Interview Power Lines</a></li>
<li><a href="#q69">Q69: Advanced dbt Macros &amp; Jinja Templating Patterns</a></li>
<li><a href="#q70">Q70: dbt Exposures &amp; Metrics â€“ Connecting to BI &amp; Analytics</a></li>
<li><a href="#q71">Q71: dbt on Snowflake: Performance Optimization &amp; Run Config</a></li>
<li><a href="#q72">Q72: Data Contracts &amp; Dynamic Testing in dbt</a></li>
<li><a href="#q73">Q73: CI/CD Workflows in dbt Cloud &amp; Git-Driven Development</a></li>
<li><a href="#q74">Q74: dbt Pre-Hooks &amp; Post-Hooks â€“ Advanced Execution Control</a></li>
</ul>
</div>
<div class="question" id="q1">
<div class="question-title">Write a query to get how much compute hours consumed overall and also how much compute hrs used for each day</div>
<div class="answer">
<p>This requires interpreting the start and end events to calculate the duration. Assuming Date_timestamp is a DATETIME or TIMESTAMP column.</p>
<pre>WITH EventSequence AS (
    SELECT
        Date_timestamp,
        progress,
        LAG(Date_timestamp) OVER (ORDER BY Date_timestamp) AS Previous_Timestamp,
        LAG(progress) OVER (ORDER BY Date_timestamp) AS Previous_Progress
    FROM your_table_name
),
ComputeDurations AS (
    SELECT
        Date_timestamp,
        progress,
        Previous_Timestamp,
        Previous_Progress,
        CASE
            WHEN progress = 'end' AND Previous_Progress = 'start'
            THEN DATEDIFF(SECOND, Previous_Timestamp, Date_timestamp)
            ELSE 0
        END AS Duration_Seconds
    FROM EventSequence
    WHERE progress = 'end' AND Previous_Progress = 'start'
)
SELECT
    SUM(Duration_Seconds) / 3600.0 AS Total_Compute_Hours_Overall,
    TO_DATE(Date_timestamp) AS Compute_Date,
    SUM(Duration_Seconds) / 3600.0 AS Daily_Compute_Hours
FROM ComputeDurations
GROUP BY TO_DATE(Date_timestamp)
ORDER BY Compute_Date;</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li><strong>EventSequence:</strong> This CTE assigns the Previous_Timestamp and Previous_Progress to each row</li>
<li><strong>ComputeDurations:</strong> Calculates the Duration_Seconds only for valid end events preceded by a start event</li>
<li><strong>Final SELECT:</strong> Sums durations to get Total and groups by date for Daily_Compute_Hours</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q2">
<div class="question-title">Snowflake Architecture</div>
<div class="answer">
<p>Snowflake's architecture is a unique multi-cluster shared data architecture. It separates compute and storage, allowing them to scale independently, and includes a cloud services layer for management.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Database Storage:</strong> Data is reorganized into a columnar, compressed format, encrypted, and stored in micro-partitions. This storage layer is elastic and can scale dynamically.</li>
<li><strong>Query Processing (Compute Layer):</strong> Consists of virtual warehouses - independent compute clusters that execute queries without sharing compute resources.</li>
<li><strong>Cloud Services Layer:</strong> The brain of Snowflake, coordinating all activities including authentication, metadata management, query optimization, infrastructure management, and transaction management.</li>
</ul>
<p><strong>Analogy:</strong> Imagine a library where storage is the books on shelves, compute is the reading rooms, and cloud services is the librarian system managing everything.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q3">
<div class="question-title">Time-Travel with scenarios like offset, query_id and timestamp</div>
<div class="answer">
<p>Snowflake's Time Travel feature allows you to query historical data that has been changed or deleted within a specified retention period (default 1 day, up to 90 days for Enterprise Edition).</p>
<p><strong>Using AT (OFFSET):</strong> Query data from a specific point in time relative to current time (negative offset for past).</p>
<pre>-- Current data
SELECT * FROM my_table;

-- Data from 5 minutes (300 seconds) ago
SELECT * FROM my_table AT (OFFSET =&gt; -300);</pre>
<p><strong>Using AT (TIMESTAMP):</strong> Query data as it existed at an exact past timestamp.</p>
<pre>SELECT * FROM my_table AT (TIMESTAMP =&gt; '2025-06-25 10:30:00'::TIMESTAMP_LTZ);</pre>
<p><strong>Using BEFORE (STATEMENT):</strong> Query data immediately before a specific DML statement executed.</p>
<pre>CREATE TABLE my_table_recovery AS
SELECT * FROM my_table BEFORE (STATEMENT =&gt; '123abc456def');</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q4">
<div class="question-title">Optimize the query performance and how you do it</div>
<div class="answer">
<p><strong>Key Optimization Strategies:</strong></p>
<ol>
<li><strong>Choose the Right Virtual Warehouse Size:</strong> Larger warehouses provide more compute power and memory. Use larger warehouses for complex queries on large datasets.</li>
<li><strong>Cluster Your Tables:</strong> Define clustering keys on columns frequently used in WHERE clauses, JOIN conditions, or GROUP BY clauses.</li>
<li><strong>Use Materialized Views:</strong> Pre-compute and store results of complex queries, automatically updating when base tables change.</li>
<li><strong>Query Pruning:</strong> Ensure your WHERE clauses are effective. Filter on clustered columns or columns with high cardinality.</li>
<li><strong>Effective Caching:</strong> Leverage Snowflake's Result Cache and Warehouse Cache by running the same queries multiple times.</li>
<li><strong>Avoid Anti-Patterns:</strong> Don't use SELECT *, avoid correlated subqueries, eliminate unnecessary ORDER BY or DISTINCT.</li>
<li><strong>Monitor Query Profile:</strong> Use Snowflake's Query Profile to identify bottlenecks and optimize accordingly.</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q5">
<div class="question-title">What is partition and what is micro-partitions</div>
<div class="answer">
<p><strong>Partition (General Database Concept):</strong> A strategy to divide a large table into smaller, manageable pieces based on a specified column (e.g., date, region). Usually managed explicitly by the database administrator.</p>
<p><strong>Micro-partitions (Snowflake Specific):</strong> Snowflake automatically organizes all data into immutable, compressed, columnar units typically ranging from 50 MB to 500 MB. This is automatic and transparent.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>Automatic and Transparent - handled by Snowflake</li>
<li>Rich metadata stored about each micro-partition (value ranges, distinct values, null counts)</li>
<li>Query Pruning - metadata allows Snowflake to skip irrelevant micro-partitions</li>
<li>Clustering - you can define clustering keys to optimize physical co-location of data</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q6">
<div class="question-title">SCD Scenarios (SCD-0, SCD-1, SCD-2, SCD-3, SCD-4)</div>
<div class="answer">
<p><strong>SCD Type 0: Retain Original</strong> - No changes tracked. Dimension attribute value never changes.</p>
<p><strong>SCD Type 1: Overwrite</strong> - New data overwrites old data. History is not preserved.</p>
<p><strong>SCD Type 2: Add New Row</strong> - New row added with effective date ranges. History is fully preserved.</p>
<pre>Initial: EmployeeID: 201, Department: Sales, StartDate: 2020-01-01, EndDate: 9999-12-31, IsCurrent: TRUE
After Change: Department changes to Marketing
Result: 
- Old row: EndDate: 2025-05-31, IsCurrent: FALSE
- New row: Department: Marketing, StartDate: 2025-06-01, IsCurrent: TRUE</pre>
<p><strong>SCD Type 3: Add New Attribute</strong> - New column added for previous value. Partial history.</p>
<p><strong>SCD Type 4: Use History Table</strong> - Main table holds current (Type 1), separate history table stores all past versions (Type 2).</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q7">
<div class="question-title">How you ingested JSON data into Snowflake, what steps you perform</div>
<div class="answer">
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>Prepare JSON Data:</strong> Ensure well-formed JSON files (individual objects or newline-delimited)</li>
<li><strong>Stage the Files:</strong> Use Internal Stage (PUT command) or External Stage (S3, Azure, GCP)</li>
<li><strong>Create File Format:</strong> Define how to interpret JSON files
                        <pre>CREATE FILE FORMAT json_file_format
    TYPE = 'JSON'
    STRIP_OUTER_ARRAY = TRUE
    NULL_IF = ('', 'NULL');</pre>
</li>
<li><strong>Create Target Table:</strong> Either single VARIANT column or structured columns
                        <pre>CREATE TABLE raw_json_data (
    id INT,
    json_payload VARIANT,
    load_timestamp TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
);</pre>
</li>
<li><strong>Copy Data:</strong> Use COPY INTO to load data from stage
                        <pre>COPY INTO raw_json_data
FROM @my_external_json_stage/
FILE_FORMAT = json_file_format
ON_ERROR = 'CONTINUE';</pre>
</li>
<li><strong>Query &amp; Transform:</strong> Use VARIANT functions to extract and transform data</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q8">
<div class="question-title">If the file size is 50GB, how you will ingest the data</div>
<div class="answer">
<p><strong>For 50GB files, the approach is:</strong></p>
<ol>
<li><strong>Stage the File:</strong> Use External Stage (S3, Azure, GCP) - faster and more reliable than internal stage</li>
<li><strong>Use Large Virtual Warehouse:</strong> Use at least X-Large or larger (2X-Large, 3X-Large) for parallel processing and sufficient memory</li>
<li><strong>Execute COPY INTO:</strong>
<pre>USE WAREHOUSE MY_LARGE_WAREHOUSE;

COPY INTO your_target_table
FROM @my_external_stage/your_50gb_file.csv
FILE_FORMAT = (FORMAT_NAME = my_csv_file_format)
ON_ERROR = 'CONTINUE'
PURGE = TRUE;</pre>
</li>
<li><strong>Key Considerations:</strong>
<ul>
<li>Use columnar formats (Parquet, ORC) if possible</li>
<li>Ensure good network bandwidth</li>
<li>Monitor COPY_HISTORY for status</li>
<li>COPY INTO is inherently parallel - larger warehouse leverages this</li>
</ul>
</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q9">
<div class="question-title">Internal and External stages in Snowflake - Uses for each</div>
<div class="answer">
<p><strong>Internal Stages (Snowflake-managed Storage):</strong></p>
<ul>
<li>User stage: @~/</li>
<li>Table stage: @%table_name</li>
<li>Named stage: CREATE STAGE my_internal_stage</li>
<li>Uses: Quick data loading, secure data transfer, temporary files</li>
</ul>
<p><strong>External Stages (User-managed Cloud Storage):</strong></p>
<ul>
<li>Points to AWS S3, Azure Blob, Google Cloud Storage</li>
<li>Uses: Large-scale automated loads, data lake integration, Snowpipe, continuous loading</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Internal Stage</th>
<th>External Stage</th>
</tr>
<tr>
<td>Storage</td>
<td>Snowflake managed</td>
<td>Your cloud storage</td>
</tr>
<tr>
<td>File Access</td>
<td>PUT / GET commands</td>
<td>COPY INTO</td>
</tr>
<tr>
<td>Cost</td>
<td>Included in Snowflake</td>
<td>Cloud provider costs</td>
</tr>
<tr>
<td>Ideal Use</td>
<td>Ad-hoc, smaller loads</td>
<td>Large-scale automated</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q10">
<div class="question-title">How you schedule a data pipeline in Snowflake</div>
<div class="answer">
<p><strong>Snowflake Tasks (Native):</strong></p>
<pre>-- Create warehouse for tasks
CREATE WAREHOUSE ETL_WH WAREHOUSE_SIZE = 'XSMALL';

-- 1. Root Task (scheduled)
CREATE TASK load_raw_data
  WAREHOUSE = ETL_WH
  SCHEDULE = 'USING CRON 0 9 * * * Asia/Kolkata'
  AS
  COPY INTO RAW_TABLE FROM @my_external_stage/raw_files/;

-- 2. Child Task (depends on load_raw_data)
CREATE TASK transform_data
  WAREHOUSE = ETL_WH
  AFTER load_raw_data
  AS
  INSERT INTO STAGING_TABLE SELECT ... FROM RAW_TABLE;

-- Enable tasks
ALTER TASK load_raw_data RESUME;
ALTER TASK transform_data RESUME;</pre>
<p><strong>External Orchestration Tools:</strong> Apache Airflow, AWS Step Functions, Azure Data Factory, Control-M, Autosys - for complex cross-platform pipelines.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q11">
<div class="question-title">Stored Procedure</div>
<div class="answer">
<p>A Stored Procedure is a set of SQL statements and procedural logic compiled and stored in the database. It can be executed by calling its name with input parameters.</p>
<p><strong>Key Characteristics:</strong></p>
<ul>
<li>Encapsulation of complex logic</li>
<li>Reusability - write once, run many times</li>
<li>Parameterization - accepts input parameters</li>
<li>Better performance - pre-compiled</li>
<li>Enhanced security - grant permissions on procedure, not underlying tables</li>
</ul>
<pre>CREATE PROCEDURE calculate_daily_sales(sales_date DATE)
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE
    total_sales DECIMAL(18, 2);
BEGIN
    DELETE FROM DAILY_SALES_SUMMARY WHERE summary_date = :sales_date;
    SELECT SUM(amount) INTO total_sales FROM RAW_SALES_DATA WHERE sale_date = :sales_date;
    INSERT INTO DAILY_SALES_SUMMARY VALUES (:sales_date, :total_sales);
    IF (total_sales IS NULL) THEN
        RETURN 'No sales data found for ' || :sales_date;
    ELSE
        RETURN 'Successfully summarized for ' || :sales_date;
    END IF;
END;
$$;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q12">
<div class="question-title">User Defined Functions (UDF)</div>
<div class="answer">
<p>A User Defined Function is a custom function that performs specific operations, similar to built-in functions. UDFs encapsulate logic that can be reused within SQL queries.</p>
<p><strong>Types:</strong></p>
<ul>
<li>Scalar UDFs - return single value per input row</li>
<li>Table UDFs (UDTFs) - return set of rows per input row</li>
</ul>
<p><strong>Scalar UDF Example:</strong></p>
<pre>CREATE FUNCTION calculate_net_sales(sales_amount DECIMAL(10,2), return_amount DECIMAL(10,2))
RETURNS DECIMAL(10,2)
AS
$$
    sales_amount - COALESCE(return_amount, 0)
$$;

SELECT order_id, calculate_net_sales(sales_amount, return_amount) AS net_sales
FROM daily_transactions;</pre>
<p><strong>Languages Supported:</strong> SQL, JavaScript, Python, Java, Scala (via Snowpark)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q13">
<div class="question-title">What is Git</div>
<div class="answer">
<p>Git is a free, open-source distributed version control system (DVCS) designed to handle projects of any size. It tracks changes in source code and other files during software development, enabling collaborative work and complete change history.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Distributed:</strong> Every developer has a complete copy of the entire repository with full history. No single point of failure</li>
<li><strong>Version Control:</strong> Tracks every change with author, timestamp, and message. Complete history allows reverting to any previous state</li>
<li><strong>Branches:</strong> Parallel lines of development (main, develop, feature branches) allowing teams to work independently</li>
<li><strong>Commits:</strong> Snapshots of the entire repository at specific points with unique SHA-1 hash identifiers</li>
<li><strong>Repository:</strong> Complete collection of files, branches, and entire change history (can be local or on remote servers like GitHub)</li>
<li><strong>Staging Area (Index):</strong> Intermediate area where you select which changes to include in the next commit</li>
</ul>
<p><strong>Typical Workflow:</strong></p>
<pre>git clone https://github.com/user/repo.git  # Clone remote repo
git branch feature/new-analysis              # Create feature branch
git checkout feature/new-analysis             # Switch to feature branch
git add analysis_script.sql                   # Stage changes
git commit -m "Add new sales analysis"        # Commit with message
git push origin feature/new-analysis          # Push to remote
# Create pull request on GitHub for review
git checkout main                             # Switch back to main
git pull origin main                          # Get latest main
git merge feature/new-analysis                # Merge feature branch</pre>
<p><strong>For Data Teams:</strong> Git is essential for version controlling DBT projects, SQL scripts, Python ETL code, and documentation. Enables code review, audit trails, and rollback capabilities</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q14">
<div class="question-title">What is QUALIFY in Snowflake - Why it's used</div>
<div class="answer">
<p>QUALIFY is a clause unique to Snowflake that filters results of window functions without needing to wrap queries in subqueries or CTEs.</p>
<p><strong>Example: Find the 2nd highest paid employee in each department</strong></p>
<p><strong>With QUALIFY (Simpler):</strong></p>
<pre>SELECT employee_id, employee_name, department, salary
FROM employees
QUALIFY RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 2;</pre>
<p><strong>Without QUALIFY (requires CTE):</strong></p>
<pre>WITH EmployeeRank AS (
    SELECT ..., RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rnk
    FROM employees
)
SELECT * FROM EmployeeRank WHERE rnk = 2;</pre>
<p><strong>Benefits:</strong> Simplicity, readability, potential performance improvements, direct filtering on window function results</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q15">
<div class="question-title">Benefits of Snowflake</div>
<div class="answer">
<p><strong>Key Benefits:</strong></p>
<ul>
<li><strong>Separation of Compute and Storage:</strong> Scale independently - add storage without compute cost. Run small XS warehouse for lightweight queries, large 3X warehouse for complex jobs. Only pay for compute when warehouse is running</li>
<li><strong>Elasticity and Scalability:</strong> Instantly resize warehouses (XS to 3X in seconds) or auto-scale with multi-cluster warehouses for concurrency</li>
<li><strong>Near-Zero Management:</strong> Fully managed - Snowflake handles patching, updates, hardware provisioning, index maintenance. No DBAs needed for infrastructure</li>
<li><strong>Support for Semi-Structured Data:</strong> Native VARIANT type handles JSON/Avro/Parquet natively. Query nested data with dot notation without flattening</li>
<li><strong>Concurrency:</strong> Multiple independent virtual warehouses access same data simultaneously without locking or contention. BI team on WH1, ETL team on WH2, no impact</li>
<li><strong>Data Sharing:</strong> Secure, instantaneous live data sharing with other Snowflake accounts (same or different regions). Share without copying data</li>
<li><strong>Time Travel &amp; Fail-safe:</strong> Query data as it existed 1-90 days ago. Accidentally dropped table? UNDROP within retention period. 7-day fail-safe for disaster recovery</li>
<li><strong>Performance Optimization:</strong> Automatic micro-partitioning with intelligent pruning, multi-layer caching (result + warehouse cache), optional clustering keys</li>
<li><strong>Security &amp; Compliance:</strong> AES-256 encryption at rest/transit, multi-factor MFA, RBAC + object-level privileges, row/column masking, SOC2/PCI/HIPAA/GDPR certified</li>
<li><strong>Ecosystem Integration:</strong> Native connectors for Tableau, Power BI, Looker. DBT integration, Spark through connectors, Python/Pandas via snowpark_python</li>
</ul>
<p><strong>Cost Efficiency Example:</strong> Traditional data warehouse requires expensive hardware, undergoes periods of low utilization (pay for unused capacity). Snowflake: suspend warehouse when not used, pay only for storage. Scale up for batch jobs, down immediately after</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q16">
<div class="question-title">What is CDC (Change Data Capture) - Is it tool or term</div>
<div class="answer">
<p>CDC is primarily a <strong>term and technique</strong> (not a single tool). It identifies and captures changes (INSERT, UPDATE, DELETE) made to data in a source database and delivers those changes to a target system efficiently instead of reprocessing entire datasets.</p>
<p><strong>Common Mechanisms:</strong></p>
<ul>
<li><strong>Timestamp-Based (Polling):</strong> Periodic queries find rows where last_updated_timestamp &gt; previous_check_time. Simple but can miss deletes, slow for large tables, unreliable if timestamps aren't consistent</li>
<li><strong>Log-Based (Most Robust):</strong> Read database transaction logs (MySQL binlog, Oracle redo logs, SQL Server transaction log). Captures ALL changes in real-time, no impact on source DB, handles deletes</li>
<li><strong>Trigger-Based:</strong> Database triggers fire on INSERT/UPDATE/DELETE, write change details to change table. Real-time but adds overhead to source system performance</li>
<li><strong>Hash-Based:</strong> Calculate hash of row, compare snapshots to find differences. Works without source modifications but resource-intensive for large datasets</li>
</ul>
<p><strong>Why CDC Matters:</strong></p>
<pre>-- Without CDC (Full Load Every Time) - INEFFICIENT
COPY INTO warehouse_sales FROM source_db  -- 100GB table
-- Takes 2 hours, processes entire table even if only 1GB changed
-- Heavy impact on source system

-- With CDC (Only Changes) - EFFICIENT
COPY INTO warehouse_sales FROM change_stream  -- Only 1GB changed
-- Takes 5 minutes, minimal source system impact
-- Near real-time data availability</pre>
<p><strong>Popular CDC Tools:</strong> Qlik Replicate (log-based), Fivetran (managed CDC), Debezium (Kafka-based open source), Oracle GoldenGate (enterprise), AWS DMS, Apache Hudi/Iceberg (data lake CDC)</p>
<p><strong>Real Use Cases:</strong> Real-time analytics dashboards, microservices data synchronization, database migration with zero downtime, maintaining dimensional data in data warehouse</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q17">
<div class="question-title">Python questions - List and Tuple</div>
<div class="answer">
<p><strong>List []: Mutable (changeable)</strong> - ordered collection that can be modified after creation</p>
<pre>my_list = [10, "apple", 3.14, True, [1, 2, 3]]  # Can contain mixed types, even nested lists
print(my_list[1])              # Output: apple (zero-indexed)
my_list[0] = 20                # Modify element
my_list.append("banana")       # Add element
my_list.extend([4, 5, 6])      # Add multiple elements
my_list.insert(1, "orange")    # Insert at specific position
my_list.remove("apple")        # Remove by value
popped = my_list.pop()         # Remove and return last element
my_list.sort()                 # Sort in-place
my_list.clear()                # Remove all elements

# Common operations
for item in my_list:
    print(item)  # Iterate

sliced = my_list[1:4]  # Slicing creates new list
reversed_list = my_list[::-1]  # Reverse</pre>
<p><strong>Tuple (): Immutable (unchangeable)</strong> - ordered collection that cannot be modified after creation</p>
<pre>my_tuple = (10, "apple", 3.14, True, (1, 2, 3))  # Can contain mixed types
print(my_tuple[1])           # Output: apple
# my_tuple[0] = 20           # TypeError! Tuples are immutable

# Single element tuple REQUIRES trailing comma
single_tuple = (5,)          # Correct
wrong_single = (5)           # This is just an int, not a tuple!

# Tuple packing and unpacking
coordinates = 10, 20, 30     # Automatic packing into tuple
x, y, z = coordinates        # Unpacking tuple to variables

# Tuples can be used as dictionary keys (lists cannot)
my_dict = {(0, 0): "origin", (1, 1): "diagonal"}

# Operations (read-only)
print(my_tuple.count(10))     # Count occurrences
print(my_tuple.index("apple"))  # Find index
print(len(my_tuple))          # Length</pre>
<table>
<tr>
<th>Feature</th>
<th>List</th>
<th>Tuple</th>
</tr>
<tr>
<td>Mutability</td>
<td>Mutable (changeable)</td>
<td>Immutable (fixed)</td>
</tr>
<tr>
<td>Performance</td>
<td>Slower (tracking changes)</td>
<td>Faster (fixed size)</td>
</tr>
<tr>
<td>Memory</td>
<td>More memory overhead</td>
<td>Less memory usage</td>
</tr>
<tr>
<td>As Dict Key</td>
<td>No - not hashable</td>
<td>Yes - if elements are hashable</td>
</tr>
<tr>
<td>Best Use Case</td>
<td>Dynamic data, frequent changes</td>
<td>Fixed data, constant values</td>
</tr>
<tr>
<td>Return from Function</td>
<td>N/A</td>
<td>Yes - functions return tuples</td>
</tr>
</table>
<p><strong>In Data Engineering:</strong> Tuples used for immutable data records, fixed field orders. Lists used for accumulating/processing data. DBT source definitions, data lineage metadata often use tuples</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q18">
<div class="question-title">AVRO, Parquet, ORC file formats - Uses</div>
<div class="answer">
<p><strong>AVRO (Apache Avro):</strong> Row-oriented data serialization format with self-describing schema</p>
<ul>
<li><strong>Schema Definition:</strong> Schema stored in JSON within each file (or managed externally via schema registry)</li>
<li><strong>Excellent Schema Evolution:</strong> Can add/remove/modify fields with clear rules for backward/forward compatibility</li>
<li><strong>Size:</strong> Compact binary format after serialization</li>
<li><strong>Uses:</strong>
<ul>
<li>Real-time streaming with Apache Kafka (Confluent)</li>
<li>Inter-process communication between services</li>
<li>Data archival where schema changes occur over time</li>
<li>CDC platforms like Debezium</li>
</ul>
</li>
<li><strong>Limitations:</strong> Row-based format, less efficient for OLAP queries (must read many rows to get one column)</li>
</ul>
<p><strong>Parquet (Apache Parquet):</strong> Columnar storage format optimized for analytics</p>
<ul>
<li><strong>Column Storage:</strong> Data organized by column, not row. Reading one column doesn't require reading others</li>
<li><strong>Compression:</strong> Column-level compression (RLE, dictionary encoding). Achieves 10x+ compression on repetitive columns</li>
<li><strong>Query Performance:</strong> Queries scanning specific columns are 10-100x faster than row-oriented formats</li>
<li><strong>Uses:</strong>
<ul>
<li>Data lakes (AWS S3, Azure ADLS) - primary format for analytics</li>
<li>Spark DataFrames and Pandas - native support</li>
<li>Snowflake, BigQuery, Redshift - optimized for Parquet</li>
<li>Machine Learning datasets - efficient feature access</li>
</ul>
</li>
<li><strong>Overhead:</strong> Largest metadata overhead, slower for row-by-row access but excellent for batch analytics</li>
</ul>
<p><strong>ORC (Optimized Row Columnar):</strong> Columnar format developed by Hortonworks for Hadoop ecosystem</p>
<ul>
<li><strong>Predicate Pushdown:</strong> Filtering applied during read from disk, not after. Scans only relevant stripes</li>
<li><strong>Type Awareness:</strong> Understanding of data types enables better compression and optimizations</li>
<li><strong>Uses:</strong>
<ul>
<li>Apache Hive (OLAP queries on Hadoop)</li>
<li>Data warehousing on Hadoop clusters</li>
<li>Spark SQL workloads in Hadoop ecosystems</li>
</ul>
</li>
<li><strong>Comparison:</strong> ORC typically smaller than Parquet (better compression) but less ecosystem support outside Hadoop</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>AVRO</th>
<th>Parquet</th>
<th>ORC</th>
</tr>
<tr>
<td>Storage Type</td>
<td>Row-oriented</td>
<td>Columnar</td>
<td>Columnar</td>
</tr>
<tr>
<td>Schema Evolution</td>
<td>Excellent (reader/writer schemas)</td>
<td>Good (backward compatible)</td>
<td>Good</td>
</tr>
<tr>
<td>Compression Ratio</td>
<td>Moderate</td>
<td>Very Good</td>
<td>Best</td>
</tr>
<tr>
<td>Query Performance</td>
<td>Good for full row reads</td>
<td>Excellent for column subset</td>
<td>Excellent for analytic queries</td>
</tr>
<tr>
<td>Ecosystem Support</td>
<td>Kafka, Pulsar, Streaming</td>
<td>Universal (Cloud DWs, Spark)</td>
<td>Hadoop/Hive focused</td>
</tr>
<tr>
<td>Best For</td>
<td>Event streaming, CDC</td>
<td>Data lakes, analytics</td>
<td>Hadoop, Hive queries</td>
</tr>
</table>
<p><strong>Recommendation for Modern Data Stack:</strong> Use Parquet for data lakes (S3/ADLS); Use Avro for streaming pipelines and CDC; ORC if using Hadoop ecosystem</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q19">
<div class="question-title">Difference between CTE and Temp table - Where to use</div>
<div class="answer">
<p><strong>CTE (Common Table Expression) - WITH Clause:</strong> Temporary named result set defined within a single query. Often called "WITH clause" or subquery alternative</p>
<pre>-- Multiple CTEs in one query
WITH 
-- CTE 1: Get top customers
TopCustomers AS (
    SELECT customer_id, SUM(order_amount) AS total_amount
    FROM orders 
    GROUP BY customer_id 
    ORDER BY total_amount DESC 
    LIMIT 100
),
-- CTE 2: Get their recent orders
RecentOrders AS (
    SELECT tc.customer_id, o.order_id, o.order_date, o.amount
    FROM TopCustomers tc
    JOIN orders o ON tc.customer_id = o.customer_id
    WHERE o.order_date &gt;= CURRENT_DATE - 30
)
-- Main query uses CTEs
SELECT customer_id, COUNT(*) as order_count, AVG(amount) as avg_amount
FROM RecentOrders
GROUP BY customer_id;</pre>
<p><strong>Temp Table (TEMPORARY Table):</strong> Session-scoped table that persists in database until session ends. Can be used across multiple queries in same session</p>
<pre>-- Create temp table that exists for this session
CREATE TEMPORARY TABLE top_customers AS
SELECT customer_id, SUM(order_amount) AS total_amount
FROM orders 
GROUP BY customer_id 
ORDER BY total_amount DESC 
LIMIT 100;

-- Use temp table in Query 1
SELECT * FROM top_customers WHERE total_amount &gt; 10000;

-- Use same temp table in Query 2
SELECT tc.customer_id, COUNT(*) 
FROM top_customers tc
JOIN orders o ON tc.customer_id = o.customer_id
GROUP BY tc.customer_id;

-- Query 3 still has access to temp table
INSERT INTO analytics_summary
SELECT * FROM top_customers;

-- Eventually session ends, temp table auto-drops</pre>
<table>
<tr>
<th>Feature</th>
<th>CTE (WITH)</th>
<th>Temporary Table</th>
</tr>
<tr>
<td>Scope</td>
<td>Single query only</td>
<td>Entire session</td>
</tr>
<tr>
<td>Persistence</td>
<td>Logical, not stored</td>
<td>Physical table in database</td>
</tr>
<tr>
<td>Reusability</td>
<td>Within same query</td>
<td>Across multiple queries</td>
</tr>
<tr>
<td>Materialization</td>
<td>Optimizer decides (often inlined)</td>
<td>Always materialized</td>
</tr>
<tr>
<td>Storage Cost</td>
<td>None</td>
<td>Counts toward table storage</td>
</tr>
<tr>
<td>Performance</td>
<td>Can be inlined (fast) or materialized</td>
<td>Predictable, always fast for reads</td>
</tr>
<tr>
<td>When to Use</td>
<td>Breaking down complex queries, single-use logic</td>
<td>Expensive operation reused many times in session</td>
</tr>
</table>
<p><strong>When to Use CTEs:</strong></p>
<ul>
<li>Breaking down complex queries into readable parts</li>
<li>Recursive queries (hierarchical data like org charts)</li>
<li>Logical organization without needing storage</li>
</ul>
<p><strong>When to Use Temp Tables:</strong></p>
<ul>
<li>Expensive calculation reused multiple times in session</li>
<li>Multi-step ETL processes in stored procedures</li>
<li>Needing to add indexes or optimize for specific access patterns</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q20">
<div class="question-title">Difference between Transient and Dynamic and Dynamic Transient tables</div>
<div class="answer">
<p><strong>TRANSIENT Table:</strong> Designed specifically for temporary data that doesn't require long-term protection</p>
<ul>
<li><strong>Time Travel:</strong> Default 0 days (configurable up to 1 day max) - cannot query historical versions</li>
<li><strong>Fail-safe:</strong> None (no 7-day recovery window)</li>
<li><strong>Storage Cost:</strong> Approximately 50% lower than permanent tables</li>
<li><strong>Creation:</strong> CREATE TRANSIENT TABLE my_table (...);</li>
<li><strong>Uses:</strong> Staging tables for daily loads, intermediate ETL results, temporary aggregations</li>
</ul>
<p><strong>Example Use Case:</strong></p>
<pre>-- Daily staging table for raw data
CREATE OR REPLACE TRANSIENT TABLE raw_daily_sales AS
COPY INTO FROM @s3_stage/daily_sales/
FILE_FORMAT = (TYPE = CSV);

-- Transform and move to permanent table
INSERT INTO fact_sales
SELECT * FROM raw_daily_sales
WHERE processing_complete = TRUE;

-- raw_daily_sales can be safely dropped - data is in fact_sales</pre>
<p><strong>Note:</strong> "Dynamic" and "Dynamic Transient" are not standard Snowflake table types. They may refer to:</p>
<ul>
<li><strong>Dynamic Data Loading:</strong> Frequently updated data with continuous CDC</li>
<li><strong>Project-specific Naming:</strong> Custom conventions for different use cases</li>
</ul>
<p>In production, most tables are either <strong>Permanent</strong> (for analytics) or <strong>Transient</strong> (for staging). See Question 22 for Ephemeral/Temporary tables comparison.</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q21">
<div class="question-title">How you will handle schema changes in the downstream</div>
<div class="answer">
<p><strong>Strategies for Handling Schema Changes:</strong></p>
<ol>
<li><strong>Communication &amp; Collaboration:</strong> Establish data governance processes, cross-functional meetings, maintain documentation</li>
<li><strong>Flexible Data Types:</strong> Use VARIANT columns for semi-structured data resilience to schema changes</li>
<li><strong>Schema-on-Read:</strong> Apply schema at query time, not load time (data lakes with Parquet/ORC)</li>
<li><strong>Additive Schema Changes (Preferred):</strong> Add new columns rather than renaming/deleting/changing types</li>
<li><strong>Soft Deletes/Deprecation:</strong> Mark columns as deprecated before removal</li>
<li><strong>Versioning Tables/Views:</strong> Create views over base tables to maintain consistent interface
                        <pre>-- Original view
CREATE VIEW v_customer AS SELECT customer_id, email FROM raw_customer;

-- Source table changes: 'email' becomes 'primary_contact_email'
-- Update view to maintain compatibility
CREATE VIEW v_customer AS SELECT customer_id, primary_contact_email AS email FROM raw_customer;</pre>
</li>
<li><strong>Data Contracts/Schema Registries:</strong> Define formal contracts using Avro/Protobuf</li>
<li><strong>Impact Analysis &amp; Testing:</strong> Understand dependencies, test in dev/test before prod</li>
<li><strong>Graceful Degradation:</strong> Implement logic that handles missing columns gracefully</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q22">
<div class="question-title">Ephemeral vs Permanent tables</div>
<div class="answer">
<p><strong>Permanent Table (Default):</strong> Standard table type with full data durability</p>
<ul>
<li>Time Travel: 1-90 days</li>
<li>Fail-safe: 7 days</li>
<li>Persistence: Until explicitly dropped</li>
<li>Cost: Highest storage cost</li>
<li>Uses: Core data, historical records</li>
</ul>
<p><strong>Ephemeral (TEMPORARY Table):</strong> Session-scoped table</p>
<ul>
<li>Persistence: Auto-dropped at session end</li>
<li>Time Travel: Session-bound (1 day max)</li>
<li>Fail-safe: None</li>
<li>Cost: Lowest storage cost</li>
<li>Uses: Ad-hoc analysis, session-specific work</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Permanent</th>
<th>TEMPORARY (Ephemeral)</th>
</tr>
<tr>
<td>Persistence</td>
<td>Until dropped</td>
<td>Session-scoped</td>
</tr>
<tr>
<td>Time Travel</td>
<td>1-90 days</td>
<td>Session-bound</td>
</tr>
<tr>
<td>Fail-safe</td>
<td>7 days</td>
<td>None</td>
</tr>
<tr>
<td>Cost</td>
<td>Highest</td>
<td>Lowest</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q23">
<div class="question-title">How to implement CDC without ETL tools in Snowflake</div>
<div class="answer">
<p><strong>Strategy 1: Timestamp-Based CDC</strong></p>
<pre>CREATE TABLE staging_data (
    id INT, data VARCHAR, last_modified TIMESTAMP
);

MERGE INTO target_table AS t
USING (SELECT * FROM staging_data WHERE last_modified &gt; :last_load_time) AS s
ON t.id = s.id
WHEN MATCHED THEN UPDATE SET t.data = s.data
WHEN NOT MATCHED THEN INSERT VALUES (s.id, s.data);</pre>
<p><strong>Strategy 2: Snowflake Streams (Recommended):</strong></p>
<pre>CREATE STREAM my_stream ON TABLE source_table;

-- Query stream - tracks inserts, updates, deletes
SELECT * FROM my_stream;
-- METADATA$ACTION shows 'INSERT' or 'DELETE'
-- METADATA$ISUPDATE shows if row is part of UPDATE

-- Consume changes atomically
MERGE INTO target_table AS t
USING my_stream AS s
ON t.id = s.id
WHEN MATCHED AND s.METADATA$ACTION='DELETE' THEN DELETE
WHEN NOT MATCHED AND s.METADATA$ACTION='INSERT' THEN INSERT ...;</pre>
<p><strong>Benefits of Streams:</strong> Exactly-once processing, captures all DML, low performance impact</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q24">
<div class="question-title">What is DBT and uses - Execution plan</div>
<div class="answer">
<p>DBT (Data Build Tool) is an open-source framework that enables data analysts to transform data in their warehouse using SQL and software engineering best practices. It focuses on the T in ELT.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>SQL-centric transformations</li>
<li>Modularity and reusability</li>
<li>Version control with Git</li>
<li>Built-in data quality tests</li>
<li>Auto-generated documentation</li>
<li>Dependency graph (DAG)</li>
<li>Jinja templating for dynamic SQL</li>
</ul>
<p><strong>DBT Execution Plan (dbt run):</strong></p>
<ol>
<li><strong>Parsing:</strong> Read .sql files, parse Jinja templates, resolve ref() functions</li>
<li><strong>DAG Building:</strong> Create dependency graph of all models</li>
<li><strong>Dependency Resolution:</strong> Determine execution order</li>
<li><strong>Materialization:</strong> Decide how to build each model (view, table, incremental, ephemeral)</li>
<li><strong>SQL Generation &amp; Execution:</strong> Generate SQL, execute in warehouse in correct order</li>
<li><strong>Post-Run Actions:</strong> Run tests, generate documentation</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q25">
<div class="question-title">Why continuous data load used - Use cases</div>
<div class="answer">
<p>Continuous data loading moves data as soon as it becomes available, providing near real-time insights instead of batch windows.</p>
<p><strong>Why Use It:</strong></p>
<ul>
<li>Near real-time analytics and operational dashboards</li>
<li>Reduced data latency</li>
<li>Improved responsiveness for decision-making</li>
<li>Eliminates batch window constraints</li>
<li>Distributes compute usage evenly over time</li>
<li>Better data quality feedback loops</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li><strong>Operational Analytics:</strong> Real-time sales dashboards, metrics</li>
<li><strong>Fraud Detection:</strong> Analyze transactions in real-time</li>
<li><strong>IoT/Sensor Data:</strong> Monitor machine performance, predictive maintenance</li>
<li><strong>Clickstream Analytics:</strong> Track user behavior, personalize experiences</li>
<li><strong>Log Analysis:</strong> Security monitoring, error detection</li>
<li><strong>Supply Chain:</strong> Real-time tracking and optimization</li>
<li><strong>Customer 360:</strong> Build comprehensive customer views for personalization</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q26">
<div class="question-title">Explain end-to-end data pipeline - What are the logics you followed</div>
<div class="answer">
<p><strong>Typical Stages:</strong></p>
<ol>
<li><strong>Data Sources:</strong> Operational databases, SaaS apps, APIs, logs (identify what data, how to access, frequency)</li>
<li><strong>Ingestion/Extraction:</strong> Batch or streaming (full load vs incremental CDC)</li>
<li><strong>Landing/Staging:</strong> Raw data storage with schema-on-read approach</li>
<li><strong>Transformation:</strong> Clean, enrich, conform, aggregate (using DBT, SQL, or external engines)</li>
<li><strong>Serving/Presentation:</strong> Optimized views/tables for consumption</li>
<li><strong>Consumption:</strong> BI tools, dashboards, data science, applications</li>
</ol>
<p><strong>Cross-cutting Concerns:</strong></p>
<ul>
<li><strong>Orchestration:</strong> Schedule and manage pipeline stages (Airflow, Tasks, Step Functions)</li>
<li><strong>Monitoring &amp; Alerting:</strong> Track pipeline health, errors, data quality</li>
<li><strong>Data Lineage:</strong> Understand data flow, debug issues</li>
<li><strong>Cost Management:</strong> Monitor and optimize cloud resource usage</li>
<li><strong>Version Control:</strong> Git for all code (SQL, Python, DBT)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q27">
<div class="question-title">Why implement CDC instead of other approaches - Use case</div>
<div class="answer">
<p><strong>Why CDC instead of full loads:</strong></p>
<ul>
<li><strong>Efficiency:</strong> Only changed data transferred/processed, reduces bandwidth and compute</li>
<li><strong>Near Real-time:</strong> Data availability within minutes/seconds</li>
<li><strong>Reduced Impact:</strong> Minimal strain on source systems (especially log-based CDC)</li>
<li><strong>Captures Deletes:</strong> Unlike timestamp queries, robust CDC captures DELETE operations</li>
<li><strong>Simplified Logic:</strong> Streams abstract away complexity of determining changes</li>
</ul>
<p><strong>Real-world Use Case: E-commerce Inventory Management</strong></p>
<p>Problem without CDC: Nightly batch updates mean inventory is 12-24 hours old â†’ customers order out-of-stock items, poor warehouse operations</p>
<p>Solution with CDC: Stream changes from inventory DB into Snowflake using CDC â†’ near real-time dashboards â†’ website always knows actual stock levels â†’ automated reordering â†’ efficient logistics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q28">
<div class="question-title">Snowflake Tasks - Troubleshooting failed queries and performance improvement</div>
<div class="answer">
<p><strong>Snowflake Tasks:</strong> Execute SQL statements or call stored procedures on a recurring schedule or when conditions are met</p>
<p><strong>Key Features:</strong> Scheduling (cron-like), DAG dependencies, conditional execution, error handling</p>
<p><strong>Troubleshooting Failed/Slow Queries:</strong></p>
<ol>
<li><strong>Identify Problem:</strong> Query History in UI or ACCOUNT_USAGE.QUERY_HISTORY</li>
<li><strong>Analyze Query Profile:</strong> Check execution phases, identify bottlenecks (scanning, joining, spilling)</li>
<li><strong>Common Issues &amp; Solutions:</strong>
<ul>
<li>Spilling to disk â†’ Increase warehouse size</li>
<li>Poor pruning â†’ Add clustering keys or improve WHERE clauses</li>
<li>Inefficient joins â†’ Rewrite query, change join order</li>
<li>Expensive operations â†’ Avoid SELECT *, DISTINCT, ORDER BY unless necessary</li>
</ul>
</li>
<li><strong>Task-Specific:</strong> Check TASK_HISTORY for STATE and ERROR_MESSAGE, verify warehouse is available</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q29">
<div class="question-title">What is Zero Copy clone - Uses</div>
<div class="answer">
<p>Zero-Copy Cloning creates an instant copy of a database, schema, or table without physically duplicating data. Snowflake creates metadata pointers to the same micro-partitions. Changes use copy-on-write.</p>
<p><strong>Key Benefits:</strong></p>
<ul>
<li>Instantaneous - created almost instantly</li>
<li>No storage cost at creation - pay only for incremental changes</li>
<li>No data movement</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>CREATE DATABASE prod_db_clone CLONE prod_db;
CREATE SCHEMA temp_schema CLONE prod_schema;
CREATE TABLE test_table CLONE prod_table;</pre>
<p><strong>Uses:</strong></p>
<ul>
<li><strong>Development/Testing:</strong> Clone production DB for developers to test without affecting prod</li>
<li><strong>Disaster Recovery:</strong> Create point-in-time recovery environment quickly</li>
<li><strong>Analytical Workloads:</strong> Isolate complex queries from main system</li>
<li><strong>Version Control:</strong> Snapshot data at milestones for rollback capability</li>
<li><strong>Historical Analysis:</strong> Create monthly snapshots with minimal cost</li>
<li><strong>What-If Scenarios:</strong> Run simulations without impacting production</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q30">
<div class="question-title">Convert timestamp from one timezone to another</div>
<div class="answer">
<p>Snowflake has three TIMESTAMP types: TIMESTAMP_LTZ (Local Time Zone), TIMESTAMP_NTZ (No Time Zone), TIMESTAMP_TZ (Time Zone).</p>
<p><strong>Using CONVERT_TIMEZONE (Recommended):</strong></p>
<pre>SELECT CONVERT_TIMEZONE('UTC', 'America/New_York', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 06:00:00.000'

SELECT CONVERT_TIMEZONE('America/Los_Angeles', 'Asia/Kolkata', '2025-07-04 10:00:00'::TIMESTAMP_NTZ);
-- Result: '2025-07-04 22:30:00.000'

-- With a column
SELECT event_id, event_timestamp,
       CONVERT_TIMEZONE('UTC', 'America/New_York', event_timestamp) AS event_timestamp_est
FROM events;</pre>
<p><strong>Using AT TIME ZONE:</strong></p>
<pre>SELECT '2025-07-04 10:00:00'::TIMESTAMP_NTZ AT TIME ZONE 'America/New_York';

-- Set session timezone first
ALTER SESSION SET TIMEZONE = 'UTC';
SELECT event_timestamp AT TIME ZONE 'Asia/Kolkata'
FROM events;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q31">
<div class="question-title">Clustering Keys in Snowflake</div>
<div class="answer">
<p>Clustering Keys optimize the physical organization of data within micro-partitions to improve query performance.</p>
<p><strong>How They Work:</strong> Snowflake's Automatic Clustering service re-clusters data if clustering becomes stale, physically reorganizing micro-partitions to group similar values together.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Improved query performance for filtered/joined queries</li>
<li>Reduced data scanning â†’ fewer credits consumed</li>
<li>Automatic and transparent</li>
</ul>
<p><strong>Syntax:</strong></p>
<pre>-- During creation
CREATE TABLE sales (
    event_date DATE,
    category VARCHAR,
    value DECIMAL
) CLUSTER BY (event_date, category);

-- After creation
ALTER TABLE sales CLUSTER BY (event_date, category);

-- Remove clustering
ALTER TABLE sales DROP CLUSTERING KEY;</pre>
<p><strong>When to Use:</strong></p>
<ul>
<li>Very large tables (hundreds of GB to TB)</li>
<li>Frequent filtering/joining on specific columns</li>
<li>High cardinality columns with skewed data</li>
<li>Data ingestion patterns cause poor clustering over time</li>
</ul>
<p><strong>Monitor with:</strong> SYSTEM$CLUSTERING_INFORMATION() function</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q32">
<div class="question-title">How to monitor Snowflake performance</div>
<div class="answer">
<p><strong>Snowflake Web Interface:</strong></p>
<ul>
<li>Query History - view all queries, duration, credits, status</li>
<li>Query Profile - visualize execution plan, identify bottlenecks</li>
<li>Warehouses - monitor status, usage, credit consumption</li>
</ul>
<p><strong>Account Usage Views (SNOWFLAKE.ACCOUNT_USAGE):</strong></p>
<ul>
<li>QUERY_HISTORY - comprehensive query data</li>
<li>WAREHOUSE_METERING_HISTORY - daily credit consumption per warehouse</li>
<li>AUTOMATIC_CLUSTERING_HISTORY - credits consumed by clustering</li>
<li>PIPE_USAGE_HISTORY - credits consumed by Snowpipe</li>
<li>COPY_HISTORY - COPY INTO command status</li>
<li>TASK_HISTORY - task execution history</li>
</ul>
<p><strong>External Tools:</strong></p>
<ul>
<li>Cloud monitoring (CloudWatch, Azure Monitor)</li>
<li>Third-party APM tools (Datadog, Splunk)</li>
<li>BI tools for custom dashboards</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q33">
<div class="question-title">Attacama and Collibra - Uses and Differences</div>
<div class="answer">
<p><strong>Collibra:</strong> Business-centric data governance platform</p>
<ul>
<li>Strong emphasis on stewardship and collaboration</li>
<li>Excellent Business Glossary, workflow automation, policy management</li>
<li>Best for: Building data governance programs, empowering business users, compliance</li>
</ul>
<p><strong>Ataccama ONE:</strong> Integrated data management platform</p>
<ul>
<li>Combines data quality, MDM, catalog, and governance</li>
<li>Strong data quality engine, Master Data Management, AI/ML automation</li>
<li>Best for: Data quality frameworks, MDM solutions, operational data management</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Collibra</th>
<th>Ataccama ONE</th>
</tr>
<tr>
<td>Core Focus</td>
<td>Business Governance, Stewardship</td>
<td>Data Quality, MDM, Governance</td>
</tr>
<tr>
<td>Strength</td>
<td>Business Glossary, Workflows</td>
<td>Data Quality, MDM, AI/ML</td>
</tr>
<tr>
<td>Target Users</td>
<td>Data Stewards, Governance Teams</td>
<td>Data Engineers, Quality Teams</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q34">
<div class="question-title">How to optimize long running queries - How to reduce time</div>
<div class="answer">
<p><strong>Step 1: Analyze Query Profile (Mandatory)</strong></p>
<ul>
<li>Identify bottleneck operators (scanning, joining, aggregating, sorting)</li>
<li>Check spilling to disk - indicates memory constraints</li>
<li>Evaluate pruning efficiency</li>
</ul>
<p><strong>Step 2: Right-Size Virtual Warehouse</strong></p>
<pre>-- Increase warehouse size if spilling or high queue time
ALTER WAREHOUSE MY_WH SET WAREHOUSE_SIZE = 'LARGE';</pre>
<p><strong>Step 3: Optimize SQL Logic</strong></p>
<ul>
<li>SELECT only necessary columns (avoid SELECT *)</li>
<li>Filter early and effectively with WHERE clauses</li>
<li>Optimize joins - proper join order, use INNER JOIN when possible</li>
<li>Avoid correlated subqueries - use JOINs instead</li>
<li>Eliminate unnecessary DISTINCT or ORDER BY</li>
</ul>
<p><strong>Step 4: Leverage Data Organization</strong></p>
<ul>
<li>Define clustering keys for very large tables</li>
<li>Create materialized views for complex, frequently-run queries</li>
</ul>
<p><strong>Step 5: Leverage Caching</strong></p>
<ul>
<li>Encourage reuse of queries for result cache</li>
<li>Keep warehouses running for warehouse cache benefits</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q35">
<div class="question-title">What is Snowpipe - How to integrate using AWS S3, SNS, SQS, Task, Stored Proc</div>
<div class="answer">
<p>Snowpipe is Snowflake's continuous data ingestion service that loads data as soon as files appear in cloud storage.</p>
<p><strong>AWS Integration Flow:</strong> S3 Event â†’ SNS â†’ SQS â†’ Snowpipe â†’ COPY INTO</p>
<p><strong>Key Steps:</strong></p>
<ol>
<li><strong>AWS Setup:</strong> Create S3 bucket, SNS topic, SQS queue, IAM role with permissions</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_pipe_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="3">
<li><strong>Create Stage:</strong></li>
</ol>
<pre>CREATE STAGE raw_stage
  STORAGE_INTEGRATION = s3_pipe_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<ol start="4">
<li><strong>Create Target Table:</strong></li>
</ol>
<pre>CREATE TABLE raw_data (col1 INT, col2 VARCHAR, load_ts TIMESTAMP);</pre>
<ol start="5">
<li><strong>Create Snowpipe:</strong></li>
</ol>
<pre>CREATE PIPE raw_data_pipe
  AUTO_INGEST = TRUE
  AWS_SQS_QUEUE_ARN = 'arn:aws:sqs:region:account:queue'
  AS
  COPY INTO raw_data FROM @raw_stage;</pre>
<ol start="6">
<li><strong>Post-Load Processing (Optional):</strong></li>
</ol>
<pre>-- Create stream on target table
CREATE STREAM raw_data_stream ON TABLE raw_data;

-- Create stored proc for transformations
CREATE PROCEDURE process_new_data()
  AS $$ 
    MERGE INTO fact_table AS t
    USING raw_data_stream AS s ON t.id = s.id
    WHEN NOT MATCHED THEN INSERT ...;
  $$;

-- Create task to run after data arrival
CREATE TASK transform_task
  WHEN SYSTEM$STREAM_HAS_DATA('raw_data_stream')
  AS CALL process_new_data();</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q36">
<div class="question-title">Max Salary for each department and 4th max salary for each department</div>
<div class="answer">
<p><strong>Max Salary for Each Department:</strong></p>
<pre>SELECT department, MAX(salary) AS max_salary_in_department
FROM employees
GROUP BY department
ORDER BY department;</pre>
<p><strong>4th Max Salary for Each Department:</strong></p>
<pre>SELECT employee_id, department, salary
FROM employees
QUALIFY DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) = 4
ORDER BY department, salary DESC;</pre>
<p><strong>Using CTE (More Portable SQL):</strong></p>
<pre>WITH RankedSalaries AS (
    SELECT employee_id, department, salary,
           DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank
    FROM employees
)
SELECT employee_id, department, salary
FROM RankedSalaries
WHERE salary_rank = 4
ORDER BY department;</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q37">
<div class="question-title">Swap the gender value for a table - Male to Female, Female to Male</div>
<div class="answer">
<pre>UPDATE users
SET gender = CASE
    WHEN gender = 'Male' THEN 'Female'
    WHEN gender = 'Female' THEN 'Male'
    ELSE gender -- Keep other values (Prefer not to say, NULL, etc.)
END
WHERE gender IN ('Male', 'Female');</pre>
<p><strong>Explanation:</strong></p>
<ul>
<li>CASE statement evaluates current value of gender column</li>
<li>If 'Male', changes to 'Female' and vice versa</li>
<li>ELSE gender preserves other values unchanged</li>
<li>WHERE clause optimizes by processing only relevant rows</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q38">
<div class="question-title">DBT Project Architecture</div>
<div class="answer">
<p><strong>Typical DBT Project Structure:</strong></p>
<pre>my_dbt_project/
â”œâ”€â”€ dbt_project.yml         # Core configuration
â”œâ”€â”€ models/                 # Data transformation models
â”‚   â”œâ”€â”€ staging/           # Raw, light transformations
â”‚   â”œâ”€â”€ intermediate/      # Complex joins, business logic
â”‚   â””â”€â”€ marts/             # Final, user-facing models
â”œâ”€â”€ analysis/              # Ad-hoc SQL queries
â”œâ”€â”€ macros/                # Reusable SQL snippets
â”œâ”€â”€ seeds/                 # Static CSV lookup tables
â”œâ”€â”€ snapshots/             # SCD Type 2 configurations
â”œâ”€â”€ tests/                 # Custom data quality tests
â”œâ”€â”€ logs/                  # DBT execution logs
â””â”€â”€ target/                # Compiled SQL, manifests</pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>dbt_project.yml:</strong> Project configuration, default materializations</li>
<li><strong>Models:</strong> SQL SELECT statements - heart of DBT project</li>
<li><strong>Tests:</strong> Data quality validations (not_null, unique, accepted_values, custom)</li>
<li><strong>Macros:</strong> Reusable Jinja templates for dynamic SQL</li>
<li><strong>Seeds:</strong> Static CSV files loaded as tables</li>
<li><strong>Snapshots:</strong> Capture historical changes (SCD Type 2)</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q39">
<div class="question-title">Difference between UPSERT and MERGE</div>
<div class="answer">
<p><strong>UPSERT:</strong> A conceptual operation - UPDATE if exists, INSERT if not</p>
<p><strong>MERGE:</strong> Standard SQL statement (SQL:2003) that implements UPSERT</p>
<p><strong>MERGE in Snowflake:</strong></p>
<pre>MERGE INTO target_table AS T
USING source_data AS S
ON T.id = S.id
WHEN MATCHED AND T.last_updated &lt; S.last_updated THEN
    UPDATE SET T.name = S.name, T.email = S.email
WHEN NOT MATCHED THEN
    INSERT (id, name, email) VALUES (S.id, S.name, S.email);</pre>
<p><strong>Example:</strong> Update customer 1's email, insert new customer 3</p>
<pre>-- Before MERGE
customers: (1, 'Alice', 'alice@old.com'), (2, 'Bob', 'bob@example.com')
staging: (1, 'Alice', 'alice@new.com'), (3, 'Charlie', 'charlie@example.com')

-- After MERGE
customers: (1, 'Alice', 'alice@new.com'), (2, 'Bob', 'bob@example.com'), (3, 'Charlie', 'charlie@example.com')</pre>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q40">
<div class="question-title">Data Masking and Masking Policy</div>
<div class="answer">
<p><strong>Data Masking:</strong> Hiding sensitive data with fictional data while maintaining usefulness for development/testing/analytics.</p>
<p><strong>Snowflake Masking Policy:</strong> Dynamic masking at query time based on user role. Underlying data is never modified.</p>
<p><strong>Example:</strong></p>
<pre>-- Create masking policy
CREATE MASKING POLICY email_mask AS (val VARCHAR) RETURNS VARCHAR -&gt;
    CASE
        WHEN CURRENT_ROLE() IN ('ANALYST_ROLE') THEN '****'
        WHEN CURRENT_ROLE() = 'DATA_STEWARD_ROLE' THEN val
        ELSE 'No access'
    END;

-- Apply to column
ALTER TABLE users ALTER COLUMN email SET MASKING POLICY email_mask;</pre>
<p><strong>Benefits:</strong> Dynamic, centralized, granular control, secure, no data duplication</p>
<p><strong>Common Techniques:</strong> Substitution, shuffling, redaction, tokenization, hashing, encryption</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q41">
<div class="question-title">Difference between Hashing and Encryption</div>
<div class="answer">
<p><strong>Hashing:</strong> One-way function creating fixed-size unique string</p>
<ul>
<li>Purpose: Integrity verification, password storage</li>
<li>One-way: Cannot reverse to get original</li>
<li>Deterministic: Same input always produces same hash</li>
<li>Examples: MD5 (32 chars), SHA256 (64 chars)</li>
</ul>
<p><strong>Encryption:</strong> Two-way function requiring key to encrypt/decrypt</p>
<ul>
<li>Purpose: Confidentiality, protecting sensitive data</li>
<li>Reversible: Decrypt with correct key to get original</li>
<li>Key-dependent: Requires encryption/decryption key</li>
<li>Examples: AES, RSA</li>
</ul>
<table>
<tr>
<th>Feature</th>
<th>Hashing</th>
<th>Encryption</th>
</tr>
<tr>
<td>Reversibility</td>
<td>One-way (irreversible)</td>
<td>Two-way (reversible)</td>
</tr>
<tr>
<td>Purpose</td>
<td>Integrity, passwords</td>
<td>Confidentiality</td>
</tr>
<tr>
<td>Key Usage</td>
<td>No key needed</td>
<td>Requires key</td>
</tr>
<tr>
<td>Output Size</td>
<td>Fixed</td>
<td>Variable</td>
</tr>
</table>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q42">
<div class="question-title">Snowflake storage integration with AWS S3</div>
<div class="answer">
<p><strong>Purpose:</strong> Secure, credential-less access to S3 from Snowflake</p>
<p><strong>Step-by-Step Setup:</strong></p>
<ol>
<li><strong>AWS IAM Policy:</strong> Grant Snowflake permissions (s3:GetObject, s3:PutObject, s3:ListBucket)</li>
<li><strong>AWS IAM Role:</strong> Create role, attach policy</li>
<li><strong>Snowflake Storage Integration:</strong></li>
</ol>
<pre>CREATE STORAGE INTEGRATION s3_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::ACCOUNT:role/SnowflakeRole'
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/');</pre>
<ol start="4">
<li><strong>Get Snowflake's External ID &amp; IAM User:</strong>
<pre>DESCRIBE INTEGRATION s3_integration;</pre>
</li>
<li><strong>Update AWS IAM Role Trust Relationship:</strong> Add Snowflake's IAM user ARN and external ID</li>
<li><strong>Create External Stage:</strong></li>
</ol>
<pre>CREATE STAGE my_s3_stage
  STORAGE_INTEGRATION = s3_integration
  URL = 's3://bucket/path/'
  FILE_FORMAT = (TYPE = CSV);</pre>
<p><strong>Benefits:</strong> Enhanced security, centralized control, no exposed AWS keys</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q43">
<div class="question-title">What is DBT and what problem does it solve in modern data stack</div>
<div class="answer">
<p>DBT (data build tool) is a command-line framework for transforming data in your warehouse with modular SQL and software engineering best practices.</p>
<p><strong>Problems it Solves:</strong></p>
<ul>
<li><strong>Code Proliferation:</strong> Centralizes transformation logic instead of scattered ETL scripts/BI tools</li>
<li><strong>Lack of Best Practices:</strong> Brings version control, modularity, testing, documentation to data work</li>
<li><strong>Dependency Management:</strong> Automatically determines correct model execution order</li>
<li><strong>Testing &amp; Quality:</strong> Provides framework for data quality tests and auto-documentation</li>
<li><strong>Transparency:</strong> Makes entire transformation process transparent and auditable</li>
</ul>
<p><strong>Core Philosophy:</strong> Leverage the power of the data warehouse itself (pushing computations) rather than using separate processing engines</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q44">
<div class="question-title">Core components of a DBT project: Models, Tests, Seeds</div>
<div class="answer">
<p><strong>Models:</strong> SQL SELECT statements organized into transformation logic</p>
<ul>
<li>Each .sql file = one model</li>
<li>Output: view or table (depends on materialization)</li>
<li>Example: Model calculating monthly sales from raw transactions</li>
</ul>
<p><strong>Tests:</strong> Data quality checks ensuring data integrity</p>
<ul>
<li><strong>Singular Tests:</strong> Custom SQL returning failing rows if condition not met</li>
<li><strong>Generic Tests:</strong> Pre-defined tests applied via YAML (not_null, unique, accepted_values)</li>
<li>Purpose: Prevent downstream errors, ensure data reliability</li>
</ul>
<p><strong>Seeds:</strong> Static CSV files loaded as reference/lookup tables</p>
<ul>
<li>Small, infrequently-changing data (country codes, mapping tables)</li>
<li>Loaded via dbt seed command</li>
</ul>
<p><strong>Macros (Bonus):</strong> Reusable Jinja+SQL code snippets (like functions)</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q45">
<div class="question-title">Materializations in DBT - Four default types</div>
<div class="answer">
<p><strong>View (Default):</strong></p>
<ul>
<li>Mechanism: Creates SQL view</li>
<li>Pros: Always latest data, fast compile</li>
<li>Cons: Slow queries (logic runs every time)</li>
<li>Best for: Simple transformations, rarely-queried models</li>
</ul>
<p><strong>Table:</strong></p>
<ul>
<li>Mechanism: Creates persistent table with CREATE TABLE AS</li>
<li>Pros: Fast queries (pre-calculated)</li>
<li>Cons: Slow to build, consumes storage</li>
<li>Best for: Complex/frequently-queried models, large datasets</li>
</ul>
<p><strong>Incremental:</strong></p>
<ul>
<li>Mechanism: INSERT/MERGE only new/changed records</li>
<li>Pros: Very fast after initial build</li>
<li>Cons: Complex design, prone to errors</li>
<li>Best for: Large fact tables with incremental changes</li>
</ul>
<p><strong>Ephemeral:</strong></p>
<ul>
<li>Mechanism: No physical object, compiles to CTE</li>
<li>Pros: Excellent modularity</li>
<li>Cons: Not queryable directly</li>
<li>Best for: Simple intermediate cleaning steps</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q46">
<div class="question-title">Managing dependencies and referencing models in DBT</div>
<div class="answer">
<p><strong>The ref() Function:</strong> Reference models without hardcoding schema names</p>
<pre>-- Instead of FROM SCHEMA.TABLE
SELECT * FROM {{ ref('stg_orders') }}
WHERE is_valid = TRUE</pre>
<p><strong>How it Works:</strong></p>
<ul>
<li>{{ ref('stg_orders') }} replaced with correct schema/table name during compilation</li>
<li>DBT automatically knows stg_orders must be built before this model</li>
<li>Creates dependency edge in DAG</li>
</ul>
<p><strong>Automatic DAG Generation:</strong></p>
<ul>
<li>DBT scans all models and their ref() calls</li>
<li>Builds DAG showing ALL dependencies</li>
<li>dbt run executes models in correct order, ensuring source tables exist before use</li>
</ul>
<p><strong>Benefits:</strong> No manual ordering, automatic parallelization, clear dependencies</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q47">
<div class="question-title">What is Jinja in DBT - Simple example</div>
<div class="answer">
<p>Jinja is a templating language that adds programming logic to SQL. Processed before SQL reaches the warehouse.</p>
<p><strong>Capabilities:</strong> if/else statements, variables, loops, macro reuse, dynamic SQL</p>
<p><strong>Simple Example:</strong></p>
<pre>{% set limit_date = '2023-01-01' %}

SELECT * FROM {{ ref('raw_data') }}
WHERE created_at &gt;= '{{ limit_date }}'

{% if target.name == 'prod' %}
  AND is_active = TRUE
{% endif %}</pre>
<p><strong>Compiled SQL (Development):</strong></p>
<pre>SELECT * FROM my_dev_schema.raw_data
WHERE created_at &gt;= '2023-01-01'</pre>
<p><strong>Compiled SQL (Production):</strong></p>
<pre>SELECT * FROM my_prod_schema.raw_data
WHERE created_at &gt;= '2023-01-01'
  AND is_active = TRUE</pre>
<p><strong>Benefits:</strong> DRY (Don't Repeat Yourself), environment-specific logic, dynamic SQL generation</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q48">
<div class="question-title">Data Governance best practices</div>
<div class="answer">
<p><strong>Key Best Practices:</strong></p>
<ol>
<li><strong>Define Clear Ownership:</strong> Assign data owners and stewards for each domain/dataset</li>
<li><strong>Establish Data Dictionary:</strong> Document all data elements, definitions, relationships</li>
<li><strong>Implement Metadata Management:</strong> Track technical metadata (schema, lineage) and business metadata</li>
<li><strong>Quality Standards:</strong> Implement data quality rules, monitoring, and remediation processes</li>
<li><strong>Access Control:</strong> RBAC, row-level security, column-level masking</li>
<li><strong>Data Lineage:</strong> Understand data flow from source to consumption</li>
<li><strong>Compliance &amp; Auditing:</strong> Track data usage, audit logs, regulatory compliance (GDPR, HIPAA)</li>
<li><strong>Version Control:</strong> Version all code (SQL, DBT, Python) in Git</li>
<li><strong>Documentation:</strong> Auto-generate and maintain docs (DBT, Collibra, Ataccama)</li>
<li><strong>Collaboration:</strong> Foster data culture, communication between teams</li>
</ol>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q49">
<div class="question-title">Data quality testing strategies</div>
<div class="answer">
<p><strong>Common Testing Approaches:</strong></p>
<ol>
<li><strong>Schema Tests:</strong> Validate data types, constraints
                        <ul>
<li><strong>not_null:</strong> Column should not have NULL values</li>
<li><strong>unique:</strong> Column values must be distinct</li>
<li><strong>relationships:</strong> Foreign key relationships</li>
<li><strong>accepted_values:</strong> Column values from allowed set</li>
</ul>
</li>
<li><strong>Statistical Tests:</strong> Validate data distributions, outliers
                        <ul>
<li>Range checks</li>
<li>Distribution analysis</li>
<li>Outlier detection</li>
</ul>
</li>
<li><strong>Business Logic Tests:</strong> Validate business rules
                        <ul>
<li>Cross-table consistency</li>
<li>Aggregation accuracy</li>
<li>Recency checks</li>
</ul>
</li>
<li><strong>Freshness Tests:</strong> Ensure data is current
                        <ul>
<li>Last update timestamp</li>
<li>Row count expectations</li>
</ul>
</li>
</ol>
<p><strong>Tools:</strong> DBT tests, Great Expectations, Ataccama, custom SQL checks</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q50">
<div class="question-title">Building a modern data stack - Technology selection</div>
<div class="answer">
<p><strong>Typical Modern Data Stack Layers:</strong></p>
<ul>
<li><strong>Source Systems:</strong> Operational databases, SaaS applications, APIs</li>
<li><strong>Ingestion:</strong> Fivetran, Stitch, Airbyte, AWS DMS, Qlik Replicate</li>
<li><strong>Cloud Data Warehouse:</strong> Snowflake, BigQuery, Redshift</li>
<li><strong>Transformation:</strong> DBT, Spark, Python</li>
<li><strong>Data Governance:</strong> Collibra, Ataccama, DataHub</li>
<li><strong>Analytics/BI:</strong> Tableau, Power BI, Looker</li>
<li><strong>Orchestration:</strong> Airflow, Prefect, Dagster</li>
<li><strong>Reverse ETL:</strong> Hightouch, Census (sync data back to source systems)</li>
</ul>
<p><strong>Selection Criteria:</strong></p>
<ul>
<li>Scalability and performance</li>
<li>Cost efficiency</li>
<li>Ease of use / learning curve</li>
<li>Integration with existing tools</li>
<li>Community and support</li>
<li>Security and compliance requirements</li>
</ul>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
<div class="question" id="q51">
<div class="question-title">Future trends in data engineering</div>
<div class="answer">
<p><strong>Emerging Trends:</strong></p>
<ul>
<li><strong>DataOps:</strong> Applying DevOps principles to data pipelines (CI/CD, monitoring, automation)</li>
<li><strong>Real-time Analytics:</strong> Move beyond batch to streaming/real-time data pipelines</li>
<li><strong>AI/ML Integration:</strong> ML models integrated into data pipelines, automated feature engineering</li>
<li><strong>Data Mesh:</strong> Decentralized data ownership model, domain-oriented data architecture</li>
<li><strong>Lakehouse Architecture:</strong> Combining data lake flexibility with warehouse performance (Delta Lake, Iceberg, Hudi)</li>
<li><strong>Cloud-Native:</strong> Serverless data platforms, compute separation from storage</li>
<li><strong>Data Quality as First-Class:</strong> Increased focus on data quality, governance, and observability</li>
<li><strong>Self-Service Analytics:</strong> Tools enabling business users to perform analysis without technical expertise</li>
<li><strong>Reverse ETL:</strong> Syncing aggregated/enriched data back to operational systems</li>
<li><strong>Privacy-Preserving Technologies:</strong> Differential privacy, federated learning for sensitive data</li>
</ul>
<p><strong>Skills for Future:</strong> Cloud platforms (AWS/Azure/GCP), DBT, SQL, Python, Spark, containers, Kubernetes, ML basics</p>
<a class="back-to-toc" href="#toc">â¬† Back to Contents</a>
</div>
</div>
</div>
<footer style="margin-top:60px; text-align:center; color:#777; font-size:0.9em;">
  Â© 2026 Â· Vemula Suryateja Â· <a href="mailto:vsurya@duck.com">vsurya@duck.com</a>
</footer>

</div>

<footer style="margin:60px auto 40px; text-align:center; color:#777; font-size:0.9em;">
  Â© 2026 Â· Vemula Suryateja Â· <a href="mailto:vsurya@duck.com">vsurya@duck.com</a>
</footer>

</body>
</html>
